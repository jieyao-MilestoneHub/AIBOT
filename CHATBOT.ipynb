{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Reference  \n",
        "model: https://pytorch.org/tutorials/beginner/chatbot_tutorial.html#define-training-procedure  \n",
        "dataset: https://github.com/zake7749/Gossiping-Chinese-Corpus/tree/master"
      ],
      "metadata": {
        "id": "uWS-OZi5m2-q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "目標: 以 flask 實現網頁 APP，並利用 pytorch 訓練語到對語言架構，把使用者輸入丟進 pre-train model 進行回應。  \n",
        "相關資訊請參考: https://github.com/efef31016/Useful_Practice/tree/main/AIBOT"
      ],
      "metadata": {
        "id": "aaKfDIFhnBVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id '1r4px0i-NcrnXy1-tkBsIwvYwbWnxAhcg' --output data.tar.gz\n",
        "!tar -zxvf data.tar.gz\n",
        "!mkdir ckpt\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noCHRpnbKpe-",
        "outputId": "b886fa34-b6b5-4372-bfc4-fc0cd0f30214"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1r4px0i-NcrnXy1-tkBsIwvYwbWnxAhcg\n",
            "To: /content/data.tar.gz\n",
            "100% 5.83M/5.83M [00:00<00:00, 165MB/s]\n",
            "cmn-eng/\n",
            "cmn-eng/int2word_cn.json\n",
            "cmn-eng/int2word_en.json\n",
            "cmn-eng/preprocess/\n",
            "cmn-eng/preprocess/build_dataset.py\n",
            "cmn-eng/preprocess/build_dictionary.sh\n",
            "cmn-eng/preprocess/cmn.txt\n",
            "cmn-eng/preprocess/cn.txt\n",
            "cmn-eng/preprocess/dict.txt.big\n",
            "cmn-eng/preprocess/dict.txt.small\n",
            "cmn-eng/preprocess/en.txt\n",
            "cmn-eng/preprocess/en_code.txt\n",
            "cmn-eng/preprocess/en_refine.txt\n",
            "cmn-eng/preprocess/en_vocab.txt\n",
            "cmn-eng/preprocess/tokenizer.py\n",
            "cmn-eng/testing.txt\n",
            "cmn-eng/training.txt\n",
            "cmn-eng/validation.txt\n",
            "cmn-eng/word2int_cn.json\n",
            "cmn-eng/word2int_en.json\n",
            "mkdir: cannot create directory ‘ckpt’: File exists\n",
            "ckpt  cmn-eng  data.tar.gz  drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip3 install --user nltk"
      ],
      "metadata": {
        "id": "_zAqsmfh1uZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.jit import script, trace\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import csv\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import unicodedata\n",
        "import codecs\n",
        "from io import open\n",
        "import itertools\n",
        "import math\n",
        "import json\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
      ],
      "metadata": {
        "id": "bn8E9_-ra2lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform(txtlist, word2int, index):\n",
        "\n",
        "  BOS = 0\n",
        "  EOS = 1\n",
        "  UNK = 2\n",
        "\n",
        "  cn = [BOS]\n",
        "\n",
        "  for word in txtlist[index]:\n",
        "    cn.append(word2int.get(word, UNK))  # 若 word 不存在就用 UNK 取代\n",
        "  cn.append(EOS)\n",
        "\n",
        "  cn = np.asarray(cn)\n",
        "  cn = torch.LongTensor(cn)\n",
        "\n",
        "  return cn"
      ],
      "metadata": {
        "id": "P50Fx9RL2l8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "word2int: key-word, value-times  \n",
        "int2word: key-int, value-time the word appear  \n",
        "(not so good)"
      ],
      "metadata": {
        "id": "Ihy58kBiwh01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PAD_token = 0  # Used for padding short sentences\n",
        "SOS_token = 1  # Start-of-sentence token\n",
        "EOS_token = 2  # End-of-sentence token"
      ],
      "metadata": {
        "id": "KEw0DoNuVaXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root = \"/content/\"\n",
        "dat = \"/content/drive/MyDrive/Gossiping-QA-Dataset.txt\"\n",
        "\n",
        "# def punc(sentence):\n",
        "#   l=[]\n",
        "#   for char in range(len(sentence)):\n",
        "#     if sentence[char] in [\"。\", \"，\", \"；\", \"：\", \"！\", \"？\", \"（\", \"）\", \"【\", \"】\", \"“\", \"”\", \"——\", \"…\", \"／\"]:\n",
        "#       l.append(char)\n",
        "#   num=0\n",
        "#   for i in l:\n",
        "#     sentence = sentence[:i+num] + \" \" + sentence[i+num] + \" \" + sentence[i+num+1:]\n",
        "#     num+=2\n",
        "#   return sentence\n",
        "\n",
        "PAD_token = 0  # Used for padding short sentences\n",
        "SOS_token = 1  # Start-of-sentence token\n",
        "EOS_token = 2  # End-of-sentence token\n",
        "\n",
        "# 建立詞彙表 & 資料、label切分\n",
        "allData = []\n",
        "int2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "word2int = {}\n",
        "PAD_token = 0\n",
        "SOS_token = 1\n",
        "EOS_token = 2\n",
        "vocdict_num = 3\n",
        "\n",
        "with open(dat, \"r\") as file:\n",
        "  for line in file:\n",
        "    combine = list(filter(None, line.replace(\" \", \"\").split('\\t')))\n",
        "    try:\n",
        "      for i in range(2):\n",
        "        for token in combine[i]:\n",
        "          # 小技巧: 唯一性\n",
        "          if token not in word2int:\n",
        "            word2int[token] = vocdict_num\n",
        "            int2word[vocdict_num] = token\n",
        "            vocdict_num += 1\n",
        "          else:\n",
        "            word2int[token] += 1\n",
        "      allData.append(combine)\n",
        "    except:\n",
        "      print(\"unuseful!\")\n",
        "\n",
        "  print(\"data size:\", len(allData))"
      ],
      "metadata": {
        "id": "uJ5eZ0l_a4OZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "排除太多沒看過的字 (if necessary)"
      ],
      "metadata": {
        "id": "4caUtYhg-bp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mintimes = 5\n",
        "# for i in trainData:\n",
        "#   for j in i:\n",
        "#     if word2int[j] < 5:\n",
        "#       traintrim = trainData.pop(i)\n",
        "#       labeltrim = labelData.pop(i)"
      ],
      "metadata": {
        "id": "IhoTjjXO-ZpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "取得隨機 mini-batch 的資料並吐出  \n",
        "input_variable  \n",
        "lengths  \n",
        "target_variable  \n",
        "mask"
      ],
      "metadata": {
        "id": "JVuAGAG8O5t4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Batchdata:\n",
        "  def __init__(self, word2int, trainData, batch_size = 1, n_iteration = 1, labelData = False, method = \"no_onehot\"):\n",
        "    self.word2int = word2int\n",
        "    self.trainData = trainData\n",
        "    self.batch_size = batch_size\n",
        "    self.n_iteration = n_iteration\n",
        "    if not labelData:\n",
        "      self.labelData = labelData\n",
        "    else:\n",
        "      self.labelData = labelData\n",
        "\n",
        "    self.method = method\n",
        "\n",
        "  def rd(self,):\n",
        "    numbers_list = list(range(len(self.trainData)))\n",
        "    rds = random.sample(numbers_list, self.batch_size)\n",
        "    return rds\n",
        "\n",
        "\n",
        "  def padding(self, x, padlen, train=True):\n",
        "\n",
        "    mask = []\n",
        "\n",
        "    for k in range(len(x)):\n",
        "\n",
        "      mk = []\n",
        "      if not train:\n",
        "        for i in range(padlen):\n",
        "          if i < len(x[k]):\n",
        "            mk.append(True)\n",
        "          else:\n",
        "            mk.append(False)\n",
        "\n",
        "      mask.append(mk)\n",
        "\n",
        "      x[k] = np.pad(x[k],\n",
        "      (0, (padlen - len(x[k]))), mode='constant', constant_values = 0)\n",
        "\n",
        "    return x, mask\n",
        "\n",
        "  def Getdata(self,):\n",
        "\n",
        "    batchTrain = []\n",
        "\n",
        "    for i in range(self.n_iteration):\n",
        "\n",
        "      trainBatchi = []\n",
        "      labelBatchi = []\n",
        "      rdidx = self.rd()\n",
        "\n",
        "      if self.method == \"no_onehot\":\n",
        "        trainBatchi.append([transform(self.trainData, word2int, j) for j in rdidx])\n",
        "      else:\n",
        "        trainBatchi.append([self.trainData[j] for j in rdidx])\n",
        "\n",
        "      if not self.labelData:\n",
        "        print(\"evaluate\")\n",
        "      else:\n",
        "        if self.method == \"no_onehot\":\n",
        "          labelBatchi.append([transform(labelData, word2int, j) for j in rdidx])\n",
        "        else:\n",
        "          labelBatchi.append([self.labelData[j] for j in rdidx])\n",
        "\n",
        "        # print(rdidx)\n",
        "        # print(labelBatchi)\n",
        "\n",
        "        labelBatchi = sorted(labelBatchi[0], key=lambda x: x.shape[0], reverse=True)\n",
        "        labelPadlen = max(labelBatchi, key=len).shape[0]\n",
        "        labelBatchi = self.padding(labelBatchi, labelPadlen, False)\n",
        "        mask = np.asarray(labelBatchi[1])\n",
        "        mask = torch.LongTensor(mask)\n",
        "        mask = mask.bool()\n",
        "        mask = torch.BoolTensor(mask)\n",
        "        mask = torch.transpose(mask, 0, 1)\n",
        "        labelBatchi = np.asarray(labelBatchi[0])\n",
        "        labelBatchi = torch.LongTensor(labelBatchi)\n",
        "        labelBatchi = torch.transpose(labelBatchi, 0, 1)\n",
        "\n",
        "\n",
        "      trainBatchi = sorted(trainBatchi[0], key=lambda x: x.shape[0], reverse=True)\n",
        "      length = torch.tensor([len(tensor) for tensor in trainBatchi]).view(-1)\n",
        "      trainPadlen = max(trainBatchi, key=len).shape[0]\n",
        "      trainBatchi = self.padding(trainBatchi, trainPadlen)\n",
        "      trainBatchi = np.asarray(trainBatchi[0])\n",
        "      trainBatchi = torch.LongTensor(trainBatchi)\n",
        "      trainBatchi = torch.transpose(trainBatchi, 0, 1)\n",
        "\n",
        "      try:\n",
        "        batchTrain.append([trainBatchi, length, labelBatchi, mask, mask.shape[0]])\n",
        "      except:\n",
        "        batchTrain.append([trainBatchi, length])\n",
        "\n",
        "    return batchTrain"
      ],
      "metadata": {
        "id": "8CD7qytsCBV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(dat, \"r\") as file:\n",
        "  for line in file:\n",
        "    combine = list(filter(None, line.replace(\" \", \"\").split('\\t')))\n",
        "    try:\n",
        "      for i in range(2):\n",
        "        for token in combine[i]:\n",
        "          # 小技巧: 唯一性\n",
        "          if token not in word2int:\n",
        "            word2int[token] = vocdict_num\n",
        "            int2word[vocdict_num] = token\n",
        "            vocdict_num += 1\n",
        "          else:\n",
        "            word2int[token] += 1\n",
        "      allData.append(combine)\n",
        "    except:\n",
        "      print(\"unuseful!\")\n",
        "\n",
        "  print(\"data size:\", len(allData))\n",
        "allData[:20]\n",
        "trainData = []\n",
        "labelData = []\n",
        "for qa in allData:\n",
        "  trainData.append([token for token in qa[0]])\n",
        "  labelData.append([token for token in qa[1]])\n",
        "trainData[:3], labelData[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4ekHyYaJen6",
        "outputId": "a26fe205-3f32-47b3-b6d8-296fef7080eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unuseful!\n",
            "unuseful!\n",
            "unuseful!\n",
            "data size: 563290\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([['為',\n",
              "   '什',\n",
              "   '麼',\n",
              "   '聖',\n",
              "   '結',\n",
              "   '石',\n",
              "   '會',\n",
              "   '被',\n",
              "   '酸',\n",
              "   '而',\n",
              "   '這',\n",
              "   '群',\n",
              "   '人',\n",
              "   '不',\n",
              "   '會',\n",
              "   '？'],\n",
              "  ['為',\n",
              "   '什',\n",
              "   '麼',\n",
              "   '慶',\n",
              "   '祝',\n",
              "   '2',\n",
              "   '2',\n",
              "   '8',\n",
              "   '會',\n",
              "   '被',\n",
              "   '罵',\n",
              "   '可',\n",
              "   '是',\n",
              "   '慶',\n",
              "   '端',\n",
              "   '午',\n",
              "   '不',\n",
              "   '會',\n",
              "   '？'],\n",
              "  ['有', '沒', '有', '戰', '神', '阿', '瑞', '斯', '的', '八', '卦', '?']],\n",
              " [['質',\n",
              "   '感',\n",
              "   '劇',\n",
              "   '本',\n",
              "   '成',\n",
              "   '員',\n",
              "   '都',\n",
              "   '差',\n",
              "   '很',\n",
              "   '多',\n",
              "   '好',\n",
              "   '嗎',\n",
              "   '不',\n",
              "   '要',\n",
              "   '拿',\n",
              "   '腎',\n",
              "   '結',\n",
              "   '石',\n",
              "   '來',\n",
              "   '污',\n",
              "   '辱',\n",
              "   '這',\n",
              "   '群',\n",
              "   '人',\n",
              "   '\\n'],\n",
              "  ['因',\n",
              "   '為',\n",
              "   '屈',\n",
              "   '原',\n",
              "   '不',\n",
              "   '是',\n",
              "   '台',\n",
              "   '灣',\n",
              "   '人',\n",
              "   '，',\n",
              "   '是',\n",
              "   '楚',\n",
              "   '國',\n",
              "   '人',\n",
              "   '。',\n",
              "   '\\n'],\n",
              "  ['爵',\n",
              "   '士',\n",
              "   '就',\n",
              "   '是',\n",
              "   '阿',\n",
              "   '瑞',\n",
              "   '斯',\n",
              "   '男',\n",
              "   '主',\n",
              "   '角',\n",
              "   '最',\n",
              "   '後',\n",
              "   '死',\n",
              "   '了',\n",
              "   '\\n']])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "n_iteration = 4000\n",
        "\n",
        "batchData = Batchdata(word2int, trainData, batch_size, n_iteration, labelData).Getdata()\n",
        "batchData[0][0].shape, batchData[0][1].shape, batchData[0][2].shape, batchData[0][3].shape, batchData[0][4]"
      ],
      "metadata": {
        "id": "o3BkpVuaExvC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b5100ec-d760-4f78-ec58-54db73877b2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([25, 64]),\n",
              " torch.Size([64]),\n",
              " torch.Size([27, 64]),\n",
              " torch.Size([27, 64]),\n",
              " 27)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "word2int: ont-hot encoding  \n",
        "int2word: same as above  \n",
        "(meaningful)"
      ],
      "metadata": {
        "id": "_nTo4W6AwhEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dat = \"/content/drive/MyDrive/Gossiping-QA-Dataset.txt\"\n",
        "\n",
        "PAD_token = 0\n",
        "SOS_token = 1\n",
        "EOS_token = 2\n",
        "\n",
        "voclist={\"PAD_token\": 0}  # 查找速度較快\n",
        "\n",
        "with open(dat, \"r\") as file:\n",
        "  for line in file:\n",
        "    for j in line:\n",
        "      if j not in voclist:\n",
        "        voclist[j] = 0"
      ],
      "metadata": {
        "id": "S1Bt_JOvxDrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "voc_list = []\n",
        "for i, j in voclist.items():\n",
        "  voc_list.append(i)"
      ],
      "metadata": {
        "id": "68ntKqYuGXIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GetTrainLable:\n",
        "  def __init__(self, allData, voc_list):\n",
        "    self.allData = allData\n",
        "    self.voc_list = voc_list\n",
        "\n",
        "  def to_binary(self, num, l):\n",
        "    if num == 0:\n",
        "      return l\n",
        "    quo = num // 2\n",
        "    rem = num % 2\n",
        "    l.append(rem)\n",
        "    return self.to_binary(quo, l)\n",
        "\n",
        "  def ohencoding(self,):\n",
        "\n",
        "    wordint = {}\n",
        "    maxlen = int(np.log2(len(self.voc_list))) + 1\n",
        "\n",
        "    for i, j in enumerate(self.voc_list):\n",
        "      l = self.to_binary(i, [])\n",
        "      comp = [0 for _ in range(maxlen - len(l))]\n",
        "      wordint[j] = np.asarray(l + comp + [1])\n",
        "      wordint[j] = torch.LongTensor(wordint[j])\n",
        "\n",
        "    self.voc_dict = wordint\n",
        "\n",
        "\n",
        "  def Getdata(self,):\n",
        "\n",
        "    self.ohencoding()\n",
        "\n",
        "    trainData = []\n",
        "    labelData = []\n",
        "    for qa in self.allData:\n",
        "      trainData.append(torch.cat([self.voc_dict[token] for token in qa[0]], dim=0))\n",
        "      labelData.append(torch.cat([self.voc_dict[token] for token in qa[1]], dim=0))\n",
        "    return trainData, labelData"
      ],
      "metadata": {
        "id": "d3q5VUO01JXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TrainLabel = GetTrainLable(allData, voc_list)\n",
        "trainData, labelData = TrainLabel.Getdata()\n",
        "voc_dict = TrainLabel.voc_dict"
      ],
      "metadata": {
        "id": "9Sgb1e_7BAJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "voc_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_0Bd4OsEYXL",
        "outputId": "f6a3078f-7229-42d6-f180-5e5ba92e988d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'PAD_token': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '為': tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '什': tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '麼': tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " ' ': tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '聖': tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '結': tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '石': tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '會': tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '被': tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '酸': tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '而': tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '這': tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '群': tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '人': tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '不': tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '？': tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '\\t': tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '質': tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '感': tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '劇': tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '本': tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '成': tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '員': tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '都': tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '差': tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '很': tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '多': tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '好': tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '嗎': tensor([1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '要': tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '拿': tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '腎': tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '來': tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '污': tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '辱': tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '\\n': tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '慶': tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '祝': tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '2': tensor([1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '8': tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '罵': tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '可': tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '是': tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '端': tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '午': tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '因': tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '屈': tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '原': tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '台': tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '灣': tensor([0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '，': tensor([1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '楚': tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '國': tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '。': tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '有': tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '沒': tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '戰': tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '神': tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '阿': tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '瑞': tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '斯': tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '的': tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '八': tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '卦': tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '?': tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '爵': tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '士': tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '就': tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '男': tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '主': tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '角': tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '最': tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '後': tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '死': tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '了': tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '理': tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '論': tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '與': tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '實': tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '務': tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '脫': tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '節': tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '系': tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '哪': tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '個': tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '.': tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '你': tensor([1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '問': tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '簡': tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '單': tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " 'P': tensor([1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " 'T': tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '看': tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '棒': tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '球': tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '肥': tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '宅': tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '才': tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '\\u3000': tensor([1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '壘': tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '一': tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '堆': tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '胖': tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '子': tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '達': tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '摩': tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '祖': tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '師': tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '傳': tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '那': tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '從': tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '頭': tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '到': tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '尾': tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '動': tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '(': tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '別': tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '他': tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '題': tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '3': tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " 'D': tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '小': tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '畫': tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '家': tensor([0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '當': tensor([1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '對': tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '天': tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " '龍': tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '說': tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '宜': tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '蘭': tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '4': tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '南': tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '部': tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '還': tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '４': tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '東': tensor([1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '事': tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '機': tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '車': tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '推': tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '出': tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " 'u': tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " 'b': tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " 'e': tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " 'r': tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '或': tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '計': tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '程': tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '怎': tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '樣': tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '載': tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '痛': tensor([1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '苦': tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '中': tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '邦': tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '世': tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '貿': tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '跳': tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '樓': tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '曾': tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '經': tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '過': tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '全': tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '第': tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '高': tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '惜': tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '年': tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '抽': tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '海': tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '陸': tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '笑': tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '娘': tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '炮': tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '兵': tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '啦': tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '下': tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '基': tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '地': tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '常': tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '態': tensor([1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '滿': tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '文': tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '美': tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '考': tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " 'G': tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " 'R': tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " 'E': tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " 'M': tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " 'A': tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '深': tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '英': tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '初': tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '太': tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '陽': tensor([1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '花': tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '留': tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '垃': tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '圾': tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '也': tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '捧': tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '我': tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '時': tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '候': tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '去': tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '像': tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '白': tensor([1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '界': tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '上': tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '難': tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '聽': tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '歌': tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '首': tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '華': tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '民': tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '根': tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '山': tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '奏': tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '旗': tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '比': tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '較': tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '合': tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '適': tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '雙': tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '筆': tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '電': tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '底': tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '以': tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '帶': tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '進': tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '星': tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '巴': tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '克': tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '十': tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '幾': tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '萬': tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '用': tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '卡': tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '皮': tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '箱': tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '裝': tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '水': tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '冷': tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '歐': tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '妮': tensor([1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '友': tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '她': tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '爸': tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '覺': tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '媽': tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '愛': tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '揍': tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '兔': tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '布': tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " '偶': tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1]),\n",
              " 'i': tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " 'm': tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " 'g': tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " 'ㄉ': tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '浪': tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '費': tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '空': tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '間': tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '潮': tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '物': tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '搖': tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '噱': tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '0': tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '至': tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '少': tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '1': tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '夜': tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '賣': tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '打': tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '手': tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '槍': tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '吧': tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '+': tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '6': tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '方': tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '話': tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '際': tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '盜': tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '懶': tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '集': tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '團': tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '盯': tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '何': tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '北': tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '韓': tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '走': tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '向': tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '新': tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '加': tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '坡': tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '模': tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '式': tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '又': tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '缺': tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '所': tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '超': tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '貴': tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '設': tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '廠': tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '速': tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '度': tensor([0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '跟': tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '五': tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '月': tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " 'l': tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " 'v': tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '聊': tensor([0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '室': tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '滑': tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '快': tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '甜': tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '柿': tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '起': tensor([0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '轉': tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '兼': tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '具': tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '麗': tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '極': tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '致': tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '次': tensor([1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '定': tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '們': tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '更': tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '厲': tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '害': tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '呢': tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '茶': tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '吃': tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '飽': tensor([0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '先': tensor([1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '鮮': tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '生': tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '魚': tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '片': tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '然': tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '熱': tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '炸': tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '飲': tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '料': tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '喝': tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '兇': tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '女': tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '胸': tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '醜': tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '凶': tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '身': tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '專': tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '業': tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '把': tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '西': tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '隨': tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '攜': tensor([1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '脂': tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '肪': tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '放': tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '在': tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '運': tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '只': tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '能': tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '靠': tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '住': tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '語': tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '貓': tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '早': tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '金': tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '裡': tensor([1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '形': tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '容': tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '詞': tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '啊': tensor([1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '日': tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '肉': tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '音': tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '譯': tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '公': tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '站': tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '統': tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '准': tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1]),\n",
              " '得': tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '政': tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '府': tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '交': tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '作': tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '準': tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '錯': tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '大': tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '學': tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '排': tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '名': tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '醫': tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '兩': tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '類': tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '分': tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '數': tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '妹': tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '請': tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '共': tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '三': tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '紙': tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '條': tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '給': tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '正': tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '寫': tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '票': tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '字': tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '額': tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '發': tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '簽': tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '磁': tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '碟': tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '槽': tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '5': tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " 'B': tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '菜': tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '逼': tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '吼': tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " 'V': tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '算': tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '翻': tensor([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '紅': tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '體': tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '前': tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '9': tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '贏': tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '重': tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '力': tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '繞': tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '著': tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '做': tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '圓': tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '周': tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '書': tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '細': tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '嚼': tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '慢': tensor([0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '嚥': tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '兒': tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '格': tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '泰': tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '越': tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '誇': tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '張': tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '抄': tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '外': tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " 'K': tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '戲': tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '鐘': tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '島': tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '法': tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '侵': tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '略': tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '滅': tensor([1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '需': tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '認': tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '真': tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '信': tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '靈': tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '媒': tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '寵': tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '母': tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '咪': tensor([0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '門': tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '溝': tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '通': tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '如': tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '拒': tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '絕': tensor([0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '喜': tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '歡': tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '：': tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '（': tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '嘴': tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '老': tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '婆': tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '安': tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '型': tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '現': tensor([0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '律': tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '無': tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '罪': tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '代': tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '表': tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '補': tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '習': tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '班': tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '狼': tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '辜': tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '呵': tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '鬼': tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '智': tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '未': tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '開': tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '髮': tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '禿': tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '甲': tensor([0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '二': tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '/': tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '假': tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '!': tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '森': tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '量': tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '哥': tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '扁': tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '鵲': tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '陀': tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '李': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1]),\n",
              " '珍': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '誰': tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '柯': tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '胡': tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '夫': tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '勝': tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '薦': tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '玖': tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '壹': tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '參': tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '央': tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '視': tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '春': tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '晚': tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '該': tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '撈': tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '告': tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '易': tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '功': tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '直': tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '完': tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '今': tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '捷': tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '！': tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '搶': tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '7': tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '篇': tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '臭': tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '約': tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '～': tensor([1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '拖': tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '卻': tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '婚': tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '渣': tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '執': tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '迷': tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '悟': tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '改': tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '變': tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '自': tensor([1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '己': tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " 'H': tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " 't': tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " 'y': tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '麥': tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '勞': tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '樂': tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '送': tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " 'p': tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '幣': tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '風': tensor([0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '演': tensor([1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '其': tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '網': tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '咖': tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '店': tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '廢': tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '接': tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '雞': tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '塊': tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '塞': tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " 'X': tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '輩': tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '佩': tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '服': tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '崇': tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '拜': tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '王': tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '永': tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '蓋': tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '院': tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '校': tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '想': tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '騙': tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '錢': tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '鄉': tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '總': tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '欺': tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '負': tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " 'Q': tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '果': tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '選': tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '科': tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '爾': tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '茲': tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '退': tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '堂': tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '始': tensor([1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '點': tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '素': tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '整': tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '掉': tensor([1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '剛': tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '玩': tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '隻': tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '騎': tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '姊': tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '觀': tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '穿': tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " 'ㄌ': tensor([1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '知': tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '道': tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '屌': tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '飛': tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '獨': tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '仙': tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '辦': tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '野': tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '採': tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '蟲': tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '幹': tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '輪': tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '灌': tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '籃': tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '利': tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '組': tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '零': tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '再': tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '同': tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '步': tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '率': tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '練': tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '滸': tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '漢': tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '魯': tensor([0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '林': tensor([1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '沖': tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '食': tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1]),\n",
              " '豬': tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '腸': tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '面': tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '糞': tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '便': tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '味': tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '火': tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '德': tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '鍋': tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '唱': tensor([1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '睡': tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '妳': tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '嗨': tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '志': tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '姻': tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '近': tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '親': tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '元': tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '犯': tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '責': tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '酢': tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '已': tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '郎': tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '凍': tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '耶': tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '益': tensor([1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '賺': tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '包': tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '裹': tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '搭': tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '廉': tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '航': tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '丟': tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '臉': tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '般': tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '買': tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '版': tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '反': tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '指': tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '標': tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '內': tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '行': tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '猴': tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '菇': tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '口': tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '竟': tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '釋': tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '憲': tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " ',': tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '萌': tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '砸': tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '爛': tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '管': tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '性': tensor([1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '戀': tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '伊': tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '湄': tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '黑': tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '和': tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '弟': tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '之': tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '癖': tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '軍': tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '回': tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '私': tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '立': tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '余': tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '鐵': tensor([1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '雄': tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '喔': tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '歲': tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '讀': tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '索': tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '瑪': tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '蛾': tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '拉': tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '毀': tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '狂': tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '情': tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '%': tensor([1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '圖': tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '故': tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '抒': tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '限': tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '崩': tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '潰': tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '心': tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '聞': tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '由': tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '氣': tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '肛': tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '列': tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '入': tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '教': tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '育': tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '關': tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '注': tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '平': tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '長': tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '矮': tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '傑': tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '尼': tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '族': tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '龜': tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '希': tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '望': tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '郭': tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '銘': tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '泓': tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '係': tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '升': tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '馬': tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '路': tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '處': tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '每': tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '鳥': tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '但': tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '受': tensor([1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '迎': tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '屁': tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '米': tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '狗': tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '戟': tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '棄': tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '追': tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '膩': tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '除': tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '宮': tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
              " '等': tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '啥': tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " 'c': tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " 'a': tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " 'd': tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '溫': tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '馨': tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '>': tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '<': tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '各': tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '位': tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '尊': tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '善': tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '烏': tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '骨': tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '硬': tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '詛': tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '咒': tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '撞': tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '掛': tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '拍': tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '影': tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '餐': tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '鎂': tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '耳': tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '突': tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '鳴': tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '聾': tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '@': tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '徵': tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '兆': tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '邊': tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '緣': tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '收': tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '震': tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '警': tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '報': tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '跑': tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '窗': tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '刪': tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '眼': tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '睛': tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " 'O': tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '種': tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '勇': tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '預': tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '訊': tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '遠': tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '跌': tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '倒': tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '傷': tensor([0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '腦': tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '袋': tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '憐': tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '趴': tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '嘛': tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '牽': tensor([0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '護': tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '古': tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '梗': tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '射': tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '朋': tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '櫻': tensor([0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '朱': tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '優': tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '☆': tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '災': tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '杯': tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '摔': tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '破': tensor([1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '左': tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '營': tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '象': tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '局': tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " 'S': tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '某': tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '示': tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '駭': tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '客': tensor([0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '攻': tensor([1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '擊': tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '油': tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '滴': tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '驗': tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '擬': tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '賽': tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '討': tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '厭': tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '核': tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '欠': tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '保': tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '值': tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '研': tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '究': tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '董': tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '、': tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '錶': tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '相': tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '提': tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '案': tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '價': tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '落': tensor([1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '談': tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '闊': tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '喪': tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '屍': tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '展': tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '訓': tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '些': tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '危': tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '險': tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '工': tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '線': tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '特': tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '隊': tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '密': tensor([1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '喊': tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '備': tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '圈': tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '講': tensor([1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '睜': tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '瞎': tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '哲': tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '源': tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '哭': tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '違': tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '右': tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '踢': tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
              " '聯': tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '霸': tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '館': tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '黎': tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '明': tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '嘻': tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '障': tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '姆': tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '滾': tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '喇': tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " 'F': tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '乳': tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '解': tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '禁': tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '狀': tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '偉': tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '典': tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '纏': tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '健': tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '康': tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '連': tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '目': tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '活': tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '久': tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '遊': tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '漫': tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '壓': tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '支': tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '持': tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '黨': tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '互': tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '權': tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '修': tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '場': tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '景': tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '砍': tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '伙': tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '佈': tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '置': tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '懂': tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '朝': tensor([0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '爽': tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '休': tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '息': tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '秘': tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '符': tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '殊': tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '召': tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '喚': tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '陣': tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '蘿': tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '莉': tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '桶': tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '煙': tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '捐': tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '救': tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '照': tensor([0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '吸': tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '洛': tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '鹼': tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '楓': tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '谷': tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '派': tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '勢': tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '者': tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '圍': tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '困': tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '遇': tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '噴': tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '嚏': tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '肩': tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '膀': tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '葛': tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '幼': tensor([1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1]),\n",
              " 'o': tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1]),\n",
              " \"'\": tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '_': tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '緊': tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '舒': tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '守': tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '街': tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '亭': tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '病': tensor([0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '毒': tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '亡': tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '興': tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '甚': tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '叫': tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '臺': tensor([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '督': tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '農': tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '-': tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '讚': tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '醃': tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '鮭': tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '四': tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '罐': tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '擔': tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '停': tensor([0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '檔': tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1]),\n",
              " 'N': tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '煞': tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '腳': tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1]),\n",
              " '確': tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1]),\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainData[100].shape, labelData[100].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gEqoi95EEnb",
        "outputId": "6fe88b8b-4350-4391-b5a6-2d6f85072404"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([56]), torch.Size([238]))"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "把字數太多的拿掉"
      ],
      "metadata": {
        "id": "cOth1u4QO5CD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mm = 0\n",
        "mean = 0\n",
        "dist = []\n",
        "for i,j in enumerate(trainData):\n",
        "  dist.append(len(j))\n",
        "  mean += len(j)\n",
        "  if len(j) > mm:\n",
        "    mm = len(j)\n",
        "print(mean/len(trainData))\n",
        "print(mm)\n",
        "plt.hist(dist)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "gFNB4ZnjK8yv",
        "outputId": "594e8e8f-101d-415c-8225-ea81f46c7ddb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "182.35126133962967\n",
            "644\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_legth = 400\n",
        "for i, j in enumerate(trainData):\n",
        "  if len(j)>400:\n",
        "    trainData.pop(i)\n",
        "    labelData.pop(i)"
      ],
      "metadata": {
        "id": "Erg_MDweON_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "n_iteration = 10000\n",
        "\n",
        "batchTrain = Batchdata(\"pass\", trainData, batch_size, n_iteration, labelData, method = \"onehot\").Getdata()\n",
        "batchData[0][0].shape, batchData[0][1].shape, batchData[0][2].shape, batchData[0][3].shape, batchData[0][4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqwUdr9IGH-B",
        "outputId": "3be02576-b4a3-45b7-df06-f2ae0d33feb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([25, 64]),\n",
              " torch.Size([64]),\n",
              " torch.Size([27, 64]),\n",
              " torch.Size([27, 64]),\n",
              " 27)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "模型"
      ],
      "metadata": {
        "id": "rcN06Nt9UU3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = embedding\n",
        "\n",
        "        # Initialize GRU; the input_size and hidden_size parameters are both set to 'hidden_size'\n",
        "        #   because our input size is a word embedding with number of features == hidden_size\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
        "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
        "\n",
        "    def forward(self, input_seq, input_lengths, hidden=None):\n",
        "        # Convert word indexes to embeddings\n",
        "        embedded = self.embedding(input_seq)\n",
        "        # Pack padded batch of sequences for RNN module\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
        "        # Forward pass through GRU\n",
        "        outputs, hidden = self.gru(packed, hidden)\n",
        "        # Unpack padding\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "        # Sum bidirectional GRU outputs\n",
        "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
        "        # Return output and final hidden state\n",
        "        return outputs, hidden"
      ],
      "metadata": {
        "id": "graY3lll_QC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Luong attention layer\n",
        "class Attn(nn.Module):\n",
        "    def __init__(self, method, hidden_size):\n",
        "        super(Attn, self).__init__()\n",
        "        self.method = method\n",
        "        if self.method not in ['dot', 'general', 'concat']:\n",
        "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
        "        self.hidden_size = hidden_size\n",
        "        if self.method == 'general':\n",
        "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
        "        elif self.method == 'concat':\n",
        "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
        "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
        "\n",
        "    def dot_score(self, hidden, encoder_output):\n",
        "        return torch.sum(hidden * encoder_output, dim=2)\n",
        "\n",
        "    def general_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(encoder_output)\n",
        "        return torch.sum(hidden * energy, dim=2)\n",
        "\n",
        "    def concat_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
        "        return torch.sum(self.v * energy, dim=2)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # Calculate the attention weights (energies) based on the given method\n",
        "        if self.method == 'general':\n",
        "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'concat':\n",
        "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'dot':\n",
        "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
        "\n",
        "        # Transpose max_length and batch_size dimensions\n",
        "        attn_energies = attn_energies.t()\n",
        "\n",
        "        # Return the softmax normalized probability scores (with added dimension)\n",
        "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
      ],
      "metadata": {
        "id": "jc_f40-VyFUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LuongAttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
        "        super(LuongAttnDecoderRNN, self).__init__()\n",
        "\n",
        "        # Keep for reference\n",
        "        self.attn_model = attn_model\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Define layers\n",
        "        self.embedding = embedding\n",
        "        self.embedding_dropout = nn.Dropout(dropout)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
        "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        self.attn = Attn(attn_model, hidden_size)\n",
        "\n",
        "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
        "        # Note: we run this one step (word) at a time\n",
        "        # Get embedding of current input word\n",
        "        embedded = self.embedding(input_step)\n",
        "        embedded = self.embedding_dropout(embedded)\n",
        "        # Forward through unidirectional GRU\n",
        "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
        "        # Calculate attention weights from the current GRU output\n",
        "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
        "        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
        "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
        "        # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
        "        rnn_output = rnn_output.squeeze(0)\n",
        "        context = context.squeeze(1)\n",
        "        concat_input = torch.cat((rnn_output, context), 1)\n",
        "        concat_output = torch.tanh(self.concat(concat_input))\n",
        "        # Predict next word using Luong eq. 6\n",
        "        output = self.out(concat_output)\n",
        "        output = F.softmax(output, dim=1)\n",
        "        # Return output and final hidden state\n",
        "        return output, hidden"
      ],
      "metadata": {
        "id": "-b5UNnrPGTtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def maskNLLLoss(inp, target, mask):\n",
        "    nTotal = mask.sum()\n",
        "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
        "    loss = crossEntropy.masked_select(mask).mean()\n",
        "    loss = loss.to(device)\n",
        "    return loss, nTotal.item()"
      ],
      "metadata": {
        "id": "fktx9Q7EMwwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n",
        "          encoder_optimizer, decoder_optimizer, batch_size, clip):\n",
        "\n",
        "    # Zero gradients\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    # Set device options\n",
        "    input_variable = input_variable.to(device)\n",
        "    target_variable = target_variable.to(device)\n",
        "    mask = mask.to(device)\n",
        "    # Lengths for RNN packing should always be on the CPU\n",
        "    lengths = lengths.to(\"cpu\")\n",
        "\n",
        "    # Initialize variables\n",
        "    loss = 0\n",
        "    print_losses = []\n",
        "    n_totals = 0\n",
        "\n",
        "    # Forward pass through encoder\n",
        "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
        "\n",
        "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
        "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
        "    decoder_input = decoder_input.to(device)\n",
        "\n",
        "    # Set initial decoder hidden state to the encoder's final hidden state\n",
        "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "\n",
        "    # Determine if we are using teacher forcing this iteration\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    # Forward batch of sequences through decoder one time step at a time\n",
        "    if use_teacher_forcing:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            # Teacher forcing: next input is current target\n",
        "            decoder_input = target_variable[t].view(1, -1)\n",
        "            # Calculate and accumulate loss\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "    else:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            # No teacher forcing: next input is decoder's own current output\n",
        "            _, topi = decoder_output.topk(1)\n",
        "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
        "            decoder_input = decoder_input.to(device)\n",
        "            # Calculate and accumulate loss\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "\n",
        "    # Perform backpropagation\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip gradients: gradients are modified in place\n",
        "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
        "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
        "\n",
        "    # Adjust model weights\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return sum(print_losses) / n_totals"
      ],
      "metadata": {
        "id": "9MgHjN-nM3i7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainIters(model_name, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename, batchTrain, voc_dict):\n",
        "\n",
        "    # Load batches for each iteration\n",
        "    training_batches = [batchTrain[i] for i in range(n_iteration)]\n",
        "\n",
        "    # Initializations\n",
        "    print('Initializing ...')\n",
        "    start_iteration = 1\n",
        "    print_loss = 0\n",
        "    # if loadFilename:\n",
        "    #     start_iteration = checkpoint['iteration'] + 1\n",
        "\n",
        "    # Training loop\n",
        "    print(\"Training...\")\n",
        "    for iteration in range(start_iteration, n_iteration + 1):\n",
        "        training_batch = training_batches[iteration - 1]\n",
        "        # Extract fields from batch\n",
        "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
        "\n",
        "        # Run a training iteration with batch\n",
        "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
        "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
        "        print_loss += loss\n",
        "\n",
        "        # Print progress\n",
        "        if iteration % print_every == 0:\n",
        "            print_loss_avg = print_loss / print_every\n",
        "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
        "            print_loss = 0\n",
        "\n",
        "        # Save checkpoint\n",
        "        if (iteration % save_every == 0):\n",
        "            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
        "            if not os.path.exists(directory):\n",
        "                os.makedirs(directory)\n",
        "            torch.save({\n",
        "                'iteration': iteration,\n",
        "                'en': encoder.state_dict(),\n",
        "                'de': decoder.state_dict(),\n",
        "                'en_opt': encoder_optimizer.state_dict(),\n",
        "                'de_opt': decoder_optimizer.state_dict(),\n",
        "                'loss': loss,\n",
        "                'embedding': embedding.state_dict(),\n",
        "                'voc_dict' : voc_dict\n",
        "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))"
      ],
      "metadata": {
        "id": "es1WUjvgM5_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "初始化超參數並訓練模型"
      ],
      "metadata": {
        "id": "3tgd0nFcosSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure models\n",
        "corpus_name = \"Gossiping-QA-Dataset\"\n",
        "model_name = 'chatbot'\n",
        "attn_model = 'dot'\n",
        "#``attn_model = 'general'``\n",
        "#``attn_model = 'concat'``\n",
        "hidden_size = 50\n",
        "encoder_n_layers = 2\n",
        "decoder_n_layers = 2\n",
        "dropout = 0.1\n",
        "# batch_size = 16\n",
        "\n",
        "# Set checkpoint to load from; set to None if starting from scratch\n",
        "loadFilename = None\n",
        "checkpoint_iter = 4000\n",
        "\n",
        "save_dir = \"/content/drive/MyDrive\"\n"
      ],
      "metadata": {
        "id": "891KHgWpNCyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Building encoder and decoder ...')\n",
        "# Initialize word embeddings\n",
        "embedding = nn.Embedding(len(voc_dict), hidden_size)\n",
        "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
        "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, len(voc_dict), decoder_n_layers, dropout)\n",
        "encoder = encoder.to(device)\n",
        "decoder = decoder.to(device)\n",
        "print('Models built and ready to go!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79rsUz20NHXz",
        "outputId": "38da9af0-5066-4431-f046-d2256a661ffb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building encoder and decoder ...\n",
            "Models built and ready to go!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure training/optimization\n",
        "clip = 50.0\n",
        "teacher_forcing_ratio = 1.0\n",
        "learning_rate = 0.0001\n",
        "decoder_learning_ratio = 5.0\n",
        "# n_iteration = 10000\n",
        "print_every = 1\n",
        "save_every = 100\n",
        "\n",
        "# Ensure dropout layers are in train mode\n",
        "encoder.train()\n",
        "decoder.train()\n",
        "\n",
        "# Initialize optimizers\n",
        "print('Building optimizers ...')\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "\n",
        "# If you have CUDA, configure CUDA to call\n",
        "for state in encoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.cuda()\n",
        "\n",
        "for state in decoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.cuda()\n",
        "\n",
        "# Run training iterations\n",
        "print(\"Starting Training!\")\n",
        "trainIters(model_name, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
        "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
        "           print_every, save_every, clip, corpus_name, loadFilename, batchTrain, voc_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8e78w4b-NKi9",
        "outputId": "f47f6c37-7107-4792-ad9e-61ae328851d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building optimizers ...\n",
            "Starting Training!\n",
            "Initializing ...\n",
            "Training...\n",
            "Iteration: 1; Percent complete: 0.0%; Average loss: 8.9172\n",
            "Iteration: 2; Percent complete: 0.0%; Average loss: 8.8501\n",
            "Iteration: 3; Percent complete: 0.0%; Average loss: 8.7852\n",
            "Iteration: 4; Percent complete: 0.0%; Average loss: 8.7198\n",
            "Iteration: 5; Percent complete: 0.1%; Average loss: 8.6547\n",
            "Iteration: 6; Percent complete: 0.1%; Average loss: 8.5886\n",
            "Iteration: 7; Percent complete: 0.1%; Average loss: 8.5114\n",
            "Iteration: 8; Percent complete: 0.1%; Average loss: 8.4558\n",
            "Iteration: 9; Percent complete: 0.1%; Average loss: 8.3746\n",
            "Iteration: 10; Percent complete: 0.1%; Average loss: 8.3139\n",
            "Iteration: 11; Percent complete: 0.1%; Average loss: 8.2275\n",
            "Iteration: 12; Percent complete: 0.1%; Average loss: 8.1407\n",
            "Iteration: 13; Percent complete: 0.1%; Average loss: 8.0467\n",
            "Iteration: 14; Percent complete: 0.1%; Average loss: 7.9499\n",
            "Iteration: 15; Percent complete: 0.1%; Average loss: 7.8375\n",
            "Iteration: 16; Percent complete: 0.2%; Average loss: 7.7673\n",
            "Iteration: 17; Percent complete: 0.2%; Average loss: 7.6676\n",
            "Iteration: 18; Percent complete: 0.2%; Average loss: 7.5546\n",
            "Iteration: 19; Percent complete: 0.2%; Average loss: 7.4415\n",
            "Iteration: 20; Percent complete: 0.2%; Average loss: 7.3584\n",
            "Iteration: 21; Percent complete: 0.2%; Average loss: 7.2698\n",
            "Iteration: 22; Percent complete: 0.2%; Average loss: 7.1648\n",
            "Iteration: 23; Percent complete: 0.2%; Average loss: 7.0423\n",
            "Iteration: 24; Percent complete: 0.2%; Average loss: 6.9643\n",
            "Iteration: 25; Percent complete: 0.2%; Average loss: 6.8520\n",
            "Iteration: 26; Percent complete: 0.3%; Average loss: 6.7381\n",
            "Iteration: 27; Percent complete: 0.3%; Average loss: 6.6359\n",
            "Iteration: 28; Percent complete: 0.3%; Average loss: 6.5201\n",
            "Iteration: 29; Percent complete: 0.3%; Average loss: 6.4391\n",
            "Iteration: 30; Percent complete: 0.3%; Average loss: 6.3018\n",
            "Iteration: 31; Percent complete: 0.3%; Average loss: 6.2160\n",
            "Iteration: 32; Percent complete: 0.3%; Average loss: 6.1149\n",
            "Iteration: 33; Percent complete: 0.3%; Average loss: 6.0308\n",
            "Iteration: 34; Percent complete: 0.3%; Average loss: 5.9218\n",
            "Iteration: 35; Percent complete: 0.4%; Average loss: 5.8469\n",
            "Iteration: 36; Percent complete: 0.4%; Average loss: 5.7724\n",
            "Iteration: 37; Percent complete: 0.4%; Average loss: 5.6318\n",
            "Iteration: 38; Percent complete: 0.4%; Average loss: 5.5659\n",
            "Iteration: 39; Percent complete: 0.4%; Average loss: 5.4644\n",
            "Iteration: 40; Percent complete: 0.4%; Average loss: 5.3695\n",
            "Iteration: 41; Percent complete: 0.4%; Average loss: 5.3064\n",
            "Iteration: 42; Percent complete: 0.4%; Average loss: 5.2181\n",
            "Iteration: 43; Percent complete: 0.4%; Average loss: 5.1438\n",
            "Iteration: 44; Percent complete: 0.4%; Average loss: 5.0811\n",
            "Iteration: 45; Percent complete: 0.4%; Average loss: 5.0005\n",
            "Iteration: 46; Percent complete: 0.5%; Average loss: 4.9210\n",
            "Iteration: 47; Percent complete: 0.5%; Average loss: 4.8399\n",
            "Iteration: 48; Percent complete: 0.5%; Average loss: 4.7805\n",
            "Iteration: 49; Percent complete: 0.5%; Average loss: 4.7527\n",
            "Iteration: 50; Percent complete: 0.5%; Average loss: 4.6242\n",
            "Iteration: 51; Percent complete: 0.5%; Average loss: 4.5598\n",
            "Iteration: 52; Percent complete: 0.5%; Average loss: 4.4897\n",
            "Iteration: 53; Percent complete: 0.5%; Average loss: 4.4101\n",
            "Iteration: 54; Percent complete: 0.5%; Average loss: 4.3438\n",
            "Iteration: 55; Percent complete: 0.5%; Average loss: 4.2825\n",
            "Iteration: 56; Percent complete: 0.6%; Average loss: 4.2085\n",
            "Iteration: 57; Percent complete: 0.6%; Average loss: 4.1556\n",
            "Iteration: 58; Percent complete: 0.6%; Average loss: 4.0895\n",
            "Iteration: 59; Percent complete: 0.6%; Average loss: 4.0041\n",
            "Iteration: 60; Percent complete: 0.6%; Average loss: 3.9697\n",
            "Iteration: 61; Percent complete: 0.6%; Average loss: 3.9082\n",
            "Iteration: 62; Percent complete: 0.6%; Average loss: 3.8392\n",
            "Iteration: 63; Percent complete: 0.6%; Average loss: 3.7177\n",
            "Iteration: 64; Percent complete: 0.6%; Average loss: 3.7269\n",
            "Iteration: 65; Percent complete: 0.7%; Average loss: 3.7007\n",
            "Iteration: 66; Percent complete: 0.7%; Average loss: 3.6160\n",
            "Iteration: 67; Percent complete: 0.7%; Average loss: 3.5299\n",
            "Iteration: 68; Percent complete: 0.7%; Average loss: 3.5146\n",
            "Iteration: 69; Percent complete: 0.7%; Average loss: 3.4442\n",
            "Iteration: 70; Percent complete: 0.7%; Average loss: 3.4098\n",
            "Iteration: 71; Percent complete: 0.7%; Average loss: 3.3347\n",
            "Iteration: 72; Percent complete: 0.7%; Average loss: 3.2874\n",
            "Iteration: 73; Percent complete: 0.7%; Average loss: 3.2405\n",
            "Iteration: 74; Percent complete: 0.7%; Average loss: 3.2339\n",
            "Iteration: 75; Percent complete: 0.8%; Average loss: 3.1267\n",
            "Iteration: 76; Percent complete: 0.8%; Average loss: 3.0865\n",
            "Iteration: 77; Percent complete: 0.8%; Average loss: 3.0293\n",
            "Iteration: 78; Percent complete: 0.8%; Average loss: 2.9903\n",
            "Iteration: 79; Percent complete: 0.8%; Average loss: 2.9235\n",
            "Iteration: 80; Percent complete: 0.8%; Average loss: 2.8862\n",
            "Iteration: 81; Percent complete: 0.8%; Average loss: 2.8530\n",
            "Iteration: 82; Percent complete: 0.8%; Average loss: 2.7973\n",
            "Iteration: 83; Percent complete: 0.8%; Average loss: 2.7212\n",
            "Iteration: 84; Percent complete: 0.8%; Average loss: 2.7244\n",
            "Iteration: 85; Percent complete: 0.9%; Average loss: 2.6584\n",
            "Iteration: 86; Percent complete: 0.9%; Average loss: 2.5838\n",
            "Iteration: 87; Percent complete: 0.9%; Average loss: 2.5980\n",
            "Iteration: 88; Percent complete: 0.9%; Average loss: 2.5357\n",
            "Iteration: 89; Percent complete: 0.9%; Average loss: 2.4722\n",
            "Iteration: 90; Percent complete: 0.9%; Average loss: 2.4030\n",
            "Iteration: 91; Percent complete: 0.9%; Average loss: 2.3898\n",
            "Iteration: 92; Percent complete: 0.9%; Average loss: 2.3323\n",
            "Iteration: 93; Percent complete: 0.9%; Average loss: 2.2568\n",
            "Iteration: 94; Percent complete: 0.9%; Average loss: 2.2687\n",
            "Iteration: 95; Percent complete: 0.9%; Average loss: 2.2621\n",
            "Iteration: 96; Percent complete: 1.0%; Average loss: 2.2117\n",
            "Iteration: 97; Percent complete: 1.0%; Average loss: 2.1729\n",
            "Iteration: 98; Percent complete: 1.0%; Average loss: 2.1264\n",
            "Iteration: 99; Percent complete: 1.0%; Average loss: 2.0757\n",
            "Iteration: 100; Percent complete: 1.0%; Average loss: 2.0409\n",
            "Iteration: 101; Percent complete: 1.0%; Average loss: 1.9491\n",
            "Iteration: 102; Percent complete: 1.0%; Average loss: 2.0182\n",
            "Iteration: 103; Percent complete: 1.0%; Average loss: 1.9970\n",
            "Iteration: 104; Percent complete: 1.0%; Average loss: 1.9357\n",
            "Iteration: 105; Percent complete: 1.1%; Average loss: 1.8777\n",
            "Iteration: 106; Percent complete: 1.1%; Average loss: 1.8246\n",
            "Iteration: 107; Percent complete: 1.1%; Average loss: 1.7862\n",
            "Iteration: 108; Percent complete: 1.1%; Average loss: 1.7882\n",
            "Iteration: 109; Percent complete: 1.1%; Average loss: 1.7780\n",
            "Iteration: 110; Percent complete: 1.1%; Average loss: 1.7403\n",
            "Iteration: 111; Percent complete: 1.1%; Average loss: 1.7025\n",
            "Iteration: 112; Percent complete: 1.1%; Average loss: 1.6577\n",
            "Iteration: 113; Percent complete: 1.1%; Average loss: 1.7263\n",
            "Iteration: 114; Percent complete: 1.1%; Average loss: 1.6397\n",
            "Iteration: 115; Percent complete: 1.1%; Average loss: 1.6570\n",
            "Iteration: 116; Percent complete: 1.2%; Average loss: 1.5643\n",
            "Iteration: 117; Percent complete: 1.2%; Average loss: 1.4949\n",
            "Iteration: 118; Percent complete: 1.2%; Average loss: 1.5124\n",
            "Iteration: 119; Percent complete: 1.2%; Average loss: 1.5176\n",
            "Iteration: 120; Percent complete: 1.2%; Average loss: 1.4763\n",
            "Iteration: 121; Percent complete: 1.2%; Average loss: 1.4410\n",
            "Iteration: 122; Percent complete: 1.2%; Average loss: 1.4406\n",
            "Iteration: 123; Percent complete: 1.2%; Average loss: 1.4279\n",
            "Iteration: 124; Percent complete: 1.2%; Average loss: 1.3727\n",
            "Iteration: 125; Percent complete: 1.2%; Average loss: 1.3730\n",
            "Iteration: 126; Percent complete: 1.3%; Average loss: 1.3756\n",
            "Iteration: 127; Percent complete: 1.3%; Average loss: 1.3848\n",
            "Iteration: 128; Percent complete: 1.3%; Average loss: 1.3351\n",
            "Iteration: 129; Percent complete: 1.3%; Average loss: 1.3024\n",
            "Iteration: 130; Percent complete: 1.3%; Average loss: 1.2488\n",
            "Iteration: 131; Percent complete: 1.3%; Average loss: 1.2774\n",
            "Iteration: 132; Percent complete: 1.3%; Average loss: 1.2440\n",
            "Iteration: 133; Percent complete: 1.3%; Average loss: 1.2319\n",
            "Iteration: 134; Percent complete: 1.3%; Average loss: 1.1989\n",
            "Iteration: 135; Percent complete: 1.4%; Average loss: 1.2286\n",
            "Iteration: 136; Percent complete: 1.4%; Average loss: 1.1725\n",
            "Iteration: 137; Percent complete: 1.4%; Average loss: 1.1451\n",
            "Iteration: 138; Percent complete: 1.4%; Average loss: 1.1673\n",
            "Iteration: 139; Percent complete: 1.4%; Average loss: 1.1290\n",
            "Iteration: 140; Percent complete: 1.4%; Average loss: 1.1160\n",
            "Iteration: 141; Percent complete: 1.4%; Average loss: 1.0733\n",
            "Iteration: 142; Percent complete: 1.4%; Average loss: 1.1050\n",
            "Iteration: 143; Percent complete: 1.4%; Average loss: 1.0681\n",
            "Iteration: 144; Percent complete: 1.4%; Average loss: 1.0689\n",
            "Iteration: 145; Percent complete: 1.5%; Average loss: 1.0517\n",
            "Iteration: 146; Percent complete: 1.5%; Average loss: 1.0298\n",
            "Iteration: 147; Percent complete: 1.5%; Average loss: 1.0135\n",
            "Iteration: 148; Percent complete: 1.5%; Average loss: 1.0041\n",
            "Iteration: 149; Percent complete: 1.5%; Average loss: 1.0069\n",
            "Iteration: 150; Percent complete: 1.5%; Average loss: 0.9731\n",
            "Iteration: 151; Percent complete: 1.5%; Average loss: 0.9701\n",
            "Iteration: 152; Percent complete: 1.5%; Average loss: 0.9353\n",
            "Iteration: 153; Percent complete: 1.5%; Average loss: 0.9472\n",
            "Iteration: 154; Percent complete: 1.5%; Average loss: 0.9291\n",
            "Iteration: 155; Percent complete: 1.6%; Average loss: 0.9201\n",
            "Iteration: 156; Percent complete: 1.6%; Average loss: 0.9035\n",
            "Iteration: 157; Percent complete: 1.6%; Average loss: 0.9001\n",
            "Iteration: 158; Percent complete: 1.6%; Average loss: 0.8900\n",
            "Iteration: 159; Percent complete: 1.6%; Average loss: 0.8796\n",
            "Iteration: 160; Percent complete: 1.6%; Average loss: 0.8773\n",
            "Iteration: 161; Percent complete: 1.6%; Average loss: 0.8660\n",
            "Iteration: 162; Percent complete: 1.6%; Average loss: 0.8531\n",
            "Iteration: 163; Percent complete: 1.6%; Average loss: 0.8523\n",
            "Iteration: 164; Percent complete: 1.6%; Average loss: 0.8339\n",
            "Iteration: 165; Percent complete: 1.7%; Average loss: 0.8284\n",
            "Iteration: 166; Percent complete: 1.7%; Average loss: 0.8362\n",
            "Iteration: 167; Percent complete: 1.7%; Average loss: 0.8183\n",
            "Iteration: 168; Percent complete: 1.7%; Average loss: 0.8262\n",
            "Iteration: 169; Percent complete: 1.7%; Average loss: 0.7998\n",
            "Iteration: 170; Percent complete: 1.7%; Average loss: 0.7963\n",
            "Iteration: 171; Percent complete: 1.7%; Average loss: 0.7879\n",
            "Iteration: 172; Percent complete: 1.7%; Average loss: 0.7975\n",
            "Iteration: 173; Percent complete: 1.7%; Average loss: 0.7806\n",
            "Iteration: 174; Percent complete: 1.7%; Average loss: 0.7820\n",
            "Iteration: 175; Percent complete: 1.8%; Average loss: 0.7777\n",
            "Iteration: 176; Percent complete: 1.8%; Average loss: 0.7652\n",
            "Iteration: 177; Percent complete: 1.8%; Average loss: 0.7617\n",
            "Iteration: 178; Percent complete: 1.8%; Average loss: 0.7584\n",
            "Iteration: 179; Percent complete: 1.8%; Average loss: 0.7548\n",
            "Iteration: 180; Percent complete: 1.8%; Average loss: 0.7504\n",
            "Iteration: 181; Percent complete: 1.8%; Average loss: 0.7431\n",
            "Iteration: 182; Percent complete: 1.8%; Average loss: 0.7370\n",
            "Iteration: 183; Percent complete: 1.8%; Average loss: 0.7360\n",
            "Iteration: 184; Percent complete: 1.8%; Average loss: 0.7347\n",
            "Iteration: 185; Percent complete: 1.8%; Average loss: 0.7305\n",
            "Iteration: 186; Percent complete: 1.9%; Average loss: 0.7284\n",
            "Iteration: 187; Percent complete: 1.9%; Average loss: 0.7253\n",
            "Iteration: 188; Percent complete: 1.9%; Average loss: 0.7269\n",
            "Iteration: 189; Percent complete: 1.9%; Average loss: 0.7219\n",
            "Iteration: 190; Percent complete: 1.9%; Average loss: 0.7146\n",
            "Iteration: 191; Percent complete: 1.9%; Average loss: 0.7138\n",
            "Iteration: 192; Percent complete: 1.9%; Average loss: 0.7138\n",
            "Iteration: 193; Percent complete: 1.9%; Average loss: 0.7108\n",
            "Iteration: 194; Percent complete: 1.9%; Average loss: 0.7090\n",
            "Iteration: 195; Percent complete: 1.9%; Average loss: 0.6996\n",
            "Iteration: 196; Percent complete: 2.0%; Average loss: 0.6984\n",
            "Iteration: 197; Percent complete: 2.0%; Average loss: 0.7011\n",
            "Iteration: 198; Percent complete: 2.0%; Average loss: 0.7081\n",
            "Iteration: 199; Percent complete: 2.0%; Average loss: 0.7003\n",
            "Iteration: 200; Percent complete: 2.0%; Average loss: 0.6944\n",
            "Iteration: 201; Percent complete: 2.0%; Average loss: 0.6925\n",
            "Iteration: 202; Percent complete: 2.0%; Average loss: 0.6951\n",
            "Iteration: 203; Percent complete: 2.0%; Average loss: 0.6941\n",
            "Iteration: 204; Percent complete: 2.0%; Average loss: 0.6950\n",
            "Iteration: 205; Percent complete: 2.1%; Average loss: 0.6968\n",
            "Iteration: 206; Percent complete: 2.1%; Average loss: 0.6865\n",
            "Iteration: 207; Percent complete: 2.1%; Average loss: 0.6874\n",
            "Iteration: 208; Percent complete: 2.1%; Average loss: 0.6957\n",
            "Iteration: 209; Percent complete: 2.1%; Average loss: 0.6889\n",
            "Iteration: 210; Percent complete: 2.1%; Average loss: 0.6877\n",
            "Iteration: 211; Percent complete: 2.1%; Average loss: 0.6908\n",
            "Iteration: 212; Percent complete: 2.1%; Average loss: 0.6805\n",
            "Iteration: 213; Percent complete: 2.1%; Average loss: 0.6793\n",
            "Iteration: 214; Percent complete: 2.1%; Average loss: 0.6864\n",
            "Iteration: 215; Percent complete: 2.1%; Average loss: 0.6747\n",
            "Iteration: 216; Percent complete: 2.2%; Average loss: 0.6786\n",
            "Iteration: 217; Percent complete: 2.2%; Average loss: 0.6752\n",
            "Iteration: 218; Percent complete: 2.2%; Average loss: 0.6816\n",
            "Iteration: 219; Percent complete: 2.2%; Average loss: 0.6879\n",
            "Iteration: 220; Percent complete: 2.2%; Average loss: 0.6806\n",
            "Iteration: 221; Percent complete: 2.2%; Average loss: 0.6836\n",
            "Iteration: 222; Percent complete: 2.2%; Average loss: 0.6849\n",
            "Iteration: 223; Percent complete: 2.2%; Average loss: 0.6770\n",
            "Iteration: 224; Percent complete: 2.2%; Average loss: 0.6806\n",
            "Iteration: 225; Percent complete: 2.2%; Average loss: 0.6794\n",
            "Iteration: 226; Percent complete: 2.3%; Average loss: 0.6704\n",
            "Iteration: 227; Percent complete: 2.3%; Average loss: 0.6799\n",
            "Iteration: 228; Percent complete: 2.3%; Average loss: 0.6736\n",
            "Iteration: 229; Percent complete: 2.3%; Average loss: 0.6792\n",
            "Iteration: 230; Percent complete: 2.3%; Average loss: 0.6727\n",
            "Iteration: 231; Percent complete: 2.3%; Average loss: 0.6764\n",
            "Iteration: 232; Percent complete: 2.3%; Average loss: 0.6786\n",
            "Iteration: 233; Percent complete: 2.3%; Average loss: 0.6832\n",
            "Iteration: 234; Percent complete: 2.3%; Average loss: 0.6732\n",
            "Iteration: 235; Percent complete: 2.4%; Average loss: 0.6731\n",
            "Iteration: 236; Percent complete: 2.4%; Average loss: 0.6781\n",
            "Iteration: 237; Percent complete: 2.4%; Average loss: 0.6812\n",
            "Iteration: 238; Percent complete: 2.4%; Average loss: 0.6736\n",
            "Iteration: 239; Percent complete: 2.4%; Average loss: 0.6829\n",
            "Iteration: 240; Percent complete: 2.4%; Average loss: 0.6695\n",
            "Iteration: 241; Percent complete: 2.4%; Average loss: 0.6719\n",
            "Iteration: 242; Percent complete: 2.4%; Average loss: 0.6772\n",
            "Iteration: 243; Percent complete: 2.4%; Average loss: 0.6727\n",
            "Iteration: 244; Percent complete: 2.4%; Average loss: 0.6737\n",
            "Iteration: 245; Percent complete: 2.5%; Average loss: 0.6700\n",
            "Iteration: 246; Percent complete: 2.5%; Average loss: 0.6696\n",
            "Iteration: 247; Percent complete: 2.5%; Average loss: 0.6770\n",
            "Iteration: 248; Percent complete: 2.5%; Average loss: 0.6797\n",
            "Iteration: 249; Percent complete: 2.5%; Average loss: 0.6742\n",
            "Iteration: 250; Percent complete: 2.5%; Average loss: 0.6704\n",
            "Iteration: 251; Percent complete: 2.5%; Average loss: 0.6750\n",
            "Iteration: 252; Percent complete: 2.5%; Average loss: 0.6706\n",
            "Iteration: 253; Percent complete: 2.5%; Average loss: 0.6731\n",
            "Iteration: 254; Percent complete: 2.5%; Average loss: 0.6701\n",
            "Iteration: 255; Percent complete: 2.5%; Average loss: 0.6674\n",
            "Iteration: 256; Percent complete: 2.6%; Average loss: 0.6718\n",
            "Iteration: 257; Percent complete: 2.6%; Average loss: 0.6684\n",
            "Iteration: 258; Percent complete: 2.6%; Average loss: 0.6780\n",
            "Iteration: 259; Percent complete: 2.6%; Average loss: 0.6761\n",
            "Iteration: 260; Percent complete: 2.6%; Average loss: 0.6744\n",
            "Iteration: 261; Percent complete: 2.6%; Average loss: 0.6757\n",
            "Iteration: 262; Percent complete: 2.6%; Average loss: 0.6735\n",
            "Iteration: 263; Percent complete: 2.6%; Average loss: 0.6684\n",
            "Iteration: 264; Percent complete: 2.6%; Average loss: 0.6732\n",
            "Iteration: 265; Percent complete: 2.6%; Average loss: 0.6726\n",
            "Iteration: 266; Percent complete: 2.7%; Average loss: 0.6726\n",
            "Iteration: 267; Percent complete: 2.7%; Average loss: 0.6808\n",
            "Iteration: 268; Percent complete: 2.7%; Average loss: 0.6787\n",
            "Iteration: 269; Percent complete: 2.7%; Average loss: 0.6688\n",
            "Iteration: 270; Percent complete: 2.7%; Average loss: 0.6652\n",
            "Iteration: 271; Percent complete: 2.7%; Average loss: 0.6704\n",
            "Iteration: 272; Percent complete: 2.7%; Average loss: 0.6751\n",
            "Iteration: 273; Percent complete: 2.7%; Average loss: 0.6640\n",
            "Iteration: 274; Percent complete: 2.7%; Average loss: 0.6728\n",
            "Iteration: 275; Percent complete: 2.8%; Average loss: 0.6708\n",
            "Iteration: 276; Percent complete: 2.8%; Average loss: 0.6733\n",
            "Iteration: 277; Percent complete: 2.8%; Average loss: 0.6656\n",
            "Iteration: 278; Percent complete: 2.8%; Average loss: 0.6736\n",
            "Iteration: 279; Percent complete: 2.8%; Average loss: 0.6707\n",
            "Iteration: 280; Percent complete: 2.8%; Average loss: 0.6711\n",
            "Iteration: 281; Percent complete: 2.8%; Average loss: 0.6649\n",
            "Iteration: 282; Percent complete: 2.8%; Average loss: 0.6725\n",
            "Iteration: 283; Percent complete: 2.8%; Average loss: 0.6718\n",
            "Iteration: 284; Percent complete: 2.8%; Average loss: 0.6749\n",
            "Iteration: 285; Percent complete: 2.9%; Average loss: 0.6673\n",
            "Iteration: 286; Percent complete: 2.9%; Average loss: 0.6734\n",
            "Iteration: 287; Percent complete: 2.9%; Average loss: 0.6700\n",
            "Iteration: 288; Percent complete: 2.9%; Average loss: 0.6695\n",
            "Iteration: 289; Percent complete: 2.9%; Average loss: 0.6751\n",
            "Iteration: 290; Percent complete: 2.9%; Average loss: 0.6673\n",
            "Iteration: 291; Percent complete: 2.9%; Average loss: 0.6701\n",
            "Iteration: 292; Percent complete: 2.9%; Average loss: 0.6664\n",
            "Iteration: 293; Percent complete: 2.9%; Average loss: 0.6703\n",
            "Iteration: 294; Percent complete: 2.9%; Average loss: 0.6664\n",
            "Iteration: 295; Percent complete: 2.9%; Average loss: 0.6649\n",
            "Iteration: 296; Percent complete: 3.0%; Average loss: 0.6789\n",
            "Iteration: 297; Percent complete: 3.0%; Average loss: 0.6654\n",
            "Iteration: 298; Percent complete: 3.0%; Average loss: 0.6639\n",
            "Iteration: 299; Percent complete: 3.0%; Average loss: 0.6689\n",
            "Iteration: 300; Percent complete: 3.0%; Average loss: 0.6708\n",
            "Iteration: 301; Percent complete: 3.0%; Average loss: 0.6698\n",
            "Iteration: 302; Percent complete: 3.0%; Average loss: 0.6735\n",
            "Iteration: 303; Percent complete: 3.0%; Average loss: 0.6814\n",
            "Iteration: 304; Percent complete: 3.0%; Average loss: 0.6637\n",
            "Iteration: 305; Percent complete: 3.0%; Average loss: 0.6737\n",
            "Iteration: 306; Percent complete: 3.1%; Average loss: 0.6666\n",
            "Iteration: 307; Percent complete: 3.1%; Average loss: 0.6725\n",
            "Iteration: 308; Percent complete: 3.1%; Average loss: 0.6701\n",
            "Iteration: 309; Percent complete: 3.1%; Average loss: 0.6730\n",
            "Iteration: 310; Percent complete: 3.1%; Average loss: 0.6790\n",
            "Iteration: 311; Percent complete: 3.1%; Average loss: 0.6716\n",
            "Iteration: 312; Percent complete: 3.1%; Average loss: 0.6742\n",
            "Iteration: 313; Percent complete: 3.1%; Average loss: 0.6666\n",
            "Iteration: 314; Percent complete: 3.1%; Average loss: 0.6706\n",
            "Iteration: 315; Percent complete: 3.1%; Average loss: 0.6725\n",
            "Iteration: 316; Percent complete: 3.2%; Average loss: 0.6716\n",
            "Iteration: 317; Percent complete: 3.2%; Average loss: 0.6789\n",
            "Iteration: 318; Percent complete: 3.2%; Average loss: 0.6740\n",
            "Iteration: 319; Percent complete: 3.2%; Average loss: 0.6609\n",
            "Iteration: 320; Percent complete: 3.2%; Average loss: 0.6781\n",
            "Iteration: 321; Percent complete: 3.2%; Average loss: 0.6656\n",
            "Iteration: 322; Percent complete: 3.2%; Average loss: 0.6680\n",
            "Iteration: 323; Percent complete: 3.2%; Average loss: 0.6733\n",
            "Iteration: 324; Percent complete: 3.2%; Average loss: 0.6690\n",
            "Iteration: 325; Percent complete: 3.2%; Average loss: 0.6596\n",
            "Iteration: 326; Percent complete: 3.3%; Average loss: 0.6706\n",
            "Iteration: 327; Percent complete: 3.3%; Average loss: 0.6824\n",
            "Iteration: 328; Percent complete: 3.3%; Average loss: 0.6641\n",
            "Iteration: 329; Percent complete: 3.3%; Average loss: 0.6624\n",
            "Iteration: 330; Percent complete: 3.3%; Average loss: 0.6638\n",
            "Iteration: 331; Percent complete: 3.3%; Average loss: 0.6650\n",
            "Iteration: 332; Percent complete: 3.3%; Average loss: 0.6729\n",
            "Iteration: 333; Percent complete: 3.3%; Average loss: 0.6659\n",
            "Iteration: 334; Percent complete: 3.3%; Average loss: 0.6734\n",
            "Iteration: 335; Percent complete: 3.4%; Average loss: 0.6733\n",
            "Iteration: 336; Percent complete: 3.4%; Average loss: 0.6742\n",
            "Iteration: 337; Percent complete: 3.4%; Average loss: 0.6669\n",
            "Iteration: 338; Percent complete: 3.4%; Average loss: 0.6676\n",
            "Iteration: 339; Percent complete: 3.4%; Average loss: 0.6676\n",
            "Iteration: 340; Percent complete: 3.4%; Average loss: 0.6721\n",
            "Iteration: 341; Percent complete: 3.4%; Average loss: 0.6663\n",
            "Iteration: 342; Percent complete: 3.4%; Average loss: 0.6718\n",
            "Iteration: 343; Percent complete: 3.4%; Average loss: 0.6680\n",
            "Iteration: 344; Percent complete: 3.4%; Average loss: 0.6684\n",
            "Iteration: 345; Percent complete: 3.5%; Average loss: 0.6729\n",
            "Iteration: 346; Percent complete: 3.5%; Average loss: 0.6702\n",
            "Iteration: 347; Percent complete: 3.5%; Average loss: 0.6758\n",
            "Iteration: 348; Percent complete: 3.5%; Average loss: 0.6720\n",
            "Iteration: 349; Percent complete: 3.5%; Average loss: 0.6752\n",
            "Iteration: 350; Percent complete: 3.5%; Average loss: 0.6709\n",
            "Iteration: 351; Percent complete: 3.5%; Average loss: 0.6665\n",
            "Iteration: 352; Percent complete: 3.5%; Average loss: 0.6672\n",
            "Iteration: 353; Percent complete: 3.5%; Average loss: 0.6722\n",
            "Iteration: 354; Percent complete: 3.5%; Average loss: 0.6738\n",
            "Iteration: 355; Percent complete: 3.5%; Average loss: 0.6711\n",
            "Iteration: 356; Percent complete: 3.6%; Average loss: 0.6670\n",
            "Iteration: 357; Percent complete: 3.6%; Average loss: 0.6667\n",
            "Iteration: 358; Percent complete: 3.6%; Average loss: 0.6663\n",
            "Iteration: 359; Percent complete: 3.6%; Average loss: 0.6769\n",
            "Iteration: 360; Percent complete: 3.6%; Average loss: 0.6744\n",
            "Iteration: 361; Percent complete: 3.6%; Average loss: 0.6700\n",
            "Iteration: 362; Percent complete: 3.6%; Average loss: 0.6631\n",
            "Iteration: 363; Percent complete: 3.6%; Average loss: 0.6645\n",
            "Iteration: 364; Percent complete: 3.6%; Average loss: 0.6724\n",
            "Iteration: 365; Percent complete: 3.6%; Average loss: 0.6646\n",
            "Iteration: 366; Percent complete: 3.7%; Average loss: 0.6741\n",
            "Iteration: 367; Percent complete: 3.7%; Average loss: 0.6704\n",
            "Iteration: 368; Percent complete: 3.7%; Average loss: 0.6641\n",
            "Iteration: 369; Percent complete: 3.7%; Average loss: 0.6651\n",
            "Iteration: 370; Percent complete: 3.7%; Average loss: 0.6649\n",
            "Iteration: 371; Percent complete: 3.7%; Average loss: 0.6685\n",
            "Iteration: 372; Percent complete: 3.7%; Average loss: 0.6643\n",
            "Iteration: 373; Percent complete: 3.7%; Average loss: 0.6705\n",
            "Iteration: 374; Percent complete: 3.7%; Average loss: 0.6747\n",
            "Iteration: 375; Percent complete: 3.8%; Average loss: 0.6716\n",
            "Iteration: 376; Percent complete: 3.8%; Average loss: 0.6699\n",
            "Iteration: 377; Percent complete: 3.8%; Average loss: 0.6692\n",
            "Iteration: 378; Percent complete: 3.8%; Average loss: 0.6745\n",
            "Iteration: 379; Percent complete: 3.8%; Average loss: 0.6689\n",
            "Iteration: 380; Percent complete: 3.8%; Average loss: 0.6667\n",
            "Iteration: 381; Percent complete: 3.8%; Average loss: 0.6682\n",
            "Iteration: 382; Percent complete: 3.8%; Average loss: 0.6699\n",
            "Iteration: 383; Percent complete: 3.8%; Average loss: 0.6700\n",
            "Iteration: 384; Percent complete: 3.8%; Average loss: 0.6700\n",
            "Iteration: 385; Percent complete: 3.9%; Average loss: 0.6690\n",
            "Iteration: 386; Percent complete: 3.9%; Average loss: 0.6733\n",
            "Iteration: 387; Percent complete: 3.9%; Average loss: 0.6689\n",
            "Iteration: 388; Percent complete: 3.9%; Average loss: 0.6681\n",
            "Iteration: 389; Percent complete: 3.9%; Average loss: 0.6686\n",
            "Iteration: 390; Percent complete: 3.9%; Average loss: 0.6672\n",
            "Iteration: 391; Percent complete: 3.9%; Average loss: 0.6708\n",
            "Iteration: 392; Percent complete: 3.9%; Average loss: 0.6704\n",
            "Iteration: 393; Percent complete: 3.9%; Average loss: 0.6662\n",
            "Iteration: 394; Percent complete: 3.9%; Average loss: 0.6650\n",
            "Iteration: 395; Percent complete: 4.0%; Average loss: 0.6742\n",
            "Iteration: 396; Percent complete: 4.0%; Average loss: 0.6770\n",
            "Iteration: 397; Percent complete: 4.0%; Average loss: 0.6679\n",
            "Iteration: 398; Percent complete: 4.0%; Average loss: 0.6630\n",
            "Iteration: 399; Percent complete: 4.0%; Average loss: 0.6699\n",
            "Iteration: 400; Percent complete: 4.0%; Average loss: 0.6649\n",
            "Iteration: 401; Percent complete: 4.0%; Average loss: 0.6716\n",
            "Iteration: 402; Percent complete: 4.0%; Average loss: 0.6680\n",
            "Iteration: 403; Percent complete: 4.0%; Average loss: 0.6717\n",
            "Iteration: 404; Percent complete: 4.0%; Average loss: 0.6741\n",
            "Iteration: 405; Percent complete: 4.0%; Average loss: 0.6660\n",
            "Iteration: 406; Percent complete: 4.1%; Average loss: 0.6753\n",
            "Iteration: 407; Percent complete: 4.1%; Average loss: 0.6601\n",
            "Iteration: 408; Percent complete: 4.1%; Average loss: 0.6649\n",
            "Iteration: 409; Percent complete: 4.1%; Average loss: 0.6653\n",
            "Iteration: 410; Percent complete: 4.1%; Average loss: 0.6674\n",
            "Iteration: 411; Percent complete: 4.1%; Average loss: 0.6760\n",
            "Iteration: 412; Percent complete: 4.1%; Average loss: 0.6701\n",
            "Iteration: 413; Percent complete: 4.1%; Average loss: 0.6674\n",
            "Iteration: 414; Percent complete: 4.1%; Average loss: 0.6665\n",
            "Iteration: 415; Percent complete: 4.2%; Average loss: 0.6651\n",
            "Iteration: 416; Percent complete: 4.2%; Average loss: 0.6673\n",
            "Iteration: 417; Percent complete: 4.2%; Average loss: 0.6690\n",
            "Iteration: 418; Percent complete: 4.2%; Average loss: 0.6635\n",
            "Iteration: 419; Percent complete: 4.2%; Average loss: 0.6671\n",
            "Iteration: 420; Percent complete: 4.2%; Average loss: 0.6623\n",
            "Iteration: 421; Percent complete: 4.2%; Average loss: 0.6709\n",
            "Iteration: 422; Percent complete: 4.2%; Average loss: 0.6711\n",
            "Iteration: 423; Percent complete: 4.2%; Average loss: 0.6709\n",
            "Iteration: 424; Percent complete: 4.2%; Average loss: 0.6684\n",
            "Iteration: 425; Percent complete: 4.2%; Average loss: 0.6679\n",
            "Iteration: 426; Percent complete: 4.3%; Average loss: 0.6778\n",
            "Iteration: 427; Percent complete: 4.3%; Average loss: 0.6712\n",
            "Iteration: 428; Percent complete: 4.3%; Average loss: 0.6665\n",
            "Iteration: 429; Percent complete: 4.3%; Average loss: 0.6702\n",
            "Iteration: 430; Percent complete: 4.3%; Average loss: 0.6637\n",
            "Iteration: 431; Percent complete: 4.3%; Average loss: 0.6673\n",
            "Iteration: 432; Percent complete: 4.3%; Average loss: 0.6722\n",
            "Iteration: 433; Percent complete: 4.3%; Average loss: 0.6563\n",
            "Iteration: 434; Percent complete: 4.3%; Average loss: 0.6678\n",
            "Iteration: 435; Percent complete: 4.3%; Average loss: 0.6704\n",
            "Iteration: 436; Percent complete: 4.4%; Average loss: 0.6680\n",
            "Iteration: 437; Percent complete: 4.4%; Average loss: 0.6678\n",
            "Iteration: 438; Percent complete: 4.4%; Average loss: 0.6656\n",
            "Iteration: 439; Percent complete: 4.4%; Average loss: 0.6732\n",
            "Iteration: 440; Percent complete: 4.4%; Average loss: 0.6635\n",
            "Iteration: 441; Percent complete: 4.4%; Average loss: 0.6749\n",
            "Iteration: 442; Percent complete: 4.4%; Average loss: 0.6689\n",
            "Iteration: 443; Percent complete: 4.4%; Average loss: 0.6739\n",
            "Iteration: 444; Percent complete: 4.4%; Average loss: 0.6608\n",
            "Iteration: 445; Percent complete: 4.5%; Average loss: 0.6681\n",
            "Iteration: 446; Percent complete: 4.5%; Average loss: 0.6693\n",
            "Iteration: 447; Percent complete: 4.5%; Average loss: 0.6663\n",
            "Iteration: 448; Percent complete: 4.5%; Average loss: 0.6669\n",
            "Iteration: 449; Percent complete: 4.5%; Average loss: 0.6747\n",
            "Iteration: 450; Percent complete: 4.5%; Average loss: 0.6629\n",
            "Iteration: 451; Percent complete: 4.5%; Average loss: 0.6640\n",
            "Iteration: 452; Percent complete: 4.5%; Average loss: 0.6688\n",
            "Iteration: 453; Percent complete: 4.5%; Average loss: 0.6682\n",
            "Iteration: 454; Percent complete: 4.5%; Average loss: 0.6720\n",
            "Iteration: 455; Percent complete: 4.5%; Average loss: 0.6738\n",
            "Iteration: 456; Percent complete: 4.6%; Average loss: 0.6698\n",
            "Iteration: 457; Percent complete: 4.6%; Average loss: 0.6656\n",
            "Iteration: 458; Percent complete: 4.6%; Average loss: 0.6642\n",
            "Iteration: 459; Percent complete: 4.6%; Average loss: 0.6652\n",
            "Iteration: 460; Percent complete: 4.6%; Average loss: 0.6660\n",
            "Iteration: 461; Percent complete: 4.6%; Average loss: 0.6676\n",
            "Iteration: 462; Percent complete: 4.6%; Average loss: 0.6666\n",
            "Iteration: 463; Percent complete: 4.6%; Average loss: 0.6720\n",
            "Iteration: 464; Percent complete: 4.6%; Average loss: 0.6609\n",
            "Iteration: 465; Percent complete: 4.7%; Average loss: 0.6647\n",
            "Iteration: 466; Percent complete: 4.7%; Average loss: 0.6686\n",
            "Iteration: 467; Percent complete: 4.7%; Average loss: 0.6636\n",
            "Iteration: 468; Percent complete: 4.7%; Average loss: 0.6672\n",
            "Iteration: 469; Percent complete: 4.7%; Average loss: 0.6688\n",
            "Iteration: 470; Percent complete: 4.7%; Average loss: 0.6724\n",
            "Iteration: 471; Percent complete: 4.7%; Average loss: 0.6683\n",
            "Iteration: 472; Percent complete: 4.7%; Average loss: 0.6656\n",
            "Iteration: 473; Percent complete: 4.7%; Average loss: 0.6633\n",
            "Iteration: 474; Percent complete: 4.7%; Average loss: 0.6787\n",
            "Iteration: 475; Percent complete: 4.8%; Average loss: 0.6603\n",
            "Iteration: 476; Percent complete: 4.8%; Average loss: 0.6664\n",
            "Iteration: 477; Percent complete: 4.8%; Average loss: 0.6669\n",
            "Iteration: 478; Percent complete: 4.8%; Average loss: 0.6682\n",
            "Iteration: 479; Percent complete: 4.8%; Average loss: 0.6714\n",
            "Iteration: 480; Percent complete: 4.8%; Average loss: 0.6725\n",
            "Iteration: 481; Percent complete: 4.8%; Average loss: 0.6680\n",
            "Iteration: 482; Percent complete: 4.8%; Average loss: 0.6733\n",
            "Iteration: 483; Percent complete: 4.8%; Average loss: 0.6667\n",
            "Iteration: 484; Percent complete: 4.8%; Average loss: 0.6672\n",
            "Iteration: 485; Percent complete: 4.9%; Average loss: 0.6708\n",
            "Iteration: 486; Percent complete: 4.9%; Average loss: 0.6679\n",
            "Iteration: 487; Percent complete: 4.9%; Average loss: 0.6714\n",
            "Iteration: 488; Percent complete: 4.9%; Average loss: 0.6639\n",
            "Iteration: 489; Percent complete: 4.9%; Average loss: 0.6763\n",
            "Iteration: 490; Percent complete: 4.9%; Average loss: 0.6710\n",
            "Iteration: 491; Percent complete: 4.9%; Average loss: 0.6662\n",
            "Iteration: 492; Percent complete: 4.9%; Average loss: 0.6660\n",
            "Iteration: 493; Percent complete: 4.9%; Average loss: 0.6722\n",
            "Iteration: 494; Percent complete: 4.9%; Average loss: 0.6695\n",
            "Iteration: 495; Percent complete: 5.0%; Average loss: 0.6703\n",
            "Iteration: 496; Percent complete: 5.0%; Average loss: 0.6671\n",
            "Iteration: 497; Percent complete: 5.0%; Average loss: 0.6730\n",
            "Iteration: 498; Percent complete: 5.0%; Average loss: 0.6694\n",
            "Iteration: 499; Percent complete: 5.0%; Average loss: 0.6651\n",
            "Iteration: 500; Percent complete: 5.0%; Average loss: 0.6709\n",
            "Iteration: 501; Percent complete: 5.0%; Average loss: 0.6708\n",
            "Iteration: 502; Percent complete: 5.0%; Average loss: 0.6648\n",
            "Iteration: 503; Percent complete: 5.0%; Average loss: 0.6751\n",
            "Iteration: 504; Percent complete: 5.0%; Average loss: 0.6679\n",
            "Iteration: 505; Percent complete: 5.1%; Average loss: 0.6704\n",
            "Iteration: 506; Percent complete: 5.1%; Average loss: 0.6666\n",
            "Iteration: 507; Percent complete: 5.1%; Average loss: 0.6742\n",
            "Iteration: 508; Percent complete: 5.1%; Average loss: 0.6695\n",
            "Iteration: 509; Percent complete: 5.1%; Average loss: 0.6703\n",
            "Iteration: 510; Percent complete: 5.1%; Average loss: 0.6608\n",
            "Iteration: 511; Percent complete: 5.1%; Average loss: 0.6694\n",
            "Iteration: 512; Percent complete: 5.1%; Average loss: 0.6698\n",
            "Iteration: 513; Percent complete: 5.1%; Average loss: 0.6622\n",
            "Iteration: 514; Percent complete: 5.1%; Average loss: 0.6663\n",
            "Iteration: 515; Percent complete: 5.1%; Average loss: 0.6629\n",
            "Iteration: 516; Percent complete: 5.2%; Average loss: 0.6650\n",
            "Iteration: 517; Percent complete: 5.2%; Average loss: 0.6661\n",
            "Iteration: 518; Percent complete: 5.2%; Average loss: 0.6753\n",
            "Iteration: 519; Percent complete: 5.2%; Average loss: 0.6686\n",
            "Iteration: 520; Percent complete: 5.2%; Average loss: 0.6740\n",
            "Iteration: 521; Percent complete: 5.2%; Average loss: 0.6566\n",
            "Iteration: 522; Percent complete: 5.2%; Average loss: 0.6688\n",
            "Iteration: 523; Percent complete: 5.2%; Average loss: 0.6563\n",
            "Iteration: 524; Percent complete: 5.2%; Average loss: 0.6687\n",
            "Iteration: 525; Percent complete: 5.2%; Average loss: 0.6683\n",
            "Iteration: 526; Percent complete: 5.3%; Average loss: 0.6614\n",
            "Iteration: 527; Percent complete: 5.3%; Average loss: 0.6699\n",
            "Iteration: 528; Percent complete: 5.3%; Average loss: 0.6716\n",
            "Iteration: 529; Percent complete: 5.3%; Average loss: 0.6683\n",
            "Iteration: 530; Percent complete: 5.3%; Average loss: 0.6654\n",
            "Iteration: 531; Percent complete: 5.3%; Average loss: 0.6656\n",
            "Iteration: 532; Percent complete: 5.3%; Average loss: 0.6683\n",
            "Iteration: 533; Percent complete: 5.3%; Average loss: 0.6721\n",
            "Iteration: 534; Percent complete: 5.3%; Average loss: 0.6637\n",
            "Iteration: 535; Percent complete: 5.3%; Average loss: 0.6691\n",
            "Iteration: 536; Percent complete: 5.4%; Average loss: 0.6682\n",
            "Iteration: 537; Percent complete: 5.4%; Average loss: 0.6698\n",
            "Iteration: 538; Percent complete: 5.4%; Average loss: 0.6680\n",
            "Iteration: 539; Percent complete: 5.4%; Average loss: 0.6714\n",
            "Iteration: 540; Percent complete: 5.4%; Average loss: 0.6747\n",
            "Iteration: 541; Percent complete: 5.4%; Average loss: 0.6671\n",
            "Iteration: 542; Percent complete: 5.4%; Average loss: 0.6617\n",
            "Iteration: 543; Percent complete: 5.4%; Average loss: 0.6702\n",
            "Iteration: 544; Percent complete: 5.4%; Average loss: 0.6644\n",
            "Iteration: 545; Percent complete: 5.5%; Average loss: 0.6642\n",
            "Iteration: 546; Percent complete: 5.5%; Average loss: 0.6695\n",
            "Iteration: 547; Percent complete: 5.5%; Average loss: 0.6566\n",
            "Iteration: 548; Percent complete: 5.5%; Average loss: 0.6593\n",
            "Iteration: 549; Percent complete: 5.5%; Average loss: 0.6696\n",
            "Iteration: 550; Percent complete: 5.5%; Average loss: 0.6696\n",
            "Iteration: 551; Percent complete: 5.5%; Average loss: 0.6605\n",
            "Iteration: 552; Percent complete: 5.5%; Average loss: 0.6655\n",
            "Iteration: 553; Percent complete: 5.5%; Average loss: 0.6643\n",
            "Iteration: 554; Percent complete: 5.5%; Average loss: 0.6732\n",
            "Iteration: 555; Percent complete: 5.5%; Average loss: 0.6662\n",
            "Iteration: 556; Percent complete: 5.6%; Average loss: 0.6617\n",
            "Iteration: 557; Percent complete: 5.6%; Average loss: 0.6707\n",
            "Iteration: 558; Percent complete: 5.6%; Average loss: 0.6625\n",
            "Iteration: 559; Percent complete: 5.6%; Average loss: 0.6688\n",
            "Iteration: 560; Percent complete: 5.6%; Average loss: 0.6703\n",
            "Iteration: 561; Percent complete: 5.6%; Average loss: 0.6646\n",
            "Iteration: 562; Percent complete: 5.6%; Average loss: 0.6696\n",
            "Iteration: 563; Percent complete: 5.6%; Average loss: 0.6770\n",
            "Iteration: 564; Percent complete: 5.6%; Average loss: 0.6668\n",
            "Iteration: 565; Percent complete: 5.7%; Average loss: 0.6637\n",
            "Iteration: 566; Percent complete: 5.7%; Average loss: 0.6632\n",
            "Iteration: 567; Percent complete: 5.7%; Average loss: 0.6683\n",
            "Iteration: 568; Percent complete: 5.7%; Average loss: 0.6692\n",
            "Iteration: 569; Percent complete: 5.7%; Average loss: 0.6729\n",
            "Iteration: 570; Percent complete: 5.7%; Average loss: 0.6705\n",
            "Iteration: 571; Percent complete: 5.7%; Average loss: 0.6698\n",
            "Iteration: 572; Percent complete: 5.7%; Average loss: 0.6665\n",
            "Iteration: 573; Percent complete: 5.7%; Average loss: 0.6649\n",
            "Iteration: 574; Percent complete: 5.7%; Average loss: 0.6751\n",
            "Iteration: 575; Percent complete: 5.8%; Average loss: 0.6678\n",
            "Iteration: 576; Percent complete: 5.8%; Average loss: 0.6747\n",
            "Iteration: 577; Percent complete: 5.8%; Average loss: 0.6631\n",
            "Iteration: 578; Percent complete: 5.8%; Average loss: 0.6667\n",
            "Iteration: 579; Percent complete: 5.8%; Average loss: 0.6668\n",
            "Iteration: 580; Percent complete: 5.8%; Average loss: 0.6630\n",
            "Iteration: 581; Percent complete: 5.8%; Average loss: 0.6713\n",
            "Iteration: 582; Percent complete: 5.8%; Average loss: 0.6655\n",
            "Iteration: 583; Percent complete: 5.8%; Average loss: 0.6693\n",
            "Iteration: 584; Percent complete: 5.8%; Average loss: 0.6649\n",
            "Iteration: 585; Percent complete: 5.9%; Average loss: 0.6664\n",
            "Iteration: 586; Percent complete: 5.9%; Average loss: 0.6731\n",
            "Iteration: 587; Percent complete: 5.9%; Average loss: 0.6635\n",
            "Iteration: 588; Percent complete: 5.9%; Average loss: 0.6631\n",
            "Iteration: 589; Percent complete: 5.9%; Average loss: 0.6606\n",
            "Iteration: 590; Percent complete: 5.9%; Average loss: 0.6565\n",
            "Iteration: 591; Percent complete: 5.9%; Average loss: 0.6706\n",
            "Iteration: 592; Percent complete: 5.9%; Average loss: 0.6641\n",
            "Iteration: 593; Percent complete: 5.9%; Average loss: 0.6752\n",
            "Iteration: 594; Percent complete: 5.9%; Average loss: 0.6684\n",
            "Iteration: 595; Percent complete: 5.9%; Average loss: 0.6648\n",
            "Iteration: 596; Percent complete: 6.0%; Average loss: 0.6704\n",
            "Iteration: 597; Percent complete: 6.0%; Average loss: 0.6696\n",
            "Iteration: 598; Percent complete: 6.0%; Average loss: 0.6703\n",
            "Iteration: 599; Percent complete: 6.0%; Average loss: 0.6692\n",
            "Iteration: 600; Percent complete: 6.0%; Average loss: 0.6713\n",
            "Iteration: 601; Percent complete: 6.0%; Average loss: 0.6674\n",
            "Iteration: 602; Percent complete: 6.0%; Average loss: 0.6652\n",
            "Iteration: 603; Percent complete: 6.0%; Average loss: 0.6639\n",
            "Iteration: 604; Percent complete: 6.0%; Average loss: 0.6649\n",
            "Iteration: 605; Percent complete: 6.0%; Average loss: 0.6670\n",
            "Iteration: 606; Percent complete: 6.1%; Average loss: 0.6614\n",
            "Iteration: 607; Percent complete: 6.1%; Average loss: 0.6749\n",
            "Iteration: 608; Percent complete: 6.1%; Average loss: 0.6680\n",
            "Iteration: 609; Percent complete: 6.1%; Average loss: 0.6659\n",
            "Iteration: 610; Percent complete: 6.1%; Average loss: 0.6659\n",
            "Iteration: 611; Percent complete: 6.1%; Average loss: 0.6710\n",
            "Iteration: 612; Percent complete: 6.1%; Average loss: 0.6745\n",
            "Iteration: 613; Percent complete: 6.1%; Average loss: 0.6705\n",
            "Iteration: 614; Percent complete: 6.1%; Average loss: 0.6647\n",
            "Iteration: 615; Percent complete: 6.2%; Average loss: 0.6691\n",
            "Iteration: 616; Percent complete: 6.2%; Average loss: 0.6685\n",
            "Iteration: 617; Percent complete: 6.2%; Average loss: 0.6582\n",
            "Iteration: 618; Percent complete: 6.2%; Average loss: 0.6715\n",
            "Iteration: 619; Percent complete: 6.2%; Average loss: 0.6622\n",
            "Iteration: 620; Percent complete: 6.2%; Average loss: 0.6747\n",
            "Iteration: 621; Percent complete: 6.2%; Average loss: 0.6690\n",
            "Iteration: 622; Percent complete: 6.2%; Average loss: 0.6673\n",
            "Iteration: 623; Percent complete: 6.2%; Average loss: 0.6675\n",
            "Iteration: 624; Percent complete: 6.2%; Average loss: 0.6692\n",
            "Iteration: 625; Percent complete: 6.2%; Average loss: 0.6715\n",
            "Iteration: 626; Percent complete: 6.3%; Average loss: 0.6650\n",
            "Iteration: 627; Percent complete: 6.3%; Average loss: 0.6656\n",
            "Iteration: 628; Percent complete: 6.3%; Average loss: 0.6692\n",
            "Iteration: 629; Percent complete: 6.3%; Average loss: 0.6623\n",
            "Iteration: 630; Percent complete: 6.3%; Average loss: 0.6684\n",
            "Iteration: 631; Percent complete: 6.3%; Average loss: 0.6717\n",
            "Iteration: 632; Percent complete: 6.3%; Average loss: 0.6605\n",
            "Iteration: 633; Percent complete: 6.3%; Average loss: 0.6740\n",
            "Iteration: 634; Percent complete: 6.3%; Average loss: 0.6683\n",
            "Iteration: 635; Percent complete: 6.3%; Average loss: 0.6692\n",
            "Iteration: 636; Percent complete: 6.4%; Average loss: 0.6678\n",
            "Iteration: 637; Percent complete: 6.4%; Average loss: 0.6590\n",
            "Iteration: 638; Percent complete: 6.4%; Average loss: 0.6729\n",
            "Iteration: 639; Percent complete: 6.4%; Average loss: 0.6621\n",
            "Iteration: 640; Percent complete: 6.4%; Average loss: 0.6700\n",
            "Iteration: 641; Percent complete: 6.4%; Average loss: 0.6681\n",
            "Iteration: 642; Percent complete: 6.4%; Average loss: 0.6685\n",
            "Iteration: 643; Percent complete: 6.4%; Average loss: 0.6681\n",
            "Iteration: 644; Percent complete: 6.4%; Average loss: 0.6673\n",
            "Iteration: 645; Percent complete: 6.5%; Average loss: 0.6676\n",
            "Iteration: 646; Percent complete: 6.5%; Average loss: 0.6623\n",
            "Iteration: 647; Percent complete: 6.5%; Average loss: 0.6659\n",
            "Iteration: 648; Percent complete: 6.5%; Average loss: 0.6660\n",
            "Iteration: 649; Percent complete: 6.5%; Average loss: 0.6778\n",
            "Iteration: 650; Percent complete: 6.5%; Average loss: 0.6685\n",
            "Iteration: 651; Percent complete: 6.5%; Average loss: 0.6687\n",
            "Iteration: 652; Percent complete: 6.5%; Average loss: 0.6646\n",
            "Iteration: 653; Percent complete: 6.5%; Average loss: 0.6760\n",
            "Iteration: 654; Percent complete: 6.5%; Average loss: 0.6736\n",
            "Iteration: 655; Percent complete: 6.6%; Average loss: 0.6614\n",
            "Iteration: 656; Percent complete: 6.6%; Average loss: 0.6705\n",
            "Iteration: 657; Percent complete: 6.6%; Average loss: 0.6606\n",
            "Iteration: 658; Percent complete: 6.6%; Average loss: 0.6608\n",
            "Iteration: 659; Percent complete: 6.6%; Average loss: 0.6667\n",
            "Iteration: 660; Percent complete: 6.6%; Average loss: 0.6770\n",
            "Iteration: 661; Percent complete: 6.6%; Average loss: 0.6721\n",
            "Iteration: 662; Percent complete: 6.6%; Average loss: 0.6684\n",
            "Iteration: 663; Percent complete: 6.6%; Average loss: 0.6702\n",
            "Iteration: 664; Percent complete: 6.6%; Average loss: 0.6745\n",
            "Iteration: 665; Percent complete: 6.7%; Average loss: 0.6700\n",
            "Iteration: 666; Percent complete: 6.7%; Average loss: 0.6664\n",
            "Iteration: 667; Percent complete: 6.7%; Average loss: 0.6607\n",
            "Iteration: 668; Percent complete: 6.7%; Average loss: 0.6695\n",
            "Iteration: 669; Percent complete: 6.7%; Average loss: 0.6679\n",
            "Iteration: 670; Percent complete: 6.7%; Average loss: 0.6655\n",
            "Iteration: 671; Percent complete: 6.7%; Average loss: 0.6753\n",
            "Iteration: 672; Percent complete: 6.7%; Average loss: 0.6646\n",
            "Iteration: 673; Percent complete: 6.7%; Average loss: 0.6685\n",
            "Iteration: 674; Percent complete: 6.7%; Average loss: 0.6657\n",
            "Iteration: 675; Percent complete: 6.8%; Average loss: 0.6685\n",
            "Iteration: 676; Percent complete: 6.8%; Average loss: 0.6664\n",
            "Iteration: 677; Percent complete: 6.8%; Average loss: 0.6661\n",
            "Iteration: 678; Percent complete: 6.8%; Average loss: 0.6663\n",
            "Iteration: 679; Percent complete: 6.8%; Average loss: 0.6665\n",
            "Iteration: 680; Percent complete: 6.8%; Average loss: 0.6737\n",
            "Iteration: 681; Percent complete: 6.8%; Average loss: 0.6693\n",
            "Iteration: 682; Percent complete: 6.8%; Average loss: 0.6648\n",
            "Iteration: 683; Percent complete: 6.8%; Average loss: 0.6709\n",
            "Iteration: 684; Percent complete: 6.8%; Average loss: 0.6684\n",
            "Iteration: 685; Percent complete: 6.9%; Average loss: 0.6721\n",
            "Iteration: 686; Percent complete: 6.9%; Average loss: 0.6709\n",
            "Iteration: 687; Percent complete: 6.9%; Average loss: 0.6760\n",
            "Iteration: 688; Percent complete: 6.9%; Average loss: 0.6742\n",
            "Iteration: 689; Percent complete: 6.9%; Average loss: 0.6679\n",
            "Iteration: 690; Percent complete: 6.9%; Average loss: 0.6673\n",
            "Iteration: 691; Percent complete: 6.9%; Average loss: 0.6679\n",
            "Iteration: 692; Percent complete: 6.9%; Average loss: 0.6703\n",
            "Iteration: 693; Percent complete: 6.9%; Average loss: 0.6720\n",
            "Iteration: 694; Percent complete: 6.9%; Average loss: 0.6683\n",
            "Iteration: 695; Percent complete: 7.0%; Average loss: 0.6667\n",
            "Iteration: 696; Percent complete: 7.0%; Average loss: 0.6690\n",
            "Iteration: 697; Percent complete: 7.0%; Average loss: 0.6625\n",
            "Iteration: 698; Percent complete: 7.0%; Average loss: 0.6629\n",
            "Iteration: 699; Percent complete: 7.0%; Average loss: 0.6729\n",
            "Iteration: 700; Percent complete: 7.0%; Average loss: 0.6679\n",
            "Iteration: 701; Percent complete: 7.0%; Average loss: 0.6638\n",
            "Iteration: 702; Percent complete: 7.0%; Average loss: 0.6754\n",
            "Iteration: 703; Percent complete: 7.0%; Average loss: 0.6663\n",
            "Iteration: 704; Percent complete: 7.0%; Average loss: 0.6646\n",
            "Iteration: 705; Percent complete: 7.0%; Average loss: 0.6647\n",
            "Iteration: 706; Percent complete: 7.1%; Average loss: 0.6684\n",
            "Iteration: 707; Percent complete: 7.1%; Average loss: 0.6669\n",
            "Iteration: 708; Percent complete: 7.1%; Average loss: 0.6679\n",
            "Iteration: 709; Percent complete: 7.1%; Average loss: 0.6657\n",
            "Iteration: 710; Percent complete: 7.1%; Average loss: 0.6632\n",
            "Iteration: 711; Percent complete: 7.1%; Average loss: 0.6647\n",
            "Iteration: 712; Percent complete: 7.1%; Average loss: 0.6673\n",
            "Iteration: 713; Percent complete: 7.1%; Average loss: 0.6646\n",
            "Iteration: 714; Percent complete: 7.1%; Average loss: 0.6653\n",
            "Iteration: 715; Percent complete: 7.1%; Average loss: 0.6711\n",
            "Iteration: 716; Percent complete: 7.2%; Average loss: 0.6646\n",
            "Iteration: 717; Percent complete: 7.2%; Average loss: 0.6683\n",
            "Iteration: 718; Percent complete: 7.2%; Average loss: 0.6647\n",
            "Iteration: 719; Percent complete: 7.2%; Average loss: 0.6753\n",
            "Iteration: 720; Percent complete: 7.2%; Average loss: 0.6673\n",
            "Iteration: 721; Percent complete: 7.2%; Average loss: 0.6602\n",
            "Iteration: 722; Percent complete: 7.2%; Average loss: 0.6721\n",
            "Iteration: 723; Percent complete: 7.2%; Average loss: 0.6586\n",
            "Iteration: 724; Percent complete: 7.2%; Average loss: 0.6725\n",
            "Iteration: 725; Percent complete: 7.2%; Average loss: 0.6692\n",
            "Iteration: 726; Percent complete: 7.3%; Average loss: 0.6628\n",
            "Iteration: 727; Percent complete: 7.3%; Average loss: 0.6694\n",
            "Iteration: 728; Percent complete: 7.3%; Average loss: 0.6767\n",
            "Iteration: 729; Percent complete: 7.3%; Average loss: 0.6615\n",
            "Iteration: 730; Percent complete: 7.3%; Average loss: 0.6679\n",
            "Iteration: 731; Percent complete: 7.3%; Average loss: 0.6656\n",
            "Iteration: 732; Percent complete: 7.3%; Average loss: 0.6609\n",
            "Iteration: 733; Percent complete: 7.3%; Average loss: 0.6680\n",
            "Iteration: 734; Percent complete: 7.3%; Average loss: 0.6615\n",
            "Iteration: 735; Percent complete: 7.3%; Average loss: 0.6655\n",
            "Iteration: 736; Percent complete: 7.4%; Average loss: 0.6657\n",
            "Iteration: 737; Percent complete: 7.4%; Average loss: 0.6706\n",
            "Iteration: 738; Percent complete: 7.4%; Average loss: 0.6696\n",
            "Iteration: 739; Percent complete: 7.4%; Average loss: 0.6585\n",
            "Iteration: 740; Percent complete: 7.4%; Average loss: 0.6752\n",
            "Iteration: 741; Percent complete: 7.4%; Average loss: 0.6719\n",
            "Iteration: 742; Percent complete: 7.4%; Average loss: 0.6647\n",
            "Iteration: 743; Percent complete: 7.4%; Average loss: 0.6666\n",
            "Iteration: 744; Percent complete: 7.4%; Average loss: 0.6670\n",
            "Iteration: 745; Percent complete: 7.4%; Average loss: 0.6643\n",
            "Iteration: 746; Percent complete: 7.5%; Average loss: 0.6687\n",
            "Iteration: 747; Percent complete: 7.5%; Average loss: 0.6615\n",
            "Iteration: 748; Percent complete: 7.5%; Average loss: 0.6620\n",
            "Iteration: 749; Percent complete: 7.5%; Average loss: 0.6659\n",
            "Iteration: 750; Percent complete: 7.5%; Average loss: 0.6732\n",
            "Iteration: 751; Percent complete: 7.5%; Average loss: 0.6660\n",
            "Iteration: 752; Percent complete: 7.5%; Average loss: 0.6675\n",
            "Iteration: 753; Percent complete: 7.5%; Average loss: 0.6686\n",
            "Iteration: 754; Percent complete: 7.5%; Average loss: 0.6608\n",
            "Iteration: 755; Percent complete: 7.5%; Average loss: 0.6719\n",
            "Iteration: 756; Percent complete: 7.6%; Average loss: 0.6591\n",
            "Iteration: 757; Percent complete: 7.6%; Average loss: 0.6662\n",
            "Iteration: 758; Percent complete: 7.6%; Average loss: 0.6604\n",
            "Iteration: 759; Percent complete: 7.6%; Average loss: 0.6636\n",
            "Iteration: 760; Percent complete: 7.6%; Average loss: 0.6670\n",
            "Iteration: 761; Percent complete: 7.6%; Average loss: 0.6679\n",
            "Iteration: 762; Percent complete: 7.6%; Average loss: 0.6714\n",
            "Iteration: 763; Percent complete: 7.6%; Average loss: 0.6688\n",
            "Iteration: 764; Percent complete: 7.6%; Average loss: 0.6718\n",
            "Iteration: 765; Percent complete: 7.6%; Average loss: 0.6642\n",
            "Iteration: 766; Percent complete: 7.7%; Average loss: 0.6641\n",
            "Iteration: 767; Percent complete: 7.7%; Average loss: 0.6660\n",
            "Iteration: 768; Percent complete: 7.7%; Average loss: 0.6592\n",
            "Iteration: 769; Percent complete: 7.7%; Average loss: 0.6683\n",
            "Iteration: 770; Percent complete: 7.7%; Average loss: 0.6640\n",
            "Iteration: 771; Percent complete: 7.7%; Average loss: 0.6754\n",
            "Iteration: 772; Percent complete: 7.7%; Average loss: 0.6689\n",
            "Iteration: 773; Percent complete: 7.7%; Average loss: 0.6630\n",
            "Iteration: 774; Percent complete: 7.7%; Average loss: 0.6679\n",
            "Iteration: 775; Percent complete: 7.8%; Average loss: 0.6733\n",
            "Iteration: 776; Percent complete: 7.8%; Average loss: 0.6698\n",
            "Iteration: 777; Percent complete: 7.8%; Average loss: 0.6716\n",
            "Iteration: 778; Percent complete: 7.8%; Average loss: 0.6631\n",
            "Iteration: 779; Percent complete: 7.8%; Average loss: 0.6718\n",
            "Iteration: 780; Percent complete: 7.8%; Average loss: 0.6645\n",
            "Iteration: 781; Percent complete: 7.8%; Average loss: 0.6718\n",
            "Iteration: 782; Percent complete: 7.8%; Average loss: 0.6655\n",
            "Iteration: 783; Percent complete: 7.8%; Average loss: 0.6666\n",
            "Iteration: 784; Percent complete: 7.8%; Average loss: 0.6654\n",
            "Iteration: 785; Percent complete: 7.8%; Average loss: 0.6712\n",
            "Iteration: 786; Percent complete: 7.9%; Average loss: 0.6614\n",
            "Iteration: 787; Percent complete: 7.9%; Average loss: 0.6656\n",
            "Iteration: 788; Percent complete: 7.9%; Average loss: 0.6632\n",
            "Iteration: 789; Percent complete: 7.9%; Average loss: 0.6675\n",
            "Iteration: 790; Percent complete: 7.9%; Average loss: 0.6768\n",
            "Iteration: 791; Percent complete: 7.9%; Average loss: 0.6656\n",
            "Iteration: 792; Percent complete: 7.9%; Average loss: 0.6736\n",
            "Iteration: 793; Percent complete: 7.9%; Average loss: 0.6707\n",
            "Iteration: 794; Percent complete: 7.9%; Average loss: 0.6683\n",
            "Iteration: 795; Percent complete: 8.0%; Average loss: 0.6651\n",
            "Iteration: 796; Percent complete: 8.0%; Average loss: 0.6717\n",
            "Iteration: 797; Percent complete: 8.0%; Average loss: 0.6711\n",
            "Iteration: 798; Percent complete: 8.0%; Average loss: 0.6671\n",
            "Iteration: 799; Percent complete: 8.0%; Average loss: 0.6664\n",
            "Iteration: 800; Percent complete: 8.0%; Average loss: 0.6573\n",
            "Iteration: 801; Percent complete: 8.0%; Average loss: 0.6669\n",
            "Iteration: 802; Percent complete: 8.0%; Average loss: 0.6662\n",
            "Iteration: 803; Percent complete: 8.0%; Average loss: 0.6638\n",
            "Iteration: 804; Percent complete: 8.0%; Average loss: 0.6671\n",
            "Iteration: 805; Percent complete: 8.1%; Average loss: 0.6655\n",
            "Iteration: 806; Percent complete: 8.1%; Average loss: 0.6659\n",
            "Iteration: 807; Percent complete: 8.1%; Average loss: 0.6740\n",
            "Iteration: 808; Percent complete: 8.1%; Average loss: 0.6656\n",
            "Iteration: 809; Percent complete: 8.1%; Average loss: 0.6658\n",
            "Iteration: 810; Percent complete: 8.1%; Average loss: 0.6678\n",
            "Iteration: 811; Percent complete: 8.1%; Average loss: 0.6644\n",
            "Iteration: 812; Percent complete: 8.1%; Average loss: 0.6758\n",
            "Iteration: 813; Percent complete: 8.1%; Average loss: 0.6699\n",
            "Iteration: 814; Percent complete: 8.1%; Average loss: 0.6720\n",
            "Iteration: 815; Percent complete: 8.2%; Average loss: 0.6727\n",
            "Iteration: 816; Percent complete: 8.2%; Average loss: 0.6712\n",
            "Iteration: 817; Percent complete: 8.2%; Average loss: 0.6663\n",
            "Iteration: 818; Percent complete: 8.2%; Average loss: 0.6666\n",
            "Iteration: 819; Percent complete: 8.2%; Average loss: 0.6654\n",
            "Iteration: 820; Percent complete: 8.2%; Average loss: 0.6624\n",
            "Iteration: 821; Percent complete: 8.2%; Average loss: 0.6678\n",
            "Iteration: 822; Percent complete: 8.2%; Average loss: 0.6608\n",
            "Iteration: 823; Percent complete: 8.2%; Average loss: 0.6687\n",
            "Iteration: 824; Percent complete: 8.2%; Average loss: 0.6703\n",
            "Iteration: 825; Percent complete: 8.2%; Average loss: 0.6688\n",
            "Iteration: 826; Percent complete: 8.3%; Average loss: 0.6642\n",
            "Iteration: 827; Percent complete: 8.3%; Average loss: 0.6711\n",
            "Iteration: 828; Percent complete: 8.3%; Average loss: 0.6640\n",
            "Iteration: 829; Percent complete: 8.3%; Average loss: 0.6673\n",
            "Iteration: 830; Percent complete: 8.3%; Average loss: 0.6663\n",
            "Iteration: 831; Percent complete: 8.3%; Average loss: 0.6748\n",
            "Iteration: 832; Percent complete: 8.3%; Average loss: 0.6710\n",
            "Iteration: 833; Percent complete: 8.3%; Average loss: 0.6667\n",
            "Iteration: 834; Percent complete: 8.3%; Average loss: 0.6625\n",
            "Iteration: 835; Percent complete: 8.3%; Average loss: 0.6665\n",
            "Iteration: 836; Percent complete: 8.4%; Average loss: 0.6715\n",
            "Iteration: 837; Percent complete: 8.4%; Average loss: 0.6702\n",
            "Iteration: 838; Percent complete: 8.4%; Average loss: 0.6681\n",
            "Iteration: 839; Percent complete: 8.4%; Average loss: 0.6666\n",
            "Iteration: 840; Percent complete: 8.4%; Average loss: 0.6615\n",
            "Iteration: 841; Percent complete: 8.4%; Average loss: 0.6756\n",
            "Iteration: 842; Percent complete: 8.4%; Average loss: 0.6662\n",
            "Iteration: 843; Percent complete: 8.4%; Average loss: 0.6583\n",
            "Iteration: 844; Percent complete: 8.4%; Average loss: 0.6696\n",
            "Iteration: 845; Percent complete: 8.5%; Average loss: 0.6632\n",
            "Iteration: 846; Percent complete: 8.5%; Average loss: 0.6679\n",
            "Iteration: 847; Percent complete: 8.5%; Average loss: 0.6719\n",
            "Iteration: 848; Percent complete: 8.5%; Average loss: 0.6644\n",
            "Iteration: 849; Percent complete: 8.5%; Average loss: 0.6662\n",
            "Iteration: 850; Percent complete: 8.5%; Average loss: 0.6668\n",
            "Iteration: 851; Percent complete: 8.5%; Average loss: 0.6693\n",
            "Iteration: 852; Percent complete: 8.5%; Average loss: 0.6676\n",
            "Iteration: 853; Percent complete: 8.5%; Average loss: 0.6645\n",
            "Iteration: 854; Percent complete: 8.5%; Average loss: 0.6728\n",
            "Iteration: 855; Percent complete: 8.6%; Average loss: 0.6675\n",
            "Iteration: 856; Percent complete: 8.6%; Average loss: 0.6587\n",
            "Iteration: 857; Percent complete: 8.6%; Average loss: 0.6646\n",
            "Iteration: 858; Percent complete: 8.6%; Average loss: 0.6616\n",
            "Iteration: 859; Percent complete: 8.6%; Average loss: 0.6717\n",
            "Iteration: 860; Percent complete: 8.6%; Average loss: 0.6680\n",
            "Iteration: 861; Percent complete: 8.6%; Average loss: 0.6602\n",
            "Iteration: 862; Percent complete: 8.6%; Average loss: 0.6605\n",
            "Iteration: 863; Percent complete: 8.6%; Average loss: 0.6588\n",
            "Iteration: 864; Percent complete: 8.6%; Average loss: 0.6721\n",
            "Iteration: 865; Percent complete: 8.6%; Average loss: 0.6609\n",
            "Iteration: 866; Percent complete: 8.7%; Average loss: 0.6661\n",
            "Iteration: 867; Percent complete: 8.7%; Average loss: 0.6685\n",
            "Iteration: 868; Percent complete: 8.7%; Average loss: 0.6754\n",
            "Iteration: 869; Percent complete: 8.7%; Average loss: 0.6656\n",
            "Iteration: 870; Percent complete: 8.7%; Average loss: 0.6650\n",
            "Iteration: 871; Percent complete: 8.7%; Average loss: 0.6698\n",
            "Iteration: 872; Percent complete: 8.7%; Average loss: 0.6715\n",
            "Iteration: 873; Percent complete: 8.7%; Average loss: 0.6674\n",
            "Iteration: 874; Percent complete: 8.7%; Average loss: 0.6710\n",
            "Iteration: 875; Percent complete: 8.8%; Average loss: 0.6636\n",
            "Iteration: 876; Percent complete: 8.8%; Average loss: 0.6632\n",
            "Iteration: 877; Percent complete: 8.8%; Average loss: 0.6682\n",
            "Iteration: 878; Percent complete: 8.8%; Average loss: 0.6673\n",
            "Iteration: 879; Percent complete: 8.8%; Average loss: 0.6715\n",
            "Iteration: 880; Percent complete: 8.8%; Average loss: 0.6697\n",
            "Iteration: 881; Percent complete: 8.8%; Average loss: 0.6659\n",
            "Iteration: 882; Percent complete: 8.8%; Average loss: 0.6662\n",
            "Iteration: 883; Percent complete: 8.8%; Average loss: 0.6675\n",
            "Iteration: 884; Percent complete: 8.8%; Average loss: 0.6710\n",
            "Iteration: 885; Percent complete: 8.8%; Average loss: 0.6728\n",
            "Iteration: 886; Percent complete: 8.9%; Average loss: 0.6645\n",
            "Iteration: 887; Percent complete: 8.9%; Average loss: 0.6645\n",
            "Iteration: 888; Percent complete: 8.9%; Average loss: 0.6613\n",
            "Iteration: 889; Percent complete: 8.9%; Average loss: 0.6644\n",
            "Iteration: 890; Percent complete: 8.9%; Average loss: 0.6618\n",
            "Iteration: 891; Percent complete: 8.9%; Average loss: 0.6712\n",
            "Iteration: 892; Percent complete: 8.9%; Average loss: 0.6636\n",
            "Iteration: 893; Percent complete: 8.9%; Average loss: 0.6638\n",
            "Iteration: 894; Percent complete: 8.9%; Average loss: 0.6705\n",
            "Iteration: 895; Percent complete: 8.9%; Average loss: 0.6640\n",
            "Iteration: 896; Percent complete: 9.0%; Average loss: 0.6709\n",
            "Iteration: 897; Percent complete: 9.0%; Average loss: 0.6687\n",
            "Iteration: 898; Percent complete: 9.0%; Average loss: 0.6614\n",
            "Iteration: 899; Percent complete: 9.0%; Average loss: 0.6642\n",
            "Iteration: 900; Percent complete: 9.0%; Average loss: 0.6626\n",
            "Iteration: 901; Percent complete: 9.0%; Average loss: 0.6627\n",
            "Iteration: 902; Percent complete: 9.0%; Average loss: 0.6645\n",
            "Iteration: 903; Percent complete: 9.0%; Average loss: 0.6664\n",
            "Iteration: 904; Percent complete: 9.0%; Average loss: 0.6727\n",
            "Iteration: 905; Percent complete: 9.0%; Average loss: 0.6700\n",
            "Iteration: 906; Percent complete: 9.1%; Average loss: 0.6615\n",
            "Iteration: 907; Percent complete: 9.1%; Average loss: 0.6708\n",
            "Iteration: 908; Percent complete: 9.1%; Average loss: 0.6740\n",
            "Iteration: 909; Percent complete: 9.1%; Average loss: 0.6660\n",
            "Iteration: 910; Percent complete: 9.1%; Average loss: 0.6614\n",
            "Iteration: 911; Percent complete: 9.1%; Average loss: 0.6620\n",
            "Iteration: 912; Percent complete: 9.1%; Average loss: 0.6693\n",
            "Iteration: 913; Percent complete: 9.1%; Average loss: 0.6693\n",
            "Iteration: 914; Percent complete: 9.1%; Average loss: 0.6635\n",
            "Iteration: 915; Percent complete: 9.2%; Average loss: 0.6672\n",
            "Iteration: 916; Percent complete: 9.2%; Average loss: 0.6707\n",
            "Iteration: 917; Percent complete: 9.2%; Average loss: 0.6647\n",
            "Iteration: 918; Percent complete: 9.2%; Average loss: 0.6661\n",
            "Iteration: 919; Percent complete: 9.2%; Average loss: 0.6716\n",
            "Iteration: 920; Percent complete: 9.2%; Average loss: 0.6565\n",
            "Iteration: 921; Percent complete: 9.2%; Average loss: 0.6660\n",
            "Iteration: 922; Percent complete: 9.2%; Average loss: 0.6656\n",
            "Iteration: 923; Percent complete: 9.2%; Average loss: 0.6632\n",
            "Iteration: 924; Percent complete: 9.2%; Average loss: 0.6621\n",
            "Iteration: 925; Percent complete: 9.2%; Average loss: 0.6621\n",
            "Iteration: 926; Percent complete: 9.3%; Average loss: 0.6643\n",
            "Iteration: 927; Percent complete: 9.3%; Average loss: 0.6633\n",
            "Iteration: 928; Percent complete: 9.3%; Average loss: 0.6594\n",
            "Iteration: 929; Percent complete: 9.3%; Average loss: 0.6664\n",
            "Iteration: 930; Percent complete: 9.3%; Average loss: 0.6714\n",
            "Iteration: 931; Percent complete: 9.3%; Average loss: 0.6622\n",
            "Iteration: 932; Percent complete: 9.3%; Average loss: 0.6646\n",
            "Iteration: 933; Percent complete: 9.3%; Average loss: 0.6683\n",
            "Iteration: 934; Percent complete: 9.3%; Average loss: 0.6613\n",
            "Iteration: 935; Percent complete: 9.3%; Average loss: 0.6670\n",
            "Iteration: 936; Percent complete: 9.4%; Average loss: 0.6667\n",
            "Iteration: 937; Percent complete: 9.4%; Average loss: 0.6611\n",
            "Iteration: 938; Percent complete: 9.4%; Average loss: 0.6613\n",
            "Iteration: 939; Percent complete: 9.4%; Average loss: 0.6582\n",
            "Iteration: 940; Percent complete: 9.4%; Average loss: 0.6675\n",
            "Iteration: 941; Percent complete: 9.4%; Average loss: 0.6704\n",
            "Iteration: 942; Percent complete: 9.4%; Average loss: 0.6664\n",
            "Iteration: 943; Percent complete: 9.4%; Average loss: 0.6641\n",
            "Iteration: 944; Percent complete: 9.4%; Average loss: 0.6628\n",
            "Iteration: 945; Percent complete: 9.4%; Average loss: 0.6639\n",
            "Iteration: 946; Percent complete: 9.5%; Average loss: 0.6607\n",
            "Iteration: 947; Percent complete: 9.5%; Average loss: 0.6572\n",
            "Iteration: 948; Percent complete: 9.5%; Average loss: 0.6623\n",
            "Iteration: 949; Percent complete: 9.5%; Average loss: 0.6678\n",
            "Iteration: 950; Percent complete: 9.5%; Average loss: 0.6654\n",
            "Iteration: 951; Percent complete: 9.5%; Average loss: 0.6656\n",
            "Iteration: 952; Percent complete: 9.5%; Average loss: 0.6633\n",
            "Iteration: 953; Percent complete: 9.5%; Average loss: 0.6636\n",
            "Iteration: 954; Percent complete: 9.5%; Average loss: 0.6630\n",
            "Iteration: 955; Percent complete: 9.6%; Average loss: 0.6652\n",
            "Iteration: 956; Percent complete: 9.6%; Average loss: 0.6684\n",
            "Iteration: 957; Percent complete: 9.6%; Average loss: 0.6599\n",
            "Iteration: 958; Percent complete: 9.6%; Average loss: 0.6603\n",
            "Iteration: 959; Percent complete: 9.6%; Average loss: 0.6646\n",
            "Iteration: 960; Percent complete: 9.6%; Average loss: 0.6573\n",
            "Iteration: 961; Percent complete: 9.6%; Average loss: 0.6654\n",
            "Iteration: 962; Percent complete: 9.6%; Average loss: 0.6628\n",
            "Iteration: 963; Percent complete: 9.6%; Average loss: 0.6651\n",
            "Iteration: 964; Percent complete: 9.6%; Average loss: 0.6551\n",
            "Iteration: 965; Percent complete: 9.7%; Average loss: 0.6647\n",
            "Iteration: 966; Percent complete: 9.7%; Average loss: 0.6627\n",
            "Iteration: 967; Percent complete: 9.7%; Average loss: 0.6656\n",
            "Iteration: 968; Percent complete: 9.7%; Average loss: 0.6632\n",
            "Iteration: 969; Percent complete: 9.7%; Average loss: 0.6639\n",
            "Iteration: 970; Percent complete: 9.7%; Average loss: 0.6652\n",
            "Iteration: 971; Percent complete: 9.7%; Average loss: 0.6674\n",
            "Iteration: 972; Percent complete: 9.7%; Average loss: 0.6536\n",
            "Iteration: 973; Percent complete: 9.7%; Average loss: 0.6644\n",
            "Iteration: 974; Percent complete: 9.7%; Average loss: 0.6606\n",
            "Iteration: 975; Percent complete: 9.8%; Average loss: 0.6613\n",
            "Iteration: 976; Percent complete: 9.8%; Average loss: 0.6668\n",
            "Iteration: 977; Percent complete: 9.8%; Average loss: 0.6613\n",
            "Iteration: 978; Percent complete: 9.8%; Average loss: 0.6672\n",
            "Iteration: 979; Percent complete: 9.8%; Average loss: 0.6678\n",
            "Iteration: 980; Percent complete: 9.8%; Average loss: 0.6618\n",
            "Iteration: 981; Percent complete: 9.8%; Average loss: 0.6591\n",
            "Iteration: 982; Percent complete: 9.8%; Average loss: 0.6600\n",
            "Iteration: 983; Percent complete: 9.8%; Average loss: 0.6663\n",
            "Iteration: 984; Percent complete: 9.8%; Average loss: 0.6621\n",
            "Iteration: 985; Percent complete: 9.8%; Average loss: 0.6588\n",
            "Iteration: 986; Percent complete: 9.9%; Average loss: 0.6641\n",
            "Iteration: 987; Percent complete: 9.9%; Average loss: 0.6621\n",
            "Iteration: 988; Percent complete: 9.9%; Average loss: 0.6595\n",
            "Iteration: 989; Percent complete: 9.9%; Average loss: 0.6568\n",
            "Iteration: 990; Percent complete: 9.9%; Average loss: 0.6620\n",
            "Iteration: 991; Percent complete: 9.9%; Average loss: 0.6598\n",
            "Iteration: 992; Percent complete: 9.9%; Average loss: 0.6627\n",
            "Iteration: 993; Percent complete: 9.9%; Average loss: 0.6661\n",
            "Iteration: 994; Percent complete: 9.9%; Average loss: 0.6642\n",
            "Iteration: 995; Percent complete: 10.0%; Average loss: 0.6624\n",
            "Iteration: 996; Percent complete: 10.0%; Average loss: 0.6594\n",
            "Iteration: 997; Percent complete: 10.0%; Average loss: 0.6627\n",
            "Iteration: 998; Percent complete: 10.0%; Average loss: 0.6684\n",
            "Iteration: 999; Percent complete: 10.0%; Average loss: 0.6598\n",
            "Iteration: 1000; Percent complete: 10.0%; Average loss: 0.6625\n",
            "Iteration: 1001; Percent complete: 10.0%; Average loss: 0.6619\n",
            "Iteration: 1002; Percent complete: 10.0%; Average loss: 0.6610\n",
            "Iteration: 1003; Percent complete: 10.0%; Average loss: 0.6626\n",
            "Iteration: 1004; Percent complete: 10.0%; Average loss: 0.6651\n",
            "Iteration: 1005; Percent complete: 10.1%; Average loss: 0.6623\n",
            "Iteration: 1006; Percent complete: 10.1%; Average loss: 0.6655\n",
            "Iteration: 1007; Percent complete: 10.1%; Average loss: 0.6611\n",
            "Iteration: 1008; Percent complete: 10.1%; Average loss: 0.6552\n",
            "Iteration: 1009; Percent complete: 10.1%; Average loss: 0.6617\n",
            "Iteration: 1010; Percent complete: 10.1%; Average loss: 0.6561\n",
            "Iteration: 1011; Percent complete: 10.1%; Average loss: 0.6597\n",
            "Iteration: 1012; Percent complete: 10.1%; Average loss: 0.6599\n",
            "Iteration: 1013; Percent complete: 10.1%; Average loss: 0.6626\n",
            "Iteration: 1014; Percent complete: 10.1%; Average loss: 0.6698\n",
            "Iteration: 1015; Percent complete: 10.2%; Average loss: 0.6631\n",
            "Iteration: 1016; Percent complete: 10.2%; Average loss: 0.6646\n",
            "Iteration: 1017; Percent complete: 10.2%; Average loss: 0.6626\n",
            "Iteration: 1018; Percent complete: 10.2%; Average loss: 0.6611\n",
            "Iteration: 1019; Percent complete: 10.2%; Average loss: 0.6546\n",
            "Iteration: 1020; Percent complete: 10.2%; Average loss: 0.6567\n",
            "Iteration: 1021; Percent complete: 10.2%; Average loss: 0.6580\n",
            "Iteration: 1022; Percent complete: 10.2%; Average loss: 0.6656\n",
            "Iteration: 1023; Percent complete: 10.2%; Average loss: 0.6594\n",
            "Iteration: 1024; Percent complete: 10.2%; Average loss: 0.6545\n",
            "Iteration: 1025; Percent complete: 10.2%; Average loss: 0.6548\n",
            "Iteration: 1026; Percent complete: 10.3%; Average loss: 0.6581\n",
            "Iteration: 1027; Percent complete: 10.3%; Average loss: 0.6599\n",
            "Iteration: 1028; Percent complete: 10.3%; Average loss: 0.6546\n",
            "Iteration: 1029; Percent complete: 10.3%; Average loss: 0.6599\n",
            "Iteration: 1030; Percent complete: 10.3%; Average loss: 0.6600\n",
            "Iteration: 1031; Percent complete: 10.3%; Average loss: 0.6572\n",
            "Iteration: 1032; Percent complete: 10.3%; Average loss: 0.6611\n",
            "Iteration: 1033; Percent complete: 10.3%; Average loss: 0.6606\n",
            "Iteration: 1034; Percent complete: 10.3%; Average loss: 0.6535\n",
            "Iteration: 1035; Percent complete: 10.3%; Average loss: 0.6525\n",
            "Iteration: 1036; Percent complete: 10.4%; Average loss: 0.6621\n",
            "Iteration: 1037; Percent complete: 10.4%; Average loss: 0.6507\n",
            "Iteration: 1038; Percent complete: 10.4%; Average loss: 0.6475\n",
            "Iteration: 1039; Percent complete: 10.4%; Average loss: 0.6585\n",
            "Iteration: 1040; Percent complete: 10.4%; Average loss: 0.6604\n",
            "Iteration: 1041; Percent complete: 10.4%; Average loss: 0.6543\n",
            "Iteration: 1042; Percent complete: 10.4%; Average loss: 0.6602\n",
            "Iteration: 1043; Percent complete: 10.4%; Average loss: 0.6546\n",
            "Iteration: 1044; Percent complete: 10.4%; Average loss: 0.6553\n",
            "Iteration: 1045; Percent complete: 10.4%; Average loss: 0.6579\n",
            "Iteration: 1046; Percent complete: 10.5%; Average loss: 0.6489\n",
            "Iteration: 1047; Percent complete: 10.5%; Average loss: 0.6519\n",
            "Iteration: 1048; Percent complete: 10.5%; Average loss: 0.6630\n",
            "Iteration: 1049; Percent complete: 10.5%; Average loss: 0.6510\n",
            "Iteration: 1050; Percent complete: 10.5%; Average loss: 0.6497\n",
            "Iteration: 1051; Percent complete: 10.5%; Average loss: 0.6562\n",
            "Iteration: 1052; Percent complete: 10.5%; Average loss: 0.6575\n",
            "Iteration: 1053; Percent complete: 10.5%; Average loss: 0.6446\n",
            "Iteration: 1054; Percent complete: 10.5%; Average loss: 0.6510\n",
            "Iteration: 1055; Percent complete: 10.5%; Average loss: 0.6433\n",
            "Iteration: 1056; Percent complete: 10.6%; Average loss: 0.6471\n",
            "Iteration: 1057; Percent complete: 10.6%; Average loss: 0.6507\n",
            "Iteration: 1058; Percent complete: 10.6%; Average loss: 0.6456\n",
            "Iteration: 1059; Percent complete: 10.6%; Average loss: 0.6464\n",
            "Iteration: 1060; Percent complete: 10.6%; Average loss: 0.6501\n",
            "Iteration: 1061; Percent complete: 10.6%; Average loss: 0.6388\n",
            "Iteration: 1062; Percent complete: 10.6%; Average loss: 0.6541\n",
            "Iteration: 1063; Percent complete: 10.6%; Average loss: 0.6463\n",
            "Iteration: 1064; Percent complete: 10.6%; Average loss: 0.6415\n",
            "Iteration: 1065; Percent complete: 10.7%; Average loss: 0.6405\n",
            "Iteration: 1066; Percent complete: 10.7%; Average loss: 0.6376\n",
            "Iteration: 1067; Percent complete: 10.7%; Average loss: 0.6398\n",
            "Iteration: 1068; Percent complete: 10.7%; Average loss: 0.6281\n",
            "Iteration: 1069; Percent complete: 10.7%; Average loss: 0.6460\n",
            "Iteration: 1070; Percent complete: 10.7%; Average loss: 0.6306\n",
            "Iteration: 1071; Percent complete: 10.7%; Average loss: 0.6270\n",
            "Iteration: 1072; Percent complete: 10.7%; Average loss: 0.6337\n",
            "Iteration: 1073; Percent complete: 10.7%; Average loss: 0.6153\n",
            "Iteration: 1074; Percent complete: 10.7%; Average loss: 0.6310\n",
            "Iteration: 1075; Percent complete: 10.8%; Average loss: 0.6112\n",
            "Iteration: 1076; Percent complete: 10.8%; Average loss: 0.6193\n",
            "Iteration: 1077; Percent complete: 10.8%; Average loss: 0.6379\n",
            "Iteration: 1078; Percent complete: 10.8%; Average loss: 0.6232\n",
            "Iteration: 1079; Percent complete: 10.8%; Average loss: 0.6246\n",
            "Iteration: 1080; Percent complete: 10.8%; Average loss: 0.6237\n",
            "Iteration: 1081; Percent complete: 10.8%; Average loss: 0.6015\n",
            "Iteration: 1082; Percent complete: 10.8%; Average loss: 0.6046\n",
            "Iteration: 1083; Percent complete: 10.8%; Average loss: 0.5991\n",
            "Iteration: 1084; Percent complete: 10.8%; Average loss: 0.6011\n",
            "Iteration: 1085; Percent complete: 10.8%; Average loss: 0.5891\n",
            "Iteration: 1086; Percent complete: 10.9%; Average loss: 0.6098\n",
            "Iteration: 1087; Percent complete: 10.9%; Average loss: 0.6167\n",
            "Iteration: 1088; Percent complete: 10.9%; Average loss: 0.5973\n",
            "Iteration: 1089; Percent complete: 10.9%; Average loss: 0.6219\n",
            "Iteration: 1090; Percent complete: 10.9%; Average loss: 0.6215\n",
            "Iteration: 1091; Percent complete: 10.9%; Average loss: 0.6081\n",
            "Iteration: 1092; Percent complete: 10.9%; Average loss: 0.5913\n",
            "Iteration: 1093; Percent complete: 10.9%; Average loss: 0.6201\n",
            "Iteration: 1094; Percent complete: 10.9%; Average loss: 0.6194\n",
            "Iteration: 1095; Percent complete: 10.9%; Average loss: 0.5859\n",
            "Iteration: 1096; Percent complete: 11.0%; Average loss: 0.5919\n",
            "Iteration: 1097; Percent complete: 11.0%; Average loss: 0.6068\n",
            "Iteration: 1098; Percent complete: 11.0%; Average loss: 0.5828\n",
            "Iteration: 1099; Percent complete: 11.0%; Average loss: 0.5951\n",
            "Iteration: 1100; Percent complete: 11.0%; Average loss: 0.5867\n",
            "Iteration: 1101; Percent complete: 11.0%; Average loss: 0.5852\n",
            "Iteration: 1102; Percent complete: 11.0%; Average loss: 0.6014\n",
            "Iteration: 1103; Percent complete: 11.0%; Average loss: 0.5840\n",
            "Iteration: 1104; Percent complete: 11.0%; Average loss: 0.5944\n",
            "Iteration: 1105; Percent complete: 11.1%; Average loss: 0.6027\n",
            "Iteration: 1106; Percent complete: 11.1%; Average loss: 0.5863\n",
            "Iteration: 1107; Percent complete: 11.1%; Average loss: 0.5855\n",
            "Iteration: 1108; Percent complete: 11.1%; Average loss: 0.5953\n",
            "Iteration: 1109; Percent complete: 11.1%; Average loss: 0.5887\n",
            "Iteration: 1110; Percent complete: 11.1%; Average loss: 0.5734\n",
            "Iteration: 1111; Percent complete: 11.1%; Average loss: 0.5938\n",
            "Iteration: 1112; Percent complete: 11.1%; Average loss: 0.5916\n",
            "Iteration: 1113; Percent complete: 11.1%; Average loss: 0.5798\n",
            "Iteration: 1114; Percent complete: 11.1%; Average loss: 0.5830\n",
            "Iteration: 1115; Percent complete: 11.2%; Average loss: 0.5837\n",
            "Iteration: 1116; Percent complete: 11.2%; Average loss: 0.6109\n",
            "Iteration: 1117; Percent complete: 11.2%; Average loss: 0.5922\n",
            "Iteration: 1118; Percent complete: 11.2%; Average loss: 0.5845\n",
            "Iteration: 1119; Percent complete: 11.2%; Average loss: 0.5630\n",
            "Iteration: 1120; Percent complete: 11.2%; Average loss: 0.5729\n",
            "Iteration: 1121; Percent complete: 11.2%; Average loss: 0.5872\n",
            "Iteration: 1122; Percent complete: 11.2%; Average loss: 0.5700\n",
            "Iteration: 1123; Percent complete: 11.2%; Average loss: 0.5666\n",
            "Iteration: 1124; Percent complete: 11.2%; Average loss: 0.5833\n",
            "Iteration: 1125; Percent complete: 11.2%; Average loss: 0.5789\n",
            "Iteration: 1126; Percent complete: 11.3%; Average loss: 0.5620\n",
            "Iteration: 1127; Percent complete: 11.3%; Average loss: 0.5708\n",
            "Iteration: 1128; Percent complete: 11.3%; Average loss: 0.5612\n",
            "Iteration: 1129; Percent complete: 11.3%; Average loss: 0.5616\n",
            "Iteration: 1130; Percent complete: 11.3%; Average loss: 0.5538\n",
            "Iteration: 1131; Percent complete: 11.3%; Average loss: 0.5671\n",
            "Iteration: 1132; Percent complete: 11.3%; Average loss: 0.5587\n",
            "Iteration: 1133; Percent complete: 11.3%; Average loss: 0.5641\n",
            "Iteration: 1134; Percent complete: 11.3%; Average loss: 0.5556\n",
            "Iteration: 1135; Percent complete: 11.3%; Average loss: 0.5532\n",
            "Iteration: 1136; Percent complete: 11.4%; Average loss: 0.5479\n",
            "Iteration: 1137; Percent complete: 11.4%; Average loss: 0.5538\n",
            "Iteration: 1138; Percent complete: 11.4%; Average loss: 0.5626\n",
            "Iteration: 1139; Percent complete: 11.4%; Average loss: 0.5573\n",
            "Iteration: 1140; Percent complete: 11.4%; Average loss: 0.5432\n",
            "Iteration: 1141; Percent complete: 11.4%; Average loss: 0.5682\n",
            "Iteration: 1142; Percent complete: 11.4%; Average loss: 0.5441\n",
            "Iteration: 1143; Percent complete: 11.4%; Average loss: 0.5526\n",
            "Iteration: 1144; Percent complete: 11.4%; Average loss: 0.5614\n",
            "Iteration: 1145; Percent complete: 11.5%; Average loss: 0.5488\n",
            "Iteration: 1146; Percent complete: 11.5%; Average loss: 0.5462\n",
            "Iteration: 1147; Percent complete: 11.5%; Average loss: 0.5416\n",
            "Iteration: 1148; Percent complete: 11.5%; Average loss: 0.5516\n",
            "Iteration: 1149; Percent complete: 11.5%; Average loss: 0.5493\n",
            "Iteration: 1150; Percent complete: 11.5%; Average loss: 0.5514\n",
            "Iteration: 1151; Percent complete: 11.5%; Average loss: 0.5365\n",
            "Iteration: 1152; Percent complete: 11.5%; Average loss: 0.5433\n",
            "Iteration: 1153; Percent complete: 11.5%; Average loss: 0.5382\n",
            "Iteration: 1154; Percent complete: 11.5%; Average loss: 0.5389\n",
            "Iteration: 1155; Percent complete: 11.6%; Average loss: 0.5495\n",
            "Iteration: 1156; Percent complete: 11.6%; Average loss: 0.5443\n",
            "Iteration: 1157; Percent complete: 11.6%; Average loss: 0.5666\n",
            "Iteration: 1158; Percent complete: 11.6%; Average loss: 0.5455\n",
            "Iteration: 1159; Percent complete: 11.6%; Average loss: 0.5376\n",
            "Iteration: 1160; Percent complete: 11.6%; Average loss: 0.5395\n",
            "Iteration: 1161; Percent complete: 11.6%; Average loss: 0.5391\n",
            "Iteration: 1162; Percent complete: 11.6%; Average loss: 0.5413\n",
            "Iteration: 1163; Percent complete: 11.6%; Average loss: 0.5508\n",
            "Iteration: 1164; Percent complete: 11.6%; Average loss: 0.5418\n",
            "Iteration: 1165; Percent complete: 11.7%; Average loss: 0.5412\n",
            "Iteration: 1166; Percent complete: 11.7%; Average loss: 0.5371\n",
            "Iteration: 1167; Percent complete: 11.7%; Average loss: 0.5351\n",
            "Iteration: 1168; Percent complete: 11.7%; Average loss: 0.5347\n",
            "Iteration: 1169; Percent complete: 11.7%; Average loss: 0.5365\n",
            "Iteration: 1170; Percent complete: 11.7%; Average loss: 0.5445\n",
            "Iteration: 1171; Percent complete: 11.7%; Average loss: 0.5337\n",
            "Iteration: 1172; Percent complete: 11.7%; Average loss: 0.5384\n",
            "Iteration: 1173; Percent complete: 11.7%; Average loss: 0.5411\n",
            "Iteration: 1174; Percent complete: 11.7%; Average loss: 0.5370\n",
            "Iteration: 1175; Percent complete: 11.8%; Average loss: 0.5325\n",
            "Iteration: 1176; Percent complete: 11.8%; Average loss: 0.5388\n",
            "Iteration: 1177; Percent complete: 11.8%; Average loss: 0.5380\n",
            "Iteration: 1178; Percent complete: 11.8%; Average loss: 0.5253\n",
            "Iteration: 1179; Percent complete: 11.8%; Average loss: 0.5436\n",
            "Iteration: 1180; Percent complete: 11.8%; Average loss: 0.5264\n",
            "Iteration: 1181; Percent complete: 11.8%; Average loss: 0.5448\n",
            "Iteration: 1182; Percent complete: 11.8%; Average loss: 0.5247\n",
            "Iteration: 1183; Percent complete: 11.8%; Average loss: 0.5331\n",
            "Iteration: 1184; Percent complete: 11.8%; Average loss: 0.5325\n",
            "Iteration: 1185; Percent complete: 11.8%; Average loss: 0.5225\n",
            "Iteration: 1186; Percent complete: 11.9%; Average loss: 0.5260\n",
            "Iteration: 1187; Percent complete: 11.9%; Average loss: 0.5419\n",
            "Iteration: 1188; Percent complete: 11.9%; Average loss: 0.5401\n",
            "Iteration: 1189; Percent complete: 11.9%; Average loss: 0.5391\n",
            "Iteration: 1190; Percent complete: 11.9%; Average loss: 0.5271\n",
            "Iteration: 1191; Percent complete: 11.9%; Average loss: 0.5186\n",
            "Iteration: 1192; Percent complete: 11.9%; Average loss: 0.5167\n",
            "Iteration: 1193; Percent complete: 11.9%; Average loss: 0.5318\n",
            "Iteration: 1194; Percent complete: 11.9%; Average loss: 0.5227\n",
            "Iteration: 1195; Percent complete: 11.9%; Average loss: 0.5492\n",
            "Iteration: 1196; Percent complete: 12.0%; Average loss: 0.5302\n",
            "Iteration: 1197; Percent complete: 12.0%; Average loss: 0.5117\n",
            "Iteration: 1198; Percent complete: 12.0%; Average loss: 0.5312\n",
            "Iteration: 1199; Percent complete: 12.0%; Average loss: 0.5194\n",
            "Iteration: 1200; Percent complete: 12.0%; Average loss: 0.5166\n",
            "Iteration: 1201; Percent complete: 12.0%; Average loss: 0.5259\n",
            "Iteration: 1202; Percent complete: 12.0%; Average loss: 0.5313\n",
            "Iteration: 1203; Percent complete: 12.0%; Average loss: 0.5237\n",
            "Iteration: 1204; Percent complete: 12.0%; Average loss: 0.5220\n",
            "Iteration: 1205; Percent complete: 12.0%; Average loss: 0.5366\n",
            "Iteration: 1206; Percent complete: 12.1%; Average loss: 0.5155\n",
            "Iteration: 1207; Percent complete: 12.1%; Average loss: 0.5157\n",
            "Iteration: 1208; Percent complete: 12.1%; Average loss: 0.5259\n",
            "Iteration: 1209; Percent complete: 12.1%; Average loss: 0.5100\n",
            "Iteration: 1210; Percent complete: 12.1%; Average loss: 0.5295\n",
            "Iteration: 1211; Percent complete: 12.1%; Average loss: 0.5228\n",
            "Iteration: 1212; Percent complete: 12.1%; Average loss: 0.5214\n",
            "Iteration: 1213; Percent complete: 12.1%; Average loss: 0.5268\n",
            "Iteration: 1214; Percent complete: 12.1%; Average loss: 0.5162\n",
            "Iteration: 1215; Percent complete: 12.2%; Average loss: 0.5389\n",
            "Iteration: 1216; Percent complete: 12.2%; Average loss: 0.5065\n",
            "Iteration: 1217; Percent complete: 12.2%; Average loss: 0.5345\n",
            "Iteration: 1218; Percent complete: 12.2%; Average loss: 0.5359\n",
            "Iteration: 1219; Percent complete: 12.2%; Average loss: 0.5171\n",
            "Iteration: 1220; Percent complete: 12.2%; Average loss: 0.5222\n",
            "Iteration: 1221; Percent complete: 12.2%; Average loss: 0.5146\n",
            "Iteration: 1222; Percent complete: 12.2%; Average loss: 0.5394\n",
            "Iteration: 1223; Percent complete: 12.2%; Average loss: 0.5168\n",
            "Iteration: 1224; Percent complete: 12.2%; Average loss: 0.5381\n",
            "Iteration: 1225; Percent complete: 12.2%; Average loss: 0.5252\n",
            "Iteration: 1226; Percent complete: 12.3%; Average loss: 0.5263\n",
            "Iteration: 1227; Percent complete: 12.3%; Average loss: 0.5264\n",
            "Iteration: 1228; Percent complete: 12.3%; Average loss: 0.5209\n",
            "Iteration: 1229; Percent complete: 12.3%; Average loss: 0.5248\n",
            "Iteration: 1230; Percent complete: 12.3%; Average loss: 0.5351\n",
            "Iteration: 1231; Percent complete: 12.3%; Average loss: 0.5203\n",
            "Iteration: 1232; Percent complete: 12.3%; Average loss: 0.5275\n",
            "Iteration: 1233; Percent complete: 12.3%; Average loss: 0.5164\n",
            "Iteration: 1234; Percent complete: 12.3%; Average loss: 0.5254\n",
            "Iteration: 1235; Percent complete: 12.3%; Average loss: 0.5193\n",
            "Iteration: 1236; Percent complete: 12.4%; Average loss: 0.5218\n",
            "Iteration: 1237; Percent complete: 12.4%; Average loss: 0.5148\n",
            "Iteration: 1238; Percent complete: 12.4%; Average loss: 0.5173\n",
            "Iteration: 1239; Percent complete: 12.4%; Average loss: 0.5309\n",
            "Iteration: 1240; Percent complete: 12.4%; Average loss: 0.5118\n",
            "Iteration: 1241; Percent complete: 12.4%; Average loss: 0.5272\n",
            "Iteration: 1242; Percent complete: 12.4%; Average loss: 0.5195\n",
            "Iteration: 1243; Percent complete: 12.4%; Average loss: 0.5217\n",
            "Iteration: 1244; Percent complete: 12.4%; Average loss: 0.5164\n",
            "Iteration: 1245; Percent complete: 12.4%; Average loss: 0.5160\n",
            "Iteration: 1246; Percent complete: 12.5%; Average loss: 0.5221\n",
            "Iteration: 1247; Percent complete: 12.5%; Average loss: 0.5185\n",
            "Iteration: 1248; Percent complete: 12.5%; Average loss: 0.5259\n",
            "Iteration: 1249; Percent complete: 12.5%; Average loss: 0.5164\n",
            "Iteration: 1250; Percent complete: 12.5%; Average loss: 0.5122\n",
            "Iteration: 1251; Percent complete: 12.5%; Average loss: 0.5131\n",
            "Iteration: 1252; Percent complete: 12.5%; Average loss: 0.5179\n",
            "Iteration: 1253; Percent complete: 12.5%; Average loss: 0.5094\n",
            "Iteration: 1254; Percent complete: 12.5%; Average loss: 0.5153\n",
            "Iteration: 1255; Percent complete: 12.6%; Average loss: 0.5209\n",
            "Iteration: 1256; Percent complete: 12.6%; Average loss: 0.5164\n",
            "Iteration: 1257; Percent complete: 12.6%; Average loss: 0.5228\n",
            "Iteration: 1258; Percent complete: 12.6%; Average loss: 0.5172\n",
            "Iteration: 1259; Percent complete: 12.6%; Average loss: 0.5143\n",
            "Iteration: 1260; Percent complete: 12.6%; Average loss: 0.5204\n",
            "Iteration: 1261; Percent complete: 12.6%; Average loss: 0.5152\n",
            "Iteration: 1262; Percent complete: 12.6%; Average loss: 0.5124\n",
            "Iteration: 1263; Percent complete: 12.6%; Average loss: 0.5223\n",
            "Iteration: 1264; Percent complete: 12.6%; Average loss: 0.5096\n",
            "Iteration: 1265; Percent complete: 12.7%; Average loss: 0.5150\n",
            "Iteration: 1266; Percent complete: 12.7%; Average loss: 0.5063\n",
            "Iteration: 1267; Percent complete: 12.7%; Average loss: 0.5275\n",
            "Iteration: 1268; Percent complete: 12.7%; Average loss: 0.5083\n",
            "Iteration: 1269; Percent complete: 12.7%; Average loss: 0.5187\n",
            "Iteration: 1270; Percent complete: 12.7%; Average loss: 0.5368\n",
            "Iteration: 1271; Percent complete: 12.7%; Average loss: 0.5080\n",
            "Iteration: 1272; Percent complete: 12.7%; Average loss: 0.5184\n",
            "Iteration: 1273; Percent complete: 12.7%; Average loss: 0.5110\n",
            "Iteration: 1274; Percent complete: 12.7%; Average loss: 0.5178\n",
            "Iteration: 1275; Percent complete: 12.8%; Average loss: 0.5168\n",
            "Iteration: 1276; Percent complete: 12.8%; Average loss: 0.5147\n",
            "Iteration: 1277; Percent complete: 12.8%; Average loss: 0.5275\n",
            "Iteration: 1278; Percent complete: 12.8%; Average loss: 0.5073\n",
            "Iteration: 1279; Percent complete: 12.8%; Average loss: 0.5190\n",
            "Iteration: 1280; Percent complete: 12.8%; Average loss: 0.5097\n",
            "Iteration: 1281; Percent complete: 12.8%; Average loss: 0.5184\n",
            "Iteration: 1282; Percent complete: 12.8%; Average loss: 0.5255\n",
            "Iteration: 1283; Percent complete: 12.8%; Average loss: 0.5160\n",
            "Iteration: 1284; Percent complete: 12.8%; Average loss: 0.5139\n",
            "Iteration: 1285; Percent complete: 12.8%; Average loss: 0.5234\n",
            "Iteration: 1286; Percent complete: 12.9%; Average loss: 0.5097\n",
            "Iteration: 1287; Percent complete: 12.9%; Average loss: 0.5171\n",
            "Iteration: 1288; Percent complete: 12.9%; Average loss: 0.5045\n",
            "Iteration: 1289; Percent complete: 12.9%; Average loss: 0.5234\n",
            "Iteration: 1290; Percent complete: 12.9%; Average loss: 0.5067\n",
            "Iteration: 1291; Percent complete: 12.9%; Average loss: 0.5130\n",
            "Iteration: 1292; Percent complete: 12.9%; Average loss: 0.5237\n",
            "Iteration: 1293; Percent complete: 12.9%; Average loss: 0.5101\n",
            "Iteration: 1294; Percent complete: 12.9%; Average loss: 0.5135\n",
            "Iteration: 1295; Percent complete: 13.0%; Average loss: 0.4961\n",
            "Iteration: 1296; Percent complete: 13.0%; Average loss: 0.5228\n",
            "Iteration: 1297; Percent complete: 13.0%; Average loss: 0.5200\n",
            "Iteration: 1298; Percent complete: 13.0%; Average loss: 0.5159\n",
            "Iteration: 1299; Percent complete: 13.0%; Average loss: 0.5120\n",
            "Iteration: 1300; Percent complete: 13.0%; Average loss: 0.5267\n",
            "Iteration: 1301; Percent complete: 13.0%; Average loss: 0.5200\n",
            "Iteration: 1302; Percent complete: 13.0%; Average loss: 0.5087\n",
            "Iteration: 1303; Percent complete: 13.0%; Average loss: 0.5098\n",
            "Iteration: 1304; Percent complete: 13.0%; Average loss: 0.5079\n",
            "Iteration: 1305; Percent complete: 13.1%; Average loss: 0.5152\n",
            "Iteration: 1306; Percent complete: 13.1%; Average loss: 0.5129\n",
            "Iteration: 1307; Percent complete: 13.1%; Average loss: 0.5100\n",
            "Iteration: 1308; Percent complete: 13.1%; Average loss: 0.5040\n",
            "Iteration: 1309; Percent complete: 13.1%; Average loss: 0.5293\n",
            "Iteration: 1310; Percent complete: 13.1%; Average loss: 0.4932\n",
            "Iteration: 1311; Percent complete: 13.1%; Average loss: 0.5065\n",
            "Iteration: 1312; Percent complete: 13.1%; Average loss: 0.5108\n",
            "Iteration: 1313; Percent complete: 13.1%; Average loss: 0.5173\n",
            "Iteration: 1314; Percent complete: 13.1%; Average loss: 0.5082\n",
            "Iteration: 1315; Percent complete: 13.2%; Average loss: 0.5015\n",
            "Iteration: 1316; Percent complete: 13.2%; Average loss: 0.4992\n",
            "Iteration: 1317; Percent complete: 13.2%; Average loss: 0.5054\n",
            "Iteration: 1318; Percent complete: 13.2%; Average loss: 0.5106\n",
            "Iteration: 1319; Percent complete: 13.2%; Average loss: 0.5070\n",
            "Iteration: 1320; Percent complete: 13.2%; Average loss: 0.5193\n",
            "Iteration: 1321; Percent complete: 13.2%; Average loss: 0.5061\n",
            "Iteration: 1322; Percent complete: 13.2%; Average loss: 0.5196\n",
            "Iteration: 1323; Percent complete: 13.2%; Average loss: 0.5149\n",
            "Iteration: 1324; Percent complete: 13.2%; Average loss: 0.5236\n",
            "Iteration: 1325; Percent complete: 13.2%; Average loss: 0.5106\n",
            "Iteration: 1326; Percent complete: 13.3%; Average loss: 0.5181\n",
            "Iteration: 1327; Percent complete: 13.3%; Average loss: 0.5311\n",
            "Iteration: 1328; Percent complete: 13.3%; Average loss: 0.5117\n",
            "Iteration: 1329; Percent complete: 13.3%; Average loss: 0.5116\n",
            "Iteration: 1330; Percent complete: 13.3%; Average loss: 0.5055\n",
            "Iteration: 1331; Percent complete: 13.3%; Average loss: 0.5061\n",
            "Iteration: 1332; Percent complete: 13.3%; Average loss: 0.5055\n",
            "Iteration: 1333; Percent complete: 13.3%; Average loss: 0.5090\n",
            "Iteration: 1334; Percent complete: 13.3%; Average loss: 0.5204\n",
            "Iteration: 1335; Percent complete: 13.4%; Average loss: 0.5239\n",
            "Iteration: 1336; Percent complete: 13.4%; Average loss: 0.5092\n",
            "Iteration: 1337; Percent complete: 13.4%; Average loss: 0.5068\n",
            "Iteration: 1338; Percent complete: 13.4%; Average loss: 0.5183\n",
            "Iteration: 1339; Percent complete: 13.4%; Average loss: 0.5036\n",
            "Iteration: 1340; Percent complete: 13.4%; Average loss: 0.5110\n",
            "Iteration: 1341; Percent complete: 13.4%; Average loss: 0.5104\n",
            "Iteration: 1342; Percent complete: 13.4%; Average loss: 0.5177\n",
            "Iteration: 1343; Percent complete: 13.4%; Average loss: 0.5152\n",
            "Iteration: 1344; Percent complete: 13.4%; Average loss: 0.5102\n",
            "Iteration: 1345; Percent complete: 13.5%; Average loss: 0.5112\n",
            "Iteration: 1346; Percent complete: 13.5%; Average loss: 0.4977\n",
            "Iteration: 1347; Percent complete: 13.5%; Average loss: 0.5136\n",
            "Iteration: 1348; Percent complete: 13.5%; Average loss: 0.5159\n",
            "Iteration: 1349; Percent complete: 13.5%; Average loss: 0.5227\n",
            "Iteration: 1350; Percent complete: 13.5%; Average loss: 0.5122\n",
            "Iteration: 1351; Percent complete: 13.5%; Average loss: 0.5194\n",
            "Iteration: 1352; Percent complete: 13.5%; Average loss: 0.5105\n",
            "Iteration: 1353; Percent complete: 13.5%; Average loss: 0.5144\n",
            "Iteration: 1354; Percent complete: 13.5%; Average loss: 0.5123\n",
            "Iteration: 1355; Percent complete: 13.6%; Average loss: 0.5010\n",
            "Iteration: 1356; Percent complete: 13.6%; Average loss: 0.5139\n",
            "Iteration: 1357; Percent complete: 13.6%; Average loss: 0.5077\n",
            "Iteration: 1358; Percent complete: 13.6%; Average loss: 0.5184\n",
            "Iteration: 1359; Percent complete: 13.6%; Average loss: 0.5072\n",
            "Iteration: 1360; Percent complete: 13.6%; Average loss: 0.5170\n",
            "Iteration: 1361; Percent complete: 13.6%; Average loss: 0.5114\n",
            "Iteration: 1362; Percent complete: 13.6%; Average loss: 0.5107\n",
            "Iteration: 1363; Percent complete: 13.6%; Average loss: 0.5112\n",
            "Iteration: 1364; Percent complete: 13.6%; Average loss: 0.5103\n",
            "Iteration: 1365; Percent complete: 13.7%; Average loss: 0.5242\n",
            "Iteration: 1366; Percent complete: 13.7%; Average loss: 0.5099\n",
            "Iteration: 1367; Percent complete: 13.7%; Average loss: 0.5180\n",
            "Iteration: 1368; Percent complete: 13.7%; Average loss: 0.5089\n",
            "Iteration: 1369; Percent complete: 13.7%; Average loss: 0.5115\n",
            "Iteration: 1370; Percent complete: 13.7%; Average loss: 0.5303\n",
            "Iteration: 1371; Percent complete: 13.7%; Average loss: 0.5041\n",
            "Iteration: 1372; Percent complete: 13.7%; Average loss: 0.5139\n",
            "Iteration: 1373; Percent complete: 13.7%; Average loss: 0.5068\n",
            "Iteration: 1374; Percent complete: 13.7%; Average loss: 0.5014\n",
            "Iteration: 1375; Percent complete: 13.8%; Average loss: 0.5115\n",
            "Iteration: 1376; Percent complete: 13.8%; Average loss: 0.4998\n",
            "Iteration: 1377; Percent complete: 13.8%; Average loss: 0.5081\n",
            "Iteration: 1378; Percent complete: 13.8%; Average loss: 0.5085\n",
            "Iteration: 1379; Percent complete: 13.8%; Average loss: 0.5112\n",
            "Iteration: 1380; Percent complete: 13.8%; Average loss: 0.5058\n",
            "Iteration: 1381; Percent complete: 13.8%; Average loss: 0.4933\n",
            "Iteration: 1382; Percent complete: 13.8%; Average loss: 0.5234\n",
            "Iteration: 1383; Percent complete: 13.8%; Average loss: 0.5060\n",
            "Iteration: 1384; Percent complete: 13.8%; Average loss: 0.5080\n",
            "Iteration: 1385; Percent complete: 13.9%; Average loss: 0.5135\n",
            "Iteration: 1386; Percent complete: 13.9%; Average loss: 0.4980\n",
            "Iteration: 1387; Percent complete: 13.9%; Average loss: 0.5073\n",
            "Iteration: 1388; Percent complete: 13.9%; Average loss: 0.4915\n",
            "Iteration: 1389; Percent complete: 13.9%; Average loss: 0.5074\n",
            "Iteration: 1390; Percent complete: 13.9%; Average loss: 0.5060\n",
            "Iteration: 1391; Percent complete: 13.9%; Average loss: 0.5073\n",
            "Iteration: 1392; Percent complete: 13.9%; Average loss: 0.5278\n",
            "Iteration: 1393; Percent complete: 13.9%; Average loss: 0.5130\n",
            "Iteration: 1394; Percent complete: 13.9%; Average loss: 0.5101\n",
            "Iteration: 1395; Percent complete: 14.0%; Average loss: 0.5088\n",
            "Iteration: 1396; Percent complete: 14.0%; Average loss: 0.5084\n",
            "Iteration: 1397; Percent complete: 14.0%; Average loss: 0.5141\n",
            "Iteration: 1398; Percent complete: 14.0%; Average loss: 0.5131\n",
            "Iteration: 1399; Percent complete: 14.0%; Average loss: 0.5036\n",
            "Iteration: 1400; Percent complete: 14.0%; Average loss: 0.5146\n",
            "Iteration: 1401; Percent complete: 14.0%; Average loss: 0.5121\n",
            "Iteration: 1402; Percent complete: 14.0%; Average loss: 0.5030\n",
            "Iteration: 1403; Percent complete: 14.0%; Average loss: 0.5160\n",
            "Iteration: 1404; Percent complete: 14.0%; Average loss: 0.5062\n",
            "Iteration: 1405; Percent complete: 14.1%; Average loss: 0.5104\n",
            "Iteration: 1406; Percent complete: 14.1%; Average loss: 0.5120\n",
            "Iteration: 1407; Percent complete: 14.1%; Average loss: 0.4971\n",
            "Iteration: 1408; Percent complete: 14.1%; Average loss: 0.5040\n",
            "Iteration: 1409; Percent complete: 14.1%; Average loss: 0.4969\n",
            "Iteration: 1410; Percent complete: 14.1%; Average loss: 0.5258\n",
            "Iteration: 1411; Percent complete: 14.1%; Average loss: 0.5143\n",
            "Iteration: 1412; Percent complete: 14.1%; Average loss: 0.5054\n",
            "Iteration: 1413; Percent complete: 14.1%; Average loss: 0.5061\n",
            "Iteration: 1414; Percent complete: 14.1%; Average loss: 0.5136\n",
            "Iteration: 1415; Percent complete: 14.1%; Average loss: 0.5045\n",
            "Iteration: 1416; Percent complete: 14.2%; Average loss: 0.5013\n",
            "Iteration: 1417; Percent complete: 14.2%; Average loss: 0.5027\n",
            "Iteration: 1418; Percent complete: 14.2%; Average loss: 0.4904\n",
            "Iteration: 1419; Percent complete: 14.2%; Average loss: 0.4933\n",
            "Iteration: 1420; Percent complete: 14.2%; Average loss: 0.5055\n",
            "Iteration: 1421; Percent complete: 14.2%; Average loss: 0.5028\n",
            "Iteration: 1422; Percent complete: 14.2%; Average loss: 0.5060\n",
            "Iteration: 1423; Percent complete: 14.2%; Average loss: 0.5101\n",
            "Iteration: 1424; Percent complete: 14.2%; Average loss: 0.5052\n",
            "Iteration: 1425; Percent complete: 14.2%; Average loss: 0.5033\n",
            "Iteration: 1426; Percent complete: 14.3%; Average loss: 0.5162\n",
            "Iteration: 1427; Percent complete: 14.3%; Average loss: 0.5107\n",
            "Iteration: 1428; Percent complete: 14.3%; Average loss: 0.5033\n",
            "Iteration: 1429; Percent complete: 14.3%; Average loss: 0.5134\n",
            "Iteration: 1430; Percent complete: 14.3%; Average loss: 0.5044\n",
            "Iteration: 1431; Percent complete: 14.3%; Average loss: 0.5179\n",
            "Iteration: 1432; Percent complete: 14.3%; Average loss: 0.5108\n",
            "Iteration: 1433; Percent complete: 14.3%; Average loss: 0.5097\n",
            "Iteration: 1434; Percent complete: 14.3%; Average loss: 0.5066\n",
            "Iteration: 1435; Percent complete: 14.3%; Average loss: 0.5048\n",
            "Iteration: 1436; Percent complete: 14.4%; Average loss: 0.5155\n",
            "Iteration: 1437; Percent complete: 14.4%; Average loss: 0.5097\n",
            "Iteration: 1438; Percent complete: 14.4%; Average loss: 0.5016\n",
            "Iteration: 1439; Percent complete: 14.4%; Average loss: 0.5074\n",
            "Iteration: 1440; Percent complete: 14.4%; Average loss: 0.5049\n",
            "Iteration: 1441; Percent complete: 14.4%; Average loss: 0.5006\n",
            "Iteration: 1442; Percent complete: 14.4%; Average loss: 0.5123\n",
            "Iteration: 1443; Percent complete: 14.4%; Average loss: 0.5105\n",
            "Iteration: 1444; Percent complete: 14.4%; Average loss: 0.5160\n",
            "Iteration: 1445; Percent complete: 14.4%; Average loss: 0.5048\n",
            "Iteration: 1446; Percent complete: 14.5%; Average loss: 0.5115\n",
            "Iteration: 1447; Percent complete: 14.5%; Average loss: 0.5111\n",
            "Iteration: 1448; Percent complete: 14.5%; Average loss: 0.5048\n",
            "Iteration: 1449; Percent complete: 14.5%; Average loss: 0.5001\n",
            "Iteration: 1450; Percent complete: 14.5%; Average loss: 0.5143\n",
            "Iteration: 1451; Percent complete: 14.5%; Average loss: 0.5120\n",
            "Iteration: 1452; Percent complete: 14.5%; Average loss: 0.5106\n",
            "Iteration: 1453; Percent complete: 14.5%; Average loss: 0.5074\n",
            "Iteration: 1454; Percent complete: 14.5%; Average loss: 0.5016\n",
            "Iteration: 1455; Percent complete: 14.5%; Average loss: 0.5055\n",
            "Iteration: 1456; Percent complete: 14.6%; Average loss: 0.5201\n",
            "Iteration: 1457; Percent complete: 14.6%; Average loss: 0.4951\n",
            "Iteration: 1458; Percent complete: 14.6%; Average loss: 0.5178\n",
            "Iteration: 1459; Percent complete: 14.6%; Average loss: 0.5129\n",
            "Iteration: 1460; Percent complete: 14.6%; Average loss: 0.5138\n",
            "Iteration: 1461; Percent complete: 14.6%; Average loss: 0.5073\n",
            "Iteration: 1462; Percent complete: 14.6%; Average loss: 0.5084\n",
            "Iteration: 1463; Percent complete: 14.6%; Average loss: 0.5081\n",
            "Iteration: 1464; Percent complete: 14.6%; Average loss: 0.5091\n",
            "Iteration: 1465; Percent complete: 14.6%; Average loss: 0.5125\n",
            "Iteration: 1466; Percent complete: 14.7%; Average loss: 0.5055\n",
            "Iteration: 1467; Percent complete: 14.7%; Average loss: 0.5095\n",
            "Iteration: 1468; Percent complete: 14.7%; Average loss: 0.5087\n",
            "Iteration: 1469; Percent complete: 14.7%; Average loss: 0.5041\n",
            "Iteration: 1470; Percent complete: 14.7%; Average loss: 0.5122\n",
            "Iteration: 1471; Percent complete: 14.7%; Average loss: 0.5016\n",
            "Iteration: 1472; Percent complete: 14.7%; Average loss: 0.5020\n",
            "Iteration: 1473; Percent complete: 14.7%; Average loss: 0.5043\n",
            "Iteration: 1474; Percent complete: 14.7%; Average loss: 0.5071\n",
            "Iteration: 1475; Percent complete: 14.8%; Average loss: 0.5166\n",
            "Iteration: 1476; Percent complete: 14.8%; Average loss: 0.5270\n",
            "Iteration: 1477; Percent complete: 14.8%; Average loss: 0.5121\n",
            "Iteration: 1478; Percent complete: 14.8%; Average loss: 0.5091\n",
            "Iteration: 1479; Percent complete: 14.8%; Average loss: 0.5141\n",
            "Iteration: 1480; Percent complete: 14.8%; Average loss: 0.4959\n",
            "Iteration: 1481; Percent complete: 14.8%; Average loss: 0.4954\n",
            "Iteration: 1482; Percent complete: 14.8%; Average loss: 0.5043\n",
            "Iteration: 1483; Percent complete: 14.8%; Average loss: 0.5068\n",
            "Iteration: 1484; Percent complete: 14.8%; Average loss: 0.4902\n",
            "Iteration: 1485; Percent complete: 14.8%; Average loss: 0.4999\n",
            "Iteration: 1486; Percent complete: 14.9%; Average loss: 0.5071\n",
            "Iteration: 1487; Percent complete: 14.9%; Average loss: 0.5056\n",
            "Iteration: 1488; Percent complete: 14.9%; Average loss: 0.4983\n",
            "Iteration: 1489; Percent complete: 14.9%; Average loss: 0.5242\n",
            "Iteration: 1490; Percent complete: 14.9%; Average loss: 0.4962\n",
            "Iteration: 1491; Percent complete: 14.9%; Average loss: 0.5261\n",
            "Iteration: 1492; Percent complete: 14.9%; Average loss: 0.5241\n",
            "Iteration: 1493; Percent complete: 14.9%; Average loss: 0.5021\n",
            "Iteration: 1494; Percent complete: 14.9%; Average loss: 0.4962\n",
            "Iteration: 1495; Percent complete: 14.9%; Average loss: 0.5068\n",
            "Iteration: 1496; Percent complete: 15.0%; Average loss: 0.5156\n",
            "Iteration: 1497; Percent complete: 15.0%; Average loss: 0.4948\n",
            "Iteration: 1498; Percent complete: 15.0%; Average loss: 0.5033\n",
            "Iteration: 1499; Percent complete: 15.0%; Average loss: 0.5029\n",
            "Iteration: 1500; Percent complete: 15.0%; Average loss: 0.4982\n",
            "Iteration: 1501; Percent complete: 15.0%; Average loss: 0.5122\n",
            "Iteration: 1502; Percent complete: 15.0%; Average loss: 0.5011\n",
            "Iteration: 1503; Percent complete: 15.0%; Average loss: 0.5138\n",
            "Iteration: 1504; Percent complete: 15.0%; Average loss: 0.5038\n",
            "Iteration: 1505; Percent complete: 15.0%; Average loss: 0.5002\n",
            "Iteration: 1506; Percent complete: 15.1%; Average loss: 0.5136\n",
            "Iteration: 1507; Percent complete: 15.1%; Average loss: 0.5091\n",
            "Iteration: 1508; Percent complete: 15.1%; Average loss: 0.5075\n",
            "Iteration: 1509; Percent complete: 15.1%; Average loss: 0.5166\n",
            "Iteration: 1510; Percent complete: 15.1%; Average loss: 0.5048\n",
            "Iteration: 1511; Percent complete: 15.1%; Average loss: 0.4901\n",
            "Iteration: 1512; Percent complete: 15.1%; Average loss: 0.4940\n",
            "Iteration: 1513; Percent complete: 15.1%; Average loss: 0.5110\n",
            "Iteration: 1514; Percent complete: 15.1%; Average loss: 0.5074\n",
            "Iteration: 1515; Percent complete: 15.2%; Average loss: 0.5015\n",
            "Iteration: 1516; Percent complete: 15.2%; Average loss: 0.5077\n",
            "Iteration: 1517; Percent complete: 15.2%; Average loss: 0.5149\n",
            "Iteration: 1518; Percent complete: 15.2%; Average loss: 0.5020\n",
            "Iteration: 1519; Percent complete: 15.2%; Average loss: 0.5098\n",
            "Iteration: 1520; Percent complete: 15.2%; Average loss: 0.4961\n",
            "Iteration: 1521; Percent complete: 15.2%; Average loss: 0.4985\n",
            "Iteration: 1522; Percent complete: 15.2%; Average loss: 0.4937\n",
            "Iteration: 1523; Percent complete: 15.2%; Average loss: 0.5014\n",
            "Iteration: 1524; Percent complete: 15.2%; Average loss: 0.5084\n",
            "Iteration: 1525; Percent complete: 15.2%; Average loss: 0.5140\n",
            "Iteration: 1526; Percent complete: 15.3%; Average loss: 0.4961\n",
            "Iteration: 1527; Percent complete: 15.3%; Average loss: 0.5081\n",
            "Iteration: 1528; Percent complete: 15.3%; Average loss: 0.4905\n",
            "Iteration: 1529; Percent complete: 15.3%; Average loss: 0.4904\n",
            "Iteration: 1530; Percent complete: 15.3%; Average loss: 0.5072\n",
            "Iteration: 1531; Percent complete: 15.3%; Average loss: 0.5054\n",
            "Iteration: 1532; Percent complete: 15.3%; Average loss: 0.5098\n",
            "Iteration: 1533; Percent complete: 15.3%; Average loss: 0.5083\n",
            "Iteration: 1534; Percent complete: 15.3%; Average loss: 0.4940\n",
            "Iteration: 1535; Percent complete: 15.3%; Average loss: 0.5062\n",
            "Iteration: 1536; Percent complete: 15.4%; Average loss: 0.5055\n",
            "Iteration: 1537; Percent complete: 15.4%; Average loss: 0.5046\n",
            "Iteration: 1538; Percent complete: 15.4%; Average loss: 0.5090\n",
            "Iteration: 1539; Percent complete: 15.4%; Average loss: 0.5035\n",
            "Iteration: 1540; Percent complete: 15.4%; Average loss: 0.5167\n",
            "Iteration: 1541; Percent complete: 15.4%; Average loss: 0.4962\n",
            "Iteration: 1542; Percent complete: 15.4%; Average loss: 0.5070\n",
            "Iteration: 1543; Percent complete: 15.4%; Average loss: 0.5177\n",
            "Iteration: 1544; Percent complete: 15.4%; Average loss: 0.5166\n",
            "Iteration: 1545; Percent complete: 15.4%; Average loss: 0.4926\n",
            "Iteration: 1546; Percent complete: 15.5%; Average loss: 0.5113\n",
            "Iteration: 1547; Percent complete: 15.5%; Average loss: 0.5056\n",
            "Iteration: 1548; Percent complete: 15.5%; Average loss: 0.4954\n",
            "Iteration: 1549; Percent complete: 15.5%; Average loss: 0.4978\n",
            "Iteration: 1550; Percent complete: 15.5%; Average loss: 0.5071\n",
            "Iteration: 1551; Percent complete: 15.5%; Average loss: 0.5038\n",
            "Iteration: 1552; Percent complete: 15.5%; Average loss: 0.5117\n",
            "Iteration: 1553; Percent complete: 15.5%; Average loss: 0.4901\n",
            "Iteration: 1554; Percent complete: 15.5%; Average loss: 0.5053\n",
            "Iteration: 1555; Percent complete: 15.6%; Average loss: 0.4960\n",
            "Iteration: 1556; Percent complete: 15.6%; Average loss: 0.4902\n",
            "Iteration: 1557; Percent complete: 15.6%; Average loss: 0.4948\n",
            "Iteration: 1558; Percent complete: 15.6%; Average loss: 0.4962\n",
            "Iteration: 1559; Percent complete: 15.6%; Average loss: 0.5135\n",
            "Iteration: 1560; Percent complete: 15.6%; Average loss: 0.4996\n",
            "Iteration: 1561; Percent complete: 15.6%; Average loss: 0.4952\n",
            "Iteration: 1562; Percent complete: 15.6%; Average loss: 0.5119\n",
            "Iteration: 1563; Percent complete: 15.6%; Average loss: 0.4991\n",
            "Iteration: 1564; Percent complete: 15.6%; Average loss: 0.5142\n",
            "Iteration: 1565; Percent complete: 15.7%; Average loss: 0.5083\n",
            "Iteration: 1566; Percent complete: 15.7%; Average loss: 0.4856\n",
            "Iteration: 1567; Percent complete: 15.7%; Average loss: 0.4931\n",
            "Iteration: 1568; Percent complete: 15.7%; Average loss: 0.5012\n",
            "Iteration: 1569; Percent complete: 15.7%; Average loss: 0.4989\n",
            "Iteration: 1570; Percent complete: 15.7%; Average loss: 0.4981\n",
            "Iteration: 1571; Percent complete: 15.7%; Average loss: 0.4959\n",
            "Iteration: 1572; Percent complete: 15.7%; Average loss: 0.4877\n",
            "Iteration: 1573; Percent complete: 15.7%; Average loss: 0.5059\n",
            "Iteration: 1574; Percent complete: 15.7%; Average loss: 0.4959\n",
            "Iteration: 1575; Percent complete: 15.8%; Average loss: 0.5089\n",
            "Iteration: 1576; Percent complete: 15.8%; Average loss: 0.4997\n",
            "Iteration: 1577; Percent complete: 15.8%; Average loss: 0.4978\n",
            "Iteration: 1578; Percent complete: 15.8%; Average loss: 0.5026\n",
            "Iteration: 1579; Percent complete: 15.8%; Average loss: 0.5147\n",
            "Iteration: 1580; Percent complete: 15.8%; Average loss: 0.4880\n",
            "Iteration: 1581; Percent complete: 15.8%; Average loss: 0.5009\n",
            "Iteration: 1582; Percent complete: 15.8%; Average loss: 0.5053\n",
            "Iteration: 1583; Percent complete: 15.8%; Average loss: 0.4940\n",
            "Iteration: 1584; Percent complete: 15.8%; Average loss: 0.5091\n",
            "Iteration: 1585; Percent complete: 15.8%; Average loss: 0.4980\n",
            "Iteration: 1586; Percent complete: 15.9%; Average loss: 0.5081\n",
            "Iteration: 1587; Percent complete: 15.9%; Average loss: 0.4950\n",
            "Iteration: 1588; Percent complete: 15.9%; Average loss: 0.5014\n",
            "Iteration: 1589; Percent complete: 15.9%; Average loss: 0.5034\n",
            "Iteration: 1590; Percent complete: 15.9%; Average loss: 0.5204\n",
            "Iteration: 1591; Percent complete: 15.9%; Average loss: 0.4859\n",
            "Iteration: 1592; Percent complete: 15.9%; Average loss: 0.4896\n",
            "Iteration: 1593; Percent complete: 15.9%; Average loss: 0.5174\n",
            "Iteration: 1594; Percent complete: 15.9%; Average loss: 0.5019\n",
            "Iteration: 1595; Percent complete: 16.0%; Average loss: 0.5056\n",
            "Iteration: 1596; Percent complete: 16.0%; Average loss: 0.5006\n",
            "Iteration: 1597; Percent complete: 16.0%; Average loss: 0.5125\n",
            "Iteration: 1598; Percent complete: 16.0%; Average loss: 0.5028\n",
            "Iteration: 1599; Percent complete: 16.0%; Average loss: 0.4990\n",
            "Iteration: 1600; Percent complete: 16.0%; Average loss: 0.5099\n",
            "Iteration: 1601; Percent complete: 16.0%; Average loss: 0.5079\n",
            "Iteration: 1602; Percent complete: 16.0%; Average loss: 0.4999\n",
            "Iteration: 1603; Percent complete: 16.0%; Average loss: 0.5028\n",
            "Iteration: 1604; Percent complete: 16.0%; Average loss: 0.5142\n",
            "Iteration: 1605; Percent complete: 16.1%; Average loss: 0.5465\n",
            "Iteration: 1606; Percent complete: 16.1%; Average loss: 0.5115\n",
            "Iteration: 1607; Percent complete: 16.1%; Average loss: 0.5037\n",
            "Iteration: 1608; Percent complete: 16.1%; Average loss: 0.4981\n",
            "Iteration: 1609; Percent complete: 16.1%; Average loss: 0.4901\n",
            "Iteration: 1610; Percent complete: 16.1%; Average loss: 0.5060\n",
            "Iteration: 1611; Percent complete: 16.1%; Average loss: 0.4994\n",
            "Iteration: 1612; Percent complete: 16.1%; Average loss: 0.5210\n",
            "Iteration: 1613; Percent complete: 16.1%; Average loss: 0.5001\n",
            "Iteration: 1614; Percent complete: 16.1%; Average loss: 0.5054\n",
            "Iteration: 1615; Percent complete: 16.2%; Average loss: 0.5032\n",
            "Iteration: 1616; Percent complete: 16.2%; Average loss: 0.5152\n",
            "Iteration: 1617; Percent complete: 16.2%; Average loss: 0.5016\n",
            "Iteration: 1618; Percent complete: 16.2%; Average loss: 0.5047\n",
            "Iteration: 1619; Percent complete: 16.2%; Average loss: 0.5017\n",
            "Iteration: 1620; Percent complete: 16.2%; Average loss: 0.5088\n",
            "Iteration: 1621; Percent complete: 16.2%; Average loss: 0.5137\n",
            "Iteration: 1622; Percent complete: 16.2%; Average loss: 0.4980\n",
            "Iteration: 1623; Percent complete: 16.2%; Average loss: 0.5125\n",
            "Iteration: 1624; Percent complete: 16.2%; Average loss: 0.5176\n",
            "Iteration: 1625; Percent complete: 16.2%; Average loss: 0.4884\n",
            "Iteration: 1626; Percent complete: 16.3%; Average loss: 0.5043\n",
            "Iteration: 1627; Percent complete: 16.3%; Average loss: 0.4928\n",
            "Iteration: 1628; Percent complete: 16.3%; Average loss: 0.5081\n",
            "Iteration: 1629; Percent complete: 16.3%; Average loss: 0.5176\n",
            "Iteration: 1630; Percent complete: 16.3%; Average loss: 0.5008\n",
            "Iteration: 1631; Percent complete: 16.3%; Average loss: 0.4870\n",
            "Iteration: 1632; Percent complete: 16.3%; Average loss: 0.5033\n",
            "Iteration: 1633; Percent complete: 16.3%; Average loss: 0.4913\n",
            "Iteration: 1634; Percent complete: 16.3%; Average loss: 0.4929\n",
            "Iteration: 1635; Percent complete: 16.4%; Average loss: 0.5089\n",
            "Iteration: 1636; Percent complete: 16.4%; Average loss: 0.5014\n",
            "Iteration: 1637; Percent complete: 16.4%; Average loss: 0.4914\n",
            "Iteration: 1638; Percent complete: 16.4%; Average loss: 0.5079\n",
            "Iteration: 1639; Percent complete: 16.4%; Average loss: 0.5027\n",
            "Iteration: 1640; Percent complete: 16.4%; Average loss: 0.4943\n",
            "Iteration: 1641; Percent complete: 16.4%; Average loss: 0.4971\n",
            "Iteration: 1642; Percent complete: 16.4%; Average loss: 0.4972\n",
            "Iteration: 1643; Percent complete: 16.4%; Average loss: 0.5132\n",
            "Iteration: 1644; Percent complete: 16.4%; Average loss: 0.4955\n",
            "Iteration: 1645; Percent complete: 16.4%; Average loss: 0.4935\n",
            "Iteration: 1646; Percent complete: 16.5%; Average loss: 0.5132\n",
            "Iteration: 1647; Percent complete: 16.5%; Average loss: 0.5084\n",
            "Iteration: 1648; Percent complete: 16.5%; Average loss: 0.5008\n",
            "Iteration: 1649; Percent complete: 16.5%; Average loss: 0.4997\n",
            "Iteration: 1650; Percent complete: 16.5%; Average loss: 0.4913\n",
            "Iteration: 1651; Percent complete: 16.5%; Average loss: 0.4982\n",
            "Iteration: 1652; Percent complete: 16.5%; Average loss: 0.5109\n",
            "Iteration: 1653; Percent complete: 16.5%; Average loss: 0.5084\n",
            "Iteration: 1654; Percent complete: 16.5%; Average loss: 0.4990\n",
            "Iteration: 1655; Percent complete: 16.6%; Average loss: 0.4949\n",
            "Iteration: 1656; Percent complete: 16.6%; Average loss: 0.4939\n",
            "Iteration: 1657; Percent complete: 16.6%; Average loss: 0.4911\n",
            "Iteration: 1658; Percent complete: 16.6%; Average loss: 0.4906\n",
            "Iteration: 1659; Percent complete: 16.6%; Average loss: 0.4908\n",
            "Iteration: 1660; Percent complete: 16.6%; Average loss: 0.5023\n",
            "Iteration: 1661; Percent complete: 16.6%; Average loss: 0.5012\n",
            "Iteration: 1662; Percent complete: 16.6%; Average loss: 0.5137\n",
            "Iteration: 1663; Percent complete: 16.6%; Average loss: 0.4931\n",
            "Iteration: 1664; Percent complete: 16.6%; Average loss: 0.4886\n",
            "Iteration: 1665; Percent complete: 16.7%; Average loss: 0.5120\n",
            "Iteration: 1666; Percent complete: 16.7%; Average loss: 0.5051\n",
            "Iteration: 1667; Percent complete: 16.7%; Average loss: 0.4836\n",
            "Iteration: 1668; Percent complete: 16.7%; Average loss: 0.4983\n",
            "Iteration: 1669; Percent complete: 16.7%; Average loss: 0.4905\n",
            "Iteration: 1670; Percent complete: 16.7%; Average loss: 0.4998\n",
            "Iteration: 1671; Percent complete: 16.7%; Average loss: 0.5152\n",
            "Iteration: 1672; Percent complete: 16.7%; Average loss: 0.4915\n",
            "Iteration: 1673; Percent complete: 16.7%; Average loss: 0.5027\n",
            "Iteration: 1674; Percent complete: 16.7%; Average loss: 0.4990\n",
            "Iteration: 1675; Percent complete: 16.8%; Average loss: 0.4976\n",
            "Iteration: 1676; Percent complete: 16.8%; Average loss: 0.4967\n",
            "Iteration: 1677; Percent complete: 16.8%; Average loss: 0.4971\n",
            "Iteration: 1678; Percent complete: 16.8%; Average loss: 0.5044\n",
            "Iteration: 1679; Percent complete: 16.8%; Average loss: 0.5017\n",
            "Iteration: 1680; Percent complete: 16.8%; Average loss: 0.4965\n",
            "Iteration: 1681; Percent complete: 16.8%; Average loss: 0.4920\n",
            "Iteration: 1682; Percent complete: 16.8%; Average loss: 0.5100\n",
            "Iteration: 1683; Percent complete: 16.8%; Average loss: 0.4991\n",
            "Iteration: 1684; Percent complete: 16.8%; Average loss: 0.4906\n",
            "Iteration: 1685; Percent complete: 16.9%; Average loss: 0.4951\n",
            "Iteration: 1686; Percent complete: 16.9%; Average loss: 0.4970\n",
            "Iteration: 1687; Percent complete: 16.9%; Average loss: 0.5123\n",
            "Iteration: 1688; Percent complete: 16.9%; Average loss: 0.5036\n",
            "Iteration: 1689; Percent complete: 16.9%; Average loss: 0.5052\n",
            "Iteration: 1690; Percent complete: 16.9%; Average loss: 0.4983\n",
            "Iteration: 1691; Percent complete: 16.9%; Average loss: 0.5093\n",
            "Iteration: 1692; Percent complete: 16.9%; Average loss: 0.4952\n",
            "Iteration: 1693; Percent complete: 16.9%; Average loss: 0.4939\n",
            "Iteration: 1694; Percent complete: 16.9%; Average loss: 0.4850\n",
            "Iteration: 1695; Percent complete: 17.0%; Average loss: 0.4964\n",
            "Iteration: 1696; Percent complete: 17.0%; Average loss: 0.5015\n",
            "Iteration: 1697; Percent complete: 17.0%; Average loss: 0.4907\n",
            "Iteration: 1698; Percent complete: 17.0%; Average loss: 0.4882\n",
            "Iteration: 1699; Percent complete: 17.0%; Average loss: 0.4943\n",
            "Iteration: 1700; Percent complete: 17.0%; Average loss: 0.5064\n",
            "Iteration: 1701; Percent complete: 17.0%; Average loss: 0.4924\n",
            "Iteration: 1702; Percent complete: 17.0%; Average loss: 0.4995\n",
            "Iteration: 1703; Percent complete: 17.0%; Average loss: 0.5117\n",
            "Iteration: 1704; Percent complete: 17.0%; Average loss: 0.5064\n",
            "Iteration: 1705; Percent complete: 17.1%; Average loss: 0.4872\n",
            "Iteration: 1706; Percent complete: 17.1%; Average loss: 0.4972\n",
            "Iteration: 1707; Percent complete: 17.1%; Average loss: 0.4941\n",
            "Iteration: 1708; Percent complete: 17.1%; Average loss: 0.4955\n",
            "Iteration: 1709; Percent complete: 17.1%; Average loss: 0.4912\n",
            "Iteration: 1710; Percent complete: 17.1%; Average loss: 0.5063\n",
            "Iteration: 1711; Percent complete: 17.1%; Average loss: 0.5045\n",
            "Iteration: 1712; Percent complete: 17.1%; Average loss: 0.5159\n",
            "Iteration: 1713; Percent complete: 17.1%; Average loss: 0.4968\n",
            "Iteration: 1714; Percent complete: 17.1%; Average loss: 0.4969\n",
            "Iteration: 1715; Percent complete: 17.2%; Average loss: 0.5039\n",
            "Iteration: 1716; Percent complete: 17.2%; Average loss: 0.4999\n",
            "Iteration: 1717; Percent complete: 17.2%; Average loss: 0.5185\n",
            "Iteration: 1718; Percent complete: 17.2%; Average loss: 0.4904\n",
            "Iteration: 1719; Percent complete: 17.2%; Average loss: 0.5040\n",
            "Iteration: 1720; Percent complete: 17.2%; Average loss: 0.5080\n",
            "Iteration: 1721; Percent complete: 17.2%; Average loss: 0.5005\n",
            "Iteration: 1722; Percent complete: 17.2%; Average loss: 0.5021\n",
            "Iteration: 1723; Percent complete: 17.2%; Average loss: 0.5017\n",
            "Iteration: 1724; Percent complete: 17.2%; Average loss: 0.5035\n",
            "Iteration: 1725; Percent complete: 17.2%; Average loss: 0.4997\n",
            "Iteration: 1726; Percent complete: 17.3%; Average loss: 0.5072\n",
            "Iteration: 1727; Percent complete: 17.3%; Average loss: 0.4998\n",
            "Iteration: 1728; Percent complete: 17.3%; Average loss: 0.4848\n",
            "Iteration: 1729; Percent complete: 17.3%; Average loss: 0.4881\n",
            "Iteration: 1730; Percent complete: 17.3%; Average loss: 0.4848\n",
            "Iteration: 1731; Percent complete: 17.3%; Average loss: 0.4931\n",
            "Iteration: 1732; Percent complete: 17.3%; Average loss: 0.4955\n",
            "Iteration: 1733; Percent complete: 17.3%; Average loss: 0.5049\n",
            "Iteration: 1734; Percent complete: 17.3%; Average loss: 0.4824\n",
            "Iteration: 1735; Percent complete: 17.3%; Average loss: 0.4917\n",
            "Iteration: 1736; Percent complete: 17.4%; Average loss: 0.5026\n",
            "Iteration: 1737; Percent complete: 17.4%; Average loss: 0.5019\n",
            "Iteration: 1738; Percent complete: 17.4%; Average loss: 0.5130\n",
            "Iteration: 1739; Percent complete: 17.4%; Average loss: 0.5057\n",
            "Iteration: 1740; Percent complete: 17.4%; Average loss: 0.5045\n",
            "Iteration: 1741; Percent complete: 17.4%; Average loss: 0.4878\n",
            "Iteration: 1742; Percent complete: 17.4%; Average loss: 0.4959\n",
            "Iteration: 1743; Percent complete: 17.4%; Average loss: 0.4979\n",
            "Iteration: 1744; Percent complete: 17.4%; Average loss: 0.4853\n",
            "Iteration: 1745; Percent complete: 17.4%; Average loss: 0.5001\n",
            "Iteration: 1746; Percent complete: 17.5%; Average loss: 0.5073\n",
            "Iteration: 1747; Percent complete: 17.5%; Average loss: 0.4983\n",
            "Iteration: 1748; Percent complete: 17.5%; Average loss: 0.4845\n",
            "Iteration: 1749; Percent complete: 17.5%; Average loss: 0.4944\n",
            "Iteration: 1750; Percent complete: 17.5%; Average loss: 0.5159\n",
            "Iteration: 1751; Percent complete: 17.5%; Average loss: 0.4985\n",
            "Iteration: 1752; Percent complete: 17.5%; Average loss: 0.5016\n",
            "Iteration: 1753; Percent complete: 17.5%; Average loss: 0.5031\n",
            "Iteration: 1754; Percent complete: 17.5%; Average loss: 0.5040\n",
            "Iteration: 1755; Percent complete: 17.5%; Average loss: 0.5137\n",
            "Iteration: 1756; Percent complete: 17.6%; Average loss: 0.4976\n",
            "Iteration: 1757; Percent complete: 17.6%; Average loss: 0.5173\n",
            "Iteration: 1758; Percent complete: 17.6%; Average loss: 0.4980\n",
            "Iteration: 1759; Percent complete: 17.6%; Average loss: 0.5112\n",
            "Iteration: 1760; Percent complete: 17.6%; Average loss: 0.4928\n",
            "Iteration: 1761; Percent complete: 17.6%; Average loss: 0.4899\n",
            "Iteration: 1762; Percent complete: 17.6%; Average loss: 0.5067\n",
            "Iteration: 1763; Percent complete: 17.6%; Average loss: 0.5129\n",
            "Iteration: 1764; Percent complete: 17.6%; Average loss: 0.5006\n",
            "Iteration: 1765; Percent complete: 17.6%; Average loss: 0.4950\n",
            "Iteration: 1766; Percent complete: 17.7%; Average loss: 0.4985\n",
            "Iteration: 1767; Percent complete: 17.7%; Average loss: 0.5045\n",
            "Iteration: 1768; Percent complete: 17.7%; Average loss: 0.4888\n",
            "Iteration: 1769; Percent complete: 17.7%; Average loss: 0.5007\n",
            "Iteration: 1770; Percent complete: 17.7%; Average loss: 0.4859\n",
            "Iteration: 1771; Percent complete: 17.7%; Average loss: 0.5112\n",
            "Iteration: 1772; Percent complete: 17.7%; Average loss: 0.5169\n",
            "Iteration: 1773; Percent complete: 17.7%; Average loss: 0.4918\n",
            "Iteration: 1774; Percent complete: 17.7%; Average loss: 0.4915\n",
            "Iteration: 1775; Percent complete: 17.8%; Average loss: 0.4940\n",
            "Iteration: 1776; Percent complete: 17.8%; Average loss: 0.5001\n",
            "Iteration: 1777; Percent complete: 17.8%; Average loss: 0.4925\n",
            "Iteration: 1778; Percent complete: 17.8%; Average loss: 0.5212\n",
            "Iteration: 1779; Percent complete: 17.8%; Average loss: 0.4956\n",
            "Iteration: 1780; Percent complete: 17.8%; Average loss: 0.4981\n",
            "Iteration: 1781; Percent complete: 17.8%; Average loss: 0.4940\n",
            "Iteration: 1782; Percent complete: 17.8%; Average loss: 0.5012\n",
            "Iteration: 1783; Percent complete: 17.8%; Average loss: 0.4933\n",
            "Iteration: 1784; Percent complete: 17.8%; Average loss: 0.4920\n",
            "Iteration: 1785; Percent complete: 17.8%; Average loss: 0.4873\n",
            "Iteration: 1786; Percent complete: 17.9%; Average loss: 0.4849\n",
            "Iteration: 1787; Percent complete: 17.9%; Average loss: 0.5012\n",
            "Iteration: 1788; Percent complete: 17.9%; Average loss: 0.5055\n",
            "Iteration: 1789; Percent complete: 17.9%; Average loss: 0.5029\n",
            "Iteration: 1790; Percent complete: 17.9%; Average loss: 0.5034\n",
            "Iteration: 1791; Percent complete: 17.9%; Average loss: 0.4970\n",
            "Iteration: 1792; Percent complete: 17.9%; Average loss: 0.4911\n",
            "Iteration: 1793; Percent complete: 17.9%; Average loss: 0.5118\n",
            "Iteration: 1794; Percent complete: 17.9%; Average loss: 0.4947\n",
            "Iteration: 1795; Percent complete: 17.9%; Average loss: 0.4970\n",
            "Iteration: 1796; Percent complete: 18.0%; Average loss: 0.4901\n",
            "Iteration: 1797; Percent complete: 18.0%; Average loss: 0.4980\n",
            "Iteration: 1798; Percent complete: 18.0%; Average loss: 0.4981\n",
            "Iteration: 1799; Percent complete: 18.0%; Average loss: 0.4888\n",
            "Iteration: 1800; Percent complete: 18.0%; Average loss: 0.4958\n",
            "Iteration: 1801; Percent complete: 18.0%; Average loss: 0.4974\n",
            "Iteration: 1802; Percent complete: 18.0%; Average loss: 0.4969\n",
            "Iteration: 1803; Percent complete: 18.0%; Average loss: 0.4885\n",
            "Iteration: 1804; Percent complete: 18.0%; Average loss: 0.4978\n",
            "Iteration: 1805; Percent complete: 18.1%; Average loss: 0.5036\n",
            "Iteration: 1806; Percent complete: 18.1%; Average loss: 0.4882\n",
            "Iteration: 1807; Percent complete: 18.1%; Average loss: 0.4954\n",
            "Iteration: 1808; Percent complete: 18.1%; Average loss: 0.5058\n",
            "Iteration: 1809; Percent complete: 18.1%; Average loss: 0.4995\n",
            "Iteration: 1810; Percent complete: 18.1%; Average loss: 0.4891\n",
            "Iteration: 1811; Percent complete: 18.1%; Average loss: 0.5054\n",
            "Iteration: 1812; Percent complete: 18.1%; Average loss: 0.4808\n",
            "Iteration: 1813; Percent complete: 18.1%; Average loss: 0.5036\n",
            "Iteration: 1814; Percent complete: 18.1%; Average loss: 0.4997\n",
            "Iteration: 1815; Percent complete: 18.1%; Average loss: 0.4856\n",
            "Iteration: 1816; Percent complete: 18.2%; Average loss: 0.4976\n",
            "Iteration: 1817; Percent complete: 18.2%; Average loss: 0.4847\n",
            "Iteration: 1818; Percent complete: 18.2%; Average loss: 0.4990\n",
            "Iteration: 1819; Percent complete: 18.2%; Average loss: 0.5020\n",
            "Iteration: 1820; Percent complete: 18.2%; Average loss: 0.4984\n",
            "Iteration: 1821; Percent complete: 18.2%; Average loss: 0.4969\n",
            "Iteration: 1822; Percent complete: 18.2%; Average loss: 0.5100\n",
            "Iteration: 1823; Percent complete: 18.2%; Average loss: 0.4920\n",
            "Iteration: 1824; Percent complete: 18.2%; Average loss: 0.4920\n",
            "Iteration: 1825; Percent complete: 18.2%; Average loss: 0.4927\n",
            "Iteration: 1826; Percent complete: 18.3%; Average loss: 0.5015\n",
            "Iteration: 1827; Percent complete: 18.3%; Average loss: 0.4953\n",
            "Iteration: 1828; Percent complete: 18.3%; Average loss: 0.4906\n",
            "Iteration: 1829; Percent complete: 18.3%; Average loss: 0.5032\n",
            "Iteration: 1830; Percent complete: 18.3%; Average loss: 0.4908\n",
            "Iteration: 1831; Percent complete: 18.3%; Average loss: 0.4953\n",
            "Iteration: 1832; Percent complete: 18.3%; Average loss: 0.4934\n",
            "Iteration: 1833; Percent complete: 18.3%; Average loss: 0.5010\n",
            "Iteration: 1834; Percent complete: 18.3%; Average loss: 0.4862\n",
            "Iteration: 1835; Percent complete: 18.4%; Average loss: 0.5023\n",
            "Iteration: 1836; Percent complete: 18.4%; Average loss: 0.4884\n",
            "Iteration: 1837; Percent complete: 18.4%; Average loss: 0.4853\n",
            "Iteration: 1838; Percent complete: 18.4%; Average loss: 0.4889\n",
            "Iteration: 1839; Percent complete: 18.4%; Average loss: 0.5037\n",
            "Iteration: 1840; Percent complete: 18.4%; Average loss: 0.5019\n",
            "Iteration: 1841; Percent complete: 18.4%; Average loss: 0.5009\n",
            "Iteration: 1842; Percent complete: 18.4%; Average loss: 0.5025\n",
            "Iteration: 1843; Percent complete: 18.4%; Average loss: 0.5042\n",
            "Iteration: 1844; Percent complete: 18.4%; Average loss: 0.5010\n",
            "Iteration: 1845; Percent complete: 18.4%; Average loss: 0.5019\n",
            "Iteration: 1846; Percent complete: 18.5%; Average loss: 0.5014\n",
            "Iteration: 1847; Percent complete: 18.5%; Average loss: 0.4974\n",
            "Iteration: 1848; Percent complete: 18.5%; Average loss: 0.4940\n",
            "Iteration: 1849; Percent complete: 18.5%; Average loss: 0.5060\n",
            "Iteration: 1850; Percent complete: 18.5%; Average loss: 0.5031\n",
            "Iteration: 1851; Percent complete: 18.5%; Average loss: 0.4844\n",
            "Iteration: 1852; Percent complete: 18.5%; Average loss: 0.4951\n",
            "Iteration: 1853; Percent complete: 18.5%; Average loss: 0.4946\n",
            "Iteration: 1854; Percent complete: 18.5%; Average loss: 0.5100\n",
            "Iteration: 1855; Percent complete: 18.6%; Average loss: 0.4911\n",
            "Iteration: 1856; Percent complete: 18.6%; Average loss: 0.5146\n",
            "Iteration: 1857; Percent complete: 18.6%; Average loss: 0.4901\n",
            "Iteration: 1858; Percent complete: 18.6%; Average loss: 0.4912\n",
            "Iteration: 1859; Percent complete: 18.6%; Average loss: 0.5086\n",
            "Iteration: 1860; Percent complete: 18.6%; Average loss: 0.4967\n",
            "Iteration: 1861; Percent complete: 18.6%; Average loss: 0.4936\n",
            "Iteration: 1862; Percent complete: 18.6%; Average loss: 0.4936\n",
            "Iteration: 1863; Percent complete: 18.6%; Average loss: 0.4975\n",
            "Iteration: 1864; Percent complete: 18.6%; Average loss: 0.4892\n",
            "Iteration: 1865; Percent complete: 18.6%; Average loss: 0.4894\n",
            "Iteration: 1866; Percent complete: 18.7%; Average loss: 0.5122\n",
            "Iteration: 1867; Percent complete: 18.7%; Average loss: 0.4943\n",
            "Iteration: 1868; Percent complete: 18.7%; Average loss: 0.4907\n",
            "Iteration: 1869; Percent complete: 18.7%; Average loss: 0.4948\n",
            "Iteration: 1870; Percent complete: 18.7%; Average loss: 0.4925\n",
            "Iteration: 1871; Percent complete: 18.7%; Average loss: 0.4942\n",
            "Iteration: 1872; Percent complete: 18.7%; Average loss: 0.4862\n",
            "Iteration: 1873; Percent complete: 18.7%; Average loss: 0.5030\n",
            "Iteration: 1874; Percent complete: 18.7%; Average loss: 0.5090\n",
            "Iteration: 1875; Percent complete: 18.8%; Average loss: 0.4793\n",
            "Iteration: 1876; Percent complete: 18.8%; Average loss: 0.4909\n",
            "Iteration: 1877; Percent complete: 18.8%; Average loss: 0.4913\n",
            "Iteration: 1878; Percent complete: 18.8%; Average loss: 0.4873\n",
            "Iteration: 1879; Percent complete: 18.8%; Average loss: 0.4937\n",
            "Iteration: 1880; Percent complete: 18.8%; Average loss: 0.4987\n",
            "Iteration: 1881; Percent complete: 18.8%; Average loss: 0.4978\n",
            "Iteration: 1882; Percent complete: 18.8%; Average loss: 0.4934\n",
            "Iteration: 1883; Percent complete: 18.8%; Average loss: 0.5018\n",
            "Iteration: 1884; Percent complete: 18.8%; Average loss: 0.4877\n",
            "Iteration: 1885; Percent complete: 18.9%; Average loss: 0.4935\n",
            "Iteration: 1886; Percent complete: 18.9%; Average loss: 0.4968\n",
            "Iteration: 1887; Percent complete: 18.9%; Average loss: 0.5029\n",
            "Iteration: 1888; Percent complete: 18.9%; Average loss: 0.4910\n",
            "Iteration: 1889; Percent complete: 18.9%; Average loss: 0.4905\n",
            "Iteration: 1890; Percent complete: 18.9%; Average loss: 0.4824\n",
            "Iteration: 1891; Percent complete: 18.9%; Average loss: 0.4989\n",
            "Iteration: 1892; Percent complete: 18.9%; Average loss: 0.4982\n",
            "Iteration: 1893; Percent complete: 18.9%; Average loss: 0.4923\n",
            "Iteration: 1894; Percent complete: 18.9%; Average loss: 0.4857\n",
            "Iteration: 1895; Percent complete: 18.9%; Average loss: 0.5054\n",
            "Iteration: 1896; Percent complete: 19.0%; Average loss: 0.4964\n",
            "Iteration: 1897; Percent complete: 19.0%; Average loss: 0.4939\n",
            "Iteration: 1898; Percent complete: 19.0%; Average loss: 0.4964\n",
            "Iteration: 1899; Percent complete: 19.0%; Average loss: 0.4866\n",
            "Iteration: 1900; Percent complete: 19.0%; Average loss: 0.5192\n",
            "Iteration: 1901; Percent complete: 19.0%; Average loss: 0.4967\n",
            "Iteration: 1902; Percent complete: 19.0%; Average loss: 0.4890\n",
            "Iteration: 1903; Percent complete: 19.0%; Average loss: 0.5016\n",
            "Iteration: 1904; Percent complete: 19.0%; Average loss: 0.5017\n",
            "Iteration: 1905; Percent complete: 19.1%; Average loss: 0.4886\n",
            "Iteration: 1906; Percent complete: 19.1%; Average loss: 0.4904\n",
            "Iteration: 1907; Percent complete: 19.1%; Average loss: 0.4901\n",
            "Iteration: 1908; Percent complete: 19.1%; Average loss: 0.5022\n",
            "Iteration: 1909; Percent complete: 19.1%; Average loss: 0.4928\n",
            "Iteration: 1910; Percent complete: 19.1%; Average loss: 0.4970\n",
            "Iteration: 1911; Percent complete: 19.1%; Average loss: 0.4932\n",
            "Iteration: 1912; Percent complete: 19.1%; Average loss: 0.4946\n",
            "Iteration: 1913; Percent complete: 19.1%; Average loss: 0.4894\n",
            "Iteration: 1914; Percent complete: 19.1%; Average loss: 0.4945\n",
            "Iteration: 1915; Percent complete: 19.1%; Average loss: 0.4914\n",
            "Iteration: 1916; Percent complete: 19.2%; Average loss: 0.4879\n",
            "Iteration: 1917; Percent complete: 19.2%; Average loss: 0.4950\n",
            "Iteration: 1918; Percent complete: 19.2%; Average loss: 0.4993\n",
            "Iteration: 1919; Percent complete: 19.2%; Average loss: 0.5013\n",
            "Iteration: 1920; Percent complete: 19.2%; Average loss: 0.4921\n",
            "Iteration: 1921; Percent complete: 19.2%; Average loss: 0.5002\n",
            "Iteration: 1922; Percent complete: 19.2%; Average loss: 0.4794\n",
            "Iteration: 1923; Percent complete: 19.2%; Average loss: 0.4959\n",
            "Iteration: 1924; Percent complete: 19.2%; Average loss: 0.4989\n",
            "Iteration: 1925; Percent complete: 19.2%; Average loss: 0.4979\n",
            "Iteration: 1926; Percent complete: 19.3%; Average loss: 0.4786\n",
            "Iteration: 1927; Percent complete: 19.3%; Average loss: 0.5021\n",
            "Iteration: 1928; Percent complete: 19.3%; Average loss: 0.4935\n",
            "Iteration: 1929; Percent complete: 19.3%; Average loss: 0.4945\n",
            "Iteration: 1930; Percent complete: 19.3%; Average loss: 0.4916\n",
            "Iteration: 1931; Percent complete: 19.3%; Average loss: 0.4835\n",
            "Iteration: 1932; Percent complete: 19.3%; Average loss: 0.4954\n",
            "Iteration: 1933; Percent complete: 19.3%; Average loss: 0.4866\n",
            "Iteration: 1934; Percent complete: 19.3%; Average loss: 0.5019\n",
            "Iteration: 1935; Percent complete: 19.4%; Average loss: 0.4978\n",
            "Iteration: 1936; Percent complete: 19.4%; Average loss: 0.4926\n",
            "Iteration: 1937; Percent complete: 19.4%; Average loss: 0.4758\n",
            "Iteration: 1938; Percent complete: 19.4%; Average loss: 0.4987\n",
            "Iteration: 1939; Percent complete: 19.4%; Average loss: 0.4922\n",
            "Iteration: 1940; Percent complete: 19.4%; Average loss: 0.4887\n",
            "Iteration: 1941; Percent complete: 19.4%; Average loss: 0.4907\n",
            "Iteration: 1942; Percent complete: 19.4%; Average loss: 0.5019\n",
            "Iteration: 1943; Percent complete: 19.4%; Average loss: 0.4782\n",
            "Iteration: 1944; Percent complete: 19.4%; Average loss: 0.4894\n",
            "Iteration: 1945; Percent complete: 19.4%; Average loss: 0.4843\n",
            "Iteration: 1946; Percent complete: 19.5%; Average loss: 0.4850\n",
            "Iteration: 1947; Percent complete: 19.5%; Average loss: 0.4899\n",
            "Iteration: 1948; Percent complete: 19.5%; Average loss: 0.4908\n",
            "Iteration: 1949; Percent complete: 19.5%; Average loss: 0.4943\n",
            "Iteration: 1950; Percent complete: 19.5%; Average loss: 0.4824\n",
            "Iteration: 1951; Percent complete: 19.5%; Average loss: 0.5037\n",
            "Iteration: 1952; Percent complete: 19.5%; Average loss: 0.4877\n",
            "Iteration: 1953; Percent complete: 19.5%; Average loss: 0.4980\n",
            "Iteration: 1954; Percent complete: 19.5%; Average loss: 0.5042\n",
            "Iteration: 1955; Percent complete: 19.6%; Average loss: 0.4994\n",
            "Iteration: 1956; Percent complete: 19.6%; Average loss: 0.4938\n",
            "Iteration: 1957; Percent complete: 19.6%; Average loss: 0.4989\n",
            "Iteration: 1958; Percent complete: 19.6%; Average loss: 0.4862\n",
            "Iteration: 1959; Percent complete: 19.6%; Average loss: 0.4935\n",
            "Iteration: 1960; Percent complete: 19.6%; Average loss: 0.5066\n",
            "Iteration: 1961; Percent complete: 19.6%; Average loss: 0.5084\n",
            "Iteration: 1962; Percent complete: 19.6%; Average loss: 0.4937\n",
            "Iteration: 1963; Percent complete: 19.6%; Average loss: 0.4907\n",
            "Iteration: 1964; Percent complete: 19.6%; Average loss: 0.5117\n",
            "Iteration: 1965; Percent complete: 19.7%; Average loss: 0.4886\n",
            "Iteration: 1966; Percent complete: 19.7%; Average loss: 0.5029\n",
            "Iteration: 1967; Percent complete: 19.7%; Average loss: 0.4869\n",
            "Iteration: 1968; Percent complete: 19.7%; Average loss: 0.4865\n",
            "Iteration: 1969; Percent complete: 19.7%; Average loss: 0.4955\n",
            "Iteration: 1970; Percent complete: 19.7%; Average loss: 0.5042\n",
            "Iteration: 1971; Percent complete: 19.7%; Average loss: 0.5042\n",
            "Iteration: 1972; Percent complete: 19.7%; Average loss: 0.4859\n",
            "Iteration: 1973; Percent complete: 19.7%; Average loss: 0.4907\n",
            "Iteration: 1974; Percent complete: 19.7%; Average loss: 0.4869\n",
            "Iteration: 1975; Percent complete: 19.8%; Average loss: 0.5113\n",
            "Iteration: 1976; Percent complete: 19.8%; Average loss: 0.5132\n",
            "Iteration: 1977; Percent complete: 19.8%; Average loss: 0.4913\n",
            "Iteration: 1978; Percent complete: 19.8%; Average loss: 0.5529\n",
            "Iteration: 1979; Percent complete: 19.8%; Average loss: 0.4987\n",
            "Iteration: 1980; Percent complete: 19.8%; Average loss: 0.4982\n",
            "Iteration: 1981; Percent complete: 19.8%; Average loss: 0.4887\n",
            "Iteration: 1982; Percent complete: 19.8%; Average loss: 0.5037\n",
            "Iteration: 1983; Percent complete: 19.8%; Average loss: 0.4940\n",
            "Iteration: 1984; Percent complete: 19.8%; Average loss: 0.5089\n",
            "Iteration: 1985; Percent complete: 19.9%; Average loss: 0.4887\n",
            "Iteration: 1986; Percent complete: 19.9%; Average loss: 0.5091\n",
            "Iteration: 1987; Percent complete: 19.9%; Average loss: 0.5153\n",
            "Iteration: 1988; Percent complete: 19.9%; Average loss: 0.4871\n",
            "Iteration: 1989; Percent complete: 19.9%; Average loss: 0.5057\n",
            "Iteration: 1990; Percent complete: 19.9%; Average loss: 0.5161\n",
            "Iteration: 1991; Percent complete: 19.9%; Average loss: 0.4898\n",
            "Iteration: 1992; Percent complete: 19.9%; Average loss: 0.4958\n",
            "Iteration: 1993; Percent complete: 19.9%; Average loss: 0.4895\n",
            "Iteration: 1994; Percent complete: 19.9%; Average loss: 0.4915\n",
            "Iteration: 1995; Percent complete: 20.0%; Average loss: 0.5024\n",
            "Iteration: 1996; Percent complete: 20.0%; Average loss: 0.4930\n",
            "Iteration: 1997; Percent complete: 20.0%; Average loss: 0.4945\n",
            "Iteration: 1998; Percent complete: 20.0%; Average loss: 0.4939\n",
            "Iteration: 1999; Percent complete: 20.0%; Average loss: 0.4977\n",
            "Iteration: 2000; Percent complete: 20.0%; Average loss: 0.4879\n",
            "Iteration: 2001; Percent complete: 20.0%; Average loss: 0.4932\n",
            "Iteration: 2002; Percent complete: 20.0%; Average loss: 0.4869\n",
            "Iteration: 2003; Percent complete: 20.0%; Average loss: 0.4924\n",
            "Iteration: 2004; Percent complete: 20.0%; Average loss: 0.5001\n",
            "Iteration: 2005; Percent complete: 20.1%; Average loss: 0.4968\n",
            "Iteration: 2006; Percent complete: 20.1%; Average loss: 0.4936\n",
            "Iteration: 2007; Percent complete: 20.1%; Average loss: 0.4985\n",
            "Iteration: 2008; Percent complete: 20.1%; Average loss: 0.4986\n",
            "Iteration: 2009; Percent complete: 20.1%; Average loss: 0.5016\n",
            "Iteration: 2010; Percent complete: 20.1%; Average loss: 0.4934\n",
            "Iteration: 2011; Percent complete: 20.1%; Average loss: 0.4869\n",
            "Iteration: 2012; Percent complete: 20.1%; Average loss: 0.4992\n",
            "Iteration: 2013; Percent complete: 20.1%; Average loss: 0.4927\n",
            "Iteration: 2014; Percent complete: 20.1%; Average loss: 0.5009\n",
            "Iteration: 2015; Percent complete: 20.2%; Average loss: 0.4856\n",
            "Iteration: 2016; Percent complete: 20.2%; Average loss: 0.4821\n",
            "Iteration: 2017; Percent complete: 20.2%; Average loss: 0.4959\n",
            "Iteration: 2018; Percent complete: 20.2%; Average loss: 0.4995\n",
            "Iteration: 2019; Percent complete: 20.2%; Average loss: 0.5006\n",
            "Iteration: 2020; Percent complete: 20.2%; Average loss: 0.5006\n",
            "Iteration: 2021; Percent complete: 20.2%; Average loss: 0.4919\n",
            "Iteration: 2022; Percent complete: 20.2%; Average loss: 0.4922\n",
            "Iteration: 2023; Percent complete: 20.2%; Average loss: 0.4883\n",
            "Iteration: 2024; Percent complete: 20.2%; Average loss: 0.4787\n",
            "Iteration: 2025; Percent complete: 20.2%; Average loss: 0.4961\n",
            "Iteration: 2026; Percent complete: 20.3%; Average loss: 0.4955\n",
            "Iteration: 2027; Percent complete: 20.3%; Average loss: 0.4898\n",
            "Iteration: 2028; Percent complete: 20.3%; Average loss: 0.4890\n",
            "Iteration: 2029; Percent complete: 20.3%; Average loss: 0.4954\n",
            "Iteration: 2030; Percent complete: 20.3%; Average loss: 0.5070\n",
            "Iteration: 2031; Percent complete: 20.3%; Average loss: 0.4899\n",
            "Iteration: 2032; Percent complete: 20.3%; Average loss: 0.4977\n",
            "Iteration: 2033; Percent complete: 20.3%; Average loss: 0.4977\n",
            "Iteration: 2034; Percent complete: 20.3%; Average loss: 0.4813\n",
            "Iteration: 2035; Percent complete: 20.3%; Average loss: 0.4809\n",
            "Iteration: 2036; Percent complete: 20.4%; Average loss: 0.4844\n",
            "Iteration: 2037; Percent complete: 20.4%; Average loss: 0.5251\n",
            "Iteration: 2038; Percent complete: 20.4%; Average loss: 0.4863\n",
            "Iteration: 2039; Percent complete: 20.4%; Average loss: 0.4832\n",
            "Iteration: 2040; Percent complete: 20.4%; Average loss: 0.5016\n",
            "Iteration: 2041; Percent complete: 20.4%; Average loss: 0.4844\n",
            "Iteration: 2042; Percent complete: 20.4%; Average loss: 0.4974\n",
            "Iteration: 2043; Percent complete: 20.4%; Average loss: 0.4988\n",
            "Iteration: 2044; Percent complete: 20.4%; Average loss: 0.4925\n",
            "Iteration: 2045; Percent complete: 20.4%; Average loss: 0.4909\n",
            "Iteration: 2046; Percent complete: 20.5%; Average loss: 0.4895\n",
            "Iteration: 2047; Percent complete: 20.5%; Average loss: 0.4927\n",
            "Iteration: 2048; Percent complete: 20.5%; Average loss: 0.4912\n",
            "Iteration: 2049; Percent complete: 20.5%; Average loss: 0.4816\n",
            "Iteration: 2050; Percent complete: 20.5%; Average loss: 0.4990\n",
            "Iteration: 2051; Percent complete: 20.5%; Average loss: 0.4900\n",
            "Iteration: 2052; Percent complete: 20.5%; Average loss: 0.4807\n",
            "Iteration: 2053; Percent complete: 20.5%; Average loss: 0.4838\n",
            "Iteration: 2054; Percent complete: 20.5%; Average loss: 0.4985\n",
            "Iteration: 2055; Percent complete: 20.5%; Average loss: 0.4819\n",
            "Iteration: 2056; Percent complete: 20.6%; Average loss: 0.5060\n",
            "Iteration: 2057; Percent complete: 20.6%; Average loss: 0.4946\n",
            "Iteration: 2058; Percent complete: 20.6%; Average loss: 0.4953\n",
            "Iteration: 2059; Percent complete: 20.6%; Average loss: 0.4889\n",
            "Iteration: 2060; Percent complete: 20.6%; Average loss: 0.4822\n",
            "Iteration: 2061; Percent complete: 20.6%; Average loss: 0.4958\n",
            "Iteration: 2062; Percent complete: 20.6%; Average loss: 0.4938\n",
            "Iteration: 2063; Percent complete: 20.6%; Average loss: 0.4941\n",
            "Iteration: 2064; Percent complete: 20.6%; Average loss: 0.4829\n",
            "Iteration: 2065; Percent complete: 20.6%; Average loss: 0.5020\n",
            "Iteration: 2066; Percent complete: 20.7%; Average loss: 0.4947\n",
            "Iteration: 2067; Percent complete: 20.7%; Average loss: 0.5113\n",
            "Iteration: 2068; Percent complete: 20.7%; Average loss: 0.4873\n",
            "Iteration: 2069; Percent complete: 20.7%; Average loss: 0.4948\n",
            "Iteration: 2070; Percent complete: 20.7%; Average loss: 0.4808\n",
            "Iteration: 2071; Percent complete: 20.7%; Average loss: 0.4927\n",
            "Iteration: 2072; Percent complete: 20.7%; Average loss: 0.4892\n",
            "Iteration: 2073; Percent complete: 20.7%; Average loss: 0.4887\n",
            "Iteration: 2074; Percent complete: 20.7%; Average loss: 0.4965\n",
            "Iteration: 2075; Percent complete: 20.8%; Average loss: 0.4945\n",
            "Iteration: 2076; Percent complete: 20.8%; Average loss: 0.4840\n",
            "Iteration: 2077; Percent complete: 20.8%; Average loss: 0.4975\n",
            "Iteration: 2078; Percent complete: 20.8%; Average loss: 0.4949\n",
            "Iteration: 2079; Percent complete: 20.8%; Average loss: 0.4893\n",
            "Iteration: 2080; Percent complete: 20.8%; Average loss: 0.5080\n",
            "Iteration: 2081; Percent complete: 20.8%; Average loss: 0.4974\n",
            "Iteration: 2082; Percent complete: 20.8%; Average loss: 0.4952\n",
            "Iteration: 2083; Percent complete: 20.8%; Average loss: 0.4953\n",
            "Iteration: 2084; Percent complete: 20.8%; Average loss: 0.4913\n",
            "Iteration: 2085; Percent complete: 20.8%; Average loss: 0.4851\n",
            "Iteration: 2086; Percent complete: 20.9%; Average loss: 0.4955\n",
            "Iteration: 2087; Percent complete: 20.9%; Average loss: 0.4909\n",
            "Iteration: 2088; Percent complete: 20.9%; Average loss: 0.4960\n",
            "Iteration: 2089; Percent complete: 20.9%; Average loss: 0.4926\n",
            "Iteration: 2090; Percent complete: 20.9%; Average loss: 0.4922\n",
            "Iteration: 2091; Percent complete: 20.9%; Average loss: 0.4901\n",
            "Iteration: 2092; Percent complete: 20.9%; Average loss: 0.4954\n",
            "Iteration: 2093; Percent complete: 20.9%; Average loss: 0.4971\n",
            "Iteration: 2094; Percent complete: 20.9%; Average loss: 0.5016\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-84b58c1e4459>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Run training iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting Training!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m trainIters(model_name, encoder, decoder, encoder_optimizer, decoder_optimizer,\n\u001b[0m\u001b[1;32m     33\u001b[0m            \u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_n_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_n_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m            print_every, save_every, clip, corpus_name, loadFilename, batchTrain, voc_dict)\n",
            "\u001b[0;32m<ipython-input-34-b9177660bb5d>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(model_name, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename, batchTrain, voc_dict)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Run a training iteration with batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n\u001b[0m\u001b[1;32m     22\u001b[0m                      decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n\u001b[1;32m     23\u001b[0m         \u001b[0mprint_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-828f242e8146>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m# Perform backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# Clip gradients: gradients are modified in place\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GreedySearchDecoder(nn.Module):\n",
        "  def __init__(self, encoder, decoder):\n",
        "      super(GreedySearchDecoder, self).__init__()\n",
        "      self.encoder = encoder\n",
        "      self.decoder = decoder\n",
        "\n",
        "  def forward(self, input_seq, input_length, max_length):\n",
        "      # Forward input through encoder model\n",
        "      encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
        "      # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
        "      decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "      # Initialize decoder input with SOS_token\n",
        "      decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
        "      # Initialize tensors to append decoded words to\n",
        "      all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
        "      all_scores = torch.zeros([0], device=device)\n",
        "      # Iteratively decode one word token at a time\n",
        "      for _ in range(max_length):\n",
        "          # Forward pass through decoder\n",
        "          decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "          # Obtain most likely word token and its softmax score\n",
        "          decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
        "          # Record token and score\n",
        "          all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
        "          all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
        "          # Prepare current token to be next decoder input (add a dimension)\n",
        "          decoder_input = torch.unsqueeze(decoder_input, 0)\n",
        "      # Return collections of word tokens and scores\n",
        "      return all_tokens, all_scores"
      ],
      "metadata": {
        "id": "AL1zZ5Lgmk7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getInput(userInput, voc_dict):\n",
        "\n",
        "  lenchar = len(next(iter(voc_dict.values())))\n",
        "  trainData = []\n",
        "  trainData.append([voc_dict.get(token, [0.5 for _ in range(lenchar)]) for token in userInput])\n",
        "  trainData = torch.cat(trainData[0], dim=0)\n",
        "  return Batchdata(\"pass\", [trainData], method=\"onehot\").Getdata()\n",
        "\n",
        "def converse(idxint, voc_dict):\n",
        "  word = next((key for key, value in voc_dict.items() if torch.equal(value, idxint)), \" \")\n",
        "  return word\n",
        "\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
        "    return s"
      ],
      "metadata": {
        "id": "cTFh0egCml1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set dropout layers to ``eval`` mode\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "# Initialize search module\n",
        "searcher = GreedySearchDecoder(encoder, decoder)"
      ],
      "metadata": {
        "id": "boRH0hybnIck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EncoderRNN：這是一個用於處理輸入句子的編碼器（Encoder）。它使用嵌入層（embedding layer）將輸入單詞轉換為嵌入向量，並通過多層的雙向 GRU 循環神經網絡進行編碼。\n",
        "\n",
        "LuongAttnDecoderRNN：這是一個帶有 Luong 注意力機制的解碼器（Decoder）。它使用嵌入層（embedding layer）將輸入單詞轉換為嵌入向量，並通過多層的 GRU 循環神經網絡進行解碼。在解碼的過程中，它使用注意力機制來對編碼器的輸出進行加權，以獲取更好的上下文信息。"
      ],
      "metadata": {
        "id": "ddTllXNLpJet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "searcher.encoder, searcher.decoder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBNRzCdInPcR",
        "outputId": "2be5b0d4-fca3-4e91-ac2a-932cb07820a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(EncoderRNN(\n",
              "   (embedding): Embedding(6932, 50)\n",
              "   (gru): GRU(50, 50, num_layers=2, dropout=0.1, bidirectional=True)\n",
              " ),\n",
              " LuongAttnDecoderRNN(\n",
              "   (embedding): Embedding(6932, 50)\n",
              "   (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
              "   (gru): GRU(50, 50, num_layers=2, dropout=0.1)\n",
              "   (concat): Linear(in_features=100, out_features=50, bias=True)\n",
              "   (out): Linear(in_features=50, out_features=6932, bias=True)\n",
              "   (attn): Attn()\n",
              " ))"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = AiBot(input(\"> \"), encoder, decoder, voc_dict)\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zp4Dwwwnyh4g",
        "outputId": "653a38ef-b7ae-46a0-ee2a-c256dd7e89ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> 公三小\n",
            "evaluate\n",
            "不石石石石石石石石石石石石石\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 底下為導入APP中的程式碼"
      ],
      "metadata": {
        "id": "yRg8vo5LwcD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def AiBot(strinput, encoder, decoder, voc_dict):\n",
        "\n",
        "  # Set dropout layers to ``eval`` mode\n",
        "  encoder.eval()\n",
        "  decoder.eval()\n",
        "\n",
        "  # Initialize search module\n",
        "  searcher = GreedySearchDecoder(encoder, decoder)\n",
        "\n",
        "  input_var, lengths = getInput(strinput, voc_dict)[0]\n",
        "\n",
        "  wordnum = len(next(iter(voc_dict.values())))\n",
        "  MAXLENGTH = 20 * wordnum\n",
        "  result = searcher.forward(input_var, lengths, MAXLENGTH)\n",
        "  word_res = [result[0][i:i+wordnum] for i in range(0, MAXLENGTH, wordnum)]\n",
        "  res = \"\"\n",
        "  for i in word_res:\n",
        "    res += converse(i, voc_dict)\n",
        "\n",
        "  return res"
      ],
      "metadata": {
        "id": "9Bt0uDsOy8II"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure models\n",
        "corpus_name = \"Gossiping-QA-Dataset\"\n",
        "model_name = 'chatbot'\n",
        "attn_model = 'dot'\n",
        "hidden_size = 50\n",
        "encoder_n_layers = 2\n",
        "decoder_n_layers = 2\n",
        "dropout = 0.1\n",
        "batch_size = 16\n",
        "\n",
        "# Set checkpoint to load from; set to None if starting from scratch\n",
        "loadFilename = None\n",
        "checkpoint_iter = 2000\n",
        "\n",
        "save_dir = \"/content/drive/MyDrive\""
      ],
      "metadata": {
        "id": "KVBxFD4qv7uL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
        "                    '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n",
        "                    '{}_checkpoint.tar'.format(checkpoint_iter))\n",
        "\n",
        "if loadFilename:\n",
        "  # If loading on same machine the model was trained on\n",
        "  checkpoint = torch.load(loadFilename)\n",
        "  # If loading a model trained on GPU to CPU\n",
        "  #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
        "  encoder_sd = checkpoint['en']\n",
        "  decoder_sd = checkpoint['de']\n",
        "  encoder_optimizer_sd = checkpoint['en_opt']\n",
        "  decoder_optimizer_sd = checkpoint['de_opt']\n",
        "  embedding_sd = checkpoint['embedding']\n",
        "  voc_dict = checkpoint['voc_dict']\n",
        "\n",
        "\n",
        "print('Building encoder and decoder ...')\n",
        "# Initialize word embeddings\n",
        "embedding = nn.Embedding(len(voc_dict), hidden_size)\n",
        "if loadFilename:\n",
        "    embedding.load_state_dict(embedding_sd)\n",
        "\n",
        "# Initialize encoder & decoder models\n",
        "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
        "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, len(voc_dict), decoder_n_layers, dropout)\n",
        "if loadFilename:\n",
        "    encoder.load_state_dict(encoder_sd)\n",
        "    decoder.load_state_dict(decoder_sd)\n",
        "# Use appropriate device\n",
        "encoder = encoder.to(device)\n",
        "decoder = decoder.to(device)\n",
        "print('Models built and ready to go!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pcy80gaFwKTa",
        "outputId": "668c9ecf-9270-43e3-d007-460498c0f107"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building encoder and decoder ...\n",
            "Models built and ready to go!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = AiBot(input(\"> \"), encoder, decoder, voc_dict)\n",
        "# 把 res 輸出到 APP"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gth0knmxvfw",
        "outputId": "7b156751-5351-461e-b859-21823204bf8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> 垃圾\n",
            "evaluate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oy87lzMzVQLW",
        "outputId": "b76a9528-321b-489a-da88-8bd0d2aa4f29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "不麼麼麼麼麼麼麼麼麼麼\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rlkGAxofVeit"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}